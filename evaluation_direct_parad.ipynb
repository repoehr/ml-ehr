{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import patchworklib as pw\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    average_precision_score, precision_recall_curve,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    recall_score, precision_score,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 100x100 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(xtrain, xtest ):\n",
    "    cols_to_impute = [col for col in xtrain.columns if  col == \"age_at_prediction_window\"]\n",
    "\n",
    "    if len(cols_to_impute) == 0:\n",
    "        print('\\tpreprocess column', None)\n",
    "        return xtrain, xtest, None\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(xtrain[cols_to_impute])\n",
    "    print('\\tpreprocess column', cols_to_impute)\n",
    "    xtrain[cols_to_impute] = scaler.transform(xtrain[cols_to_impute])\n",
    "    xtest[cols_to_impute] = scaler.transform(xtest[cols_to_impute])\n",
    "\n",
    "    return xtrain, xtest, scaler\n",
    "\n",
    "   \n",
    "def preprocess_unmatched(xtrain ):\n",
    "    cols_to_impute = [col for col in xtrain.columns if  col == \"age_at_prediction_window\"]\n",
    "\n",
    "    if len(cols_to_impute) == 0:\n",
    "        print('\\tpreprocess column', None)\n",
    "        return xtrain\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(xtrain[cols_to_impute])\n",
    "    print('\\tpreprocess column', cols_to_impute)\n",
    "    xtrain[cols_to_impute] = scaler.transform(xtrain[cols_to_impute])\n",
    "\n",
    "    return xtrain\n",
    "\n",
    "\n",
    "def preprocess_unmatched_scalar(xinput, scalar):\n",
    "    cols_to_impute = [col for col in xinput.columns if  col == \"age_at_prediction_window\"]\n",
    "    if len(cols_to_impute) == 0:\n",
    "        print('\\tpreprocess column', None)\n",
    "        return  xinput\n",
    "\n",
    "    xinput[cols_to_impute] = scalar.transform(xinput[cols_to_impute])\n",
    "    print('Use pretrained Scalar ', scalar)\n",
    "    return xinput\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_prob, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    avpre = average_precision_score(y_true, y_pred_prob)\n",
    "    return auc, avpre, None, None, None, None\n",
    "\n",
    "\n",
    "def ppv_sensitivity(specificity_levels, _y_true, _y_pred_proba):\n",
    "    add_sensitivity_results = []\n",
    "    add_ppv_results =  []\n",
    "    add_fpr, add_tpr, add_thresholds = roc_curve(_y_true, _y_pred_proba)\n",
    "    _results = {}\n",
    "    for specificity in specificity_levels:\n",
    "\n",
    "        _threshold_index = np.where(add_fpr <= (1 - specificity))[0][-1]\n",
    "        _threshold = add_thresholds[_threshold_index]\n",
    "\n",
    "        # Sensitivity (True Positive Rate)\n",
    "        _sensitivity = add_tpr[_threshold_index]\n",
    "        add_sensitivity_results.append( _sensitivity)\n",
    "\n",
    "        # Positive Predictive Value (PPV)\n",
    "        _y_pred_binary = (_y_pred_proba >= _threshold).astype(int)\n",
    "        _ppv = precision_score(_y_true, _y_pred_binary)\n",
    "        add_ppv_results.append(_ppv)\n",
    "\n",
    "    # Add metrics to results\n",
    "    _results['Sensitivity'] = add_sensitivity_results\n",
    "    _results['PPV'] = add_ppv_results\n",
    "    return _results\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_base_model(X_test, y_test, model, CP_num, prediction_window, N, feature_map, model_name, plot=True):\n",
    "    best_mod = model\n",
    "\n",
    "    predicted_proba = best_mod.predict_proba(X_test)[:, 1]\n",
    "    predicted_labels = best_mod.predict(X_test)\n",
    "\n",
    "    auc, pre,sensitivity , specificity , ppv, npv = calculate_metrics(y_test, predicted_proba, predicted_labels)\n",
    "    \n",
    "    add_results = ppv_sensitivity([0.9, 0.95], y_test, predicted_proba)\n",
    "    sensitivity_90, sensitivity_95 = add_results['Sensitivity']\n",
    "    ppv_90, ppv_95 = add_results['PPV']\n",
    "    \n",
    "    if plot:\n",
    "        print(f\"\\nDisplaying performance for CP {CP_num} with a {prediction_window}-year prediction window:\\n\")\n",
    "\n",
    "        prec, rec, threshold = precision_recall_curve(y_test, predicted_proba)\n",
    "        prc_df = pd.DataFrame({\"Recall\": rec, \"Precision\": prec})\n",
    "        ap_score = average_precision_score(y_test, predicted_proba)\n",
    "        base_ap_score = np.mean(y_test)\n",
    "        prc_plot = (\n",
    "            ggplot(prc_df, aes(\"Recall\", \"Precision\")) + \n",
    "            geom_line(color=\"#3C5488B2\") +\n",
    "            theme_bw() +\n",
    "            theme() +\n",
    "            coord_fixed() +\n",
    "            geom_hline(yintercept=base_ap_score, linetype=\"dashed\") +\n",
    "            labs(title=\"Precision-Recall Curve\") +\n",
    "            annotate(\"text\", x=0.15, y=1, label=f\"AP={ap_score:.2f}\", size=8) +\n",
    "            annotate(\"text\", x=0.3, y=0.95, label=f\"Chance Level AP={base_ap_score:.2f}\", size=8)\n",
    "            )\n",
    "        ax1 = pw.load_ggplot(prc_plot, figsize=(2.5, 2.5))\n",
    "    \n",
    "        ax2 = pw.Brick(figsize=(2.5, 2.5))\n",
    "        cm = confusion_matrix(y_test, predicted_labels, labels=best_mod.classes_)\n",
    "        sns.heatmap(cm, annot=True, linewidth=1, cmap=\"GnBu\", fmt=\"g\",\n",
    "                    yticklabels=[\"Control\", \"Case\"], xticklabels=[\"Control\", \"Case\"], ax=ax2)\n",
    "        ax2.set_title(\"Confusion Matrix\")\n",
    "        ax2.set_xlabel(\"Predicted Label\")\n",
    "        ax2.set_ylabel(\"True Label\")\n",
    "    \n",
    "        coefs = best_mod.coef_[0]\n",
    "        top_N_feature_index = np.argsort(abs(coefs))[-N:]\n",
    "        top_N_feature_names = X_test.columns[top_N_feature_index]\n",
    "\n",
    "        def get_name(x):\n",
    "            if isinstance(x, str):\n",
    "                return feature_map.get(x.strip(), x)\n",
    "            else:\n",
    "                return feature_map.get(x, x)\n",
    "        top_N_feature_names = pd.Series(top_N_feature_names).apply(get_name)\n",
    "\n",
    "\n",
    "        top_N_coefs_abs = abs(coefs)[top_N_feature_index]\n",
    "        top_N_coefs = coefs[top_N_feature_index]\n",
    "        coef_plot_df = pd.DataFrame({\"feature_name\": top_N_feature_names,\n",
    "                                \"abs_coef\": top_N_coefs_abs,\n",
    "                                \"coef\": top_N_coefs})\n",
    "        coef_plot_df['feature_name'] = pd.Categorical(coef_plot_df['feature_name'], categories=coef_plot_df.sort_values('abs_coef')['feature_name'])\n",
    "\n",
    "        feature_importance_plot = (\n",
    "            ggplot(coef_plot_df, aes(\"feature_name\", \"coef\")) +\n",
    "                geom_bar(stat=\"identity\", fill=\"#91D1C2B2\", color=\"black\") +\n",
    "                coord_flip() +\n",
    "                theme_bw() +\n",
    "                labs(x=\"\", y=\"Feature Coefficients\", title=f\"{model_name} Model (CP {CP_num} with a {prediction_window}-yr prediction window)\")\n",
    "            )\n",
    "        ax3 = pw.load_ggplot(feature_importance_plot, figsize=(5, 4))\n",
    "        ax_all = (ax1 | ax2)/ax3\n",
    "    else:\n",
    "        ax_all = None\n",
    "    results_list = [auc , pre, sensitivity , specificity , ppv, npv , sensitivity_90, sensitivity_95, ppv_90, ppv_95]\n",
    "\n",
    "    return ax_all, results_list\n",
    "\n",
    "\n",
    "def parallel_forest_predict_proba(forest, X, n_jobs=-1):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    n_jobs = max(1, math.floor(cores*0.7))\n",
    "\n",
    "    def predict_proba_tree(tree, X_subset):\n",
    "        leaf_ids = tree.apply(X_subset)\n",
    "        \n",
    "        leaf_values = tree.tree_.value  # Shape: (n_nodes, n_classes)\n",
    "        \n",
    "        probas = leaf_values[leaf_ids][:, 0]   \n",
    "        \n",
    "        probas = probas / probas.sum(axis=1, keepdims=True)\n",
    "        return probas\n",
    "\n",
    "    tree_probas = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(predict_proba_tree)(tree, X) for tree in forest.estimators_\n",
    "    )\n",
    "    # print(len(tree_probas), tree_probas[0])\n",
    "    all_probas = np.sum(tree_probas, axis=0)  # Shape: (n_samples, n_classes)\n",
    "    # print(all_probas, all_probas.shape)\n",
    "    avg_probas = all_probas / len(forest.estimators_)\n",
    "    return all_probas\n",
    "\n",
    "\n",
    "def predict_proba_stack( _loadmodel,  input_np_format):\n",
    "    \n",
    "    def predict_proba_tree(tree, X_subset):\n",
    "        leaf_ids = tree.apply(X_subset)\n",
    "        \n",
    "        leaf_values = tree.tree_.value  # Shape: (n_nodes, n_classes)\n",
    "        \n",
    "        probas = leaf_values[leaf_ids][:, 0]   \n",
    "        \n",
    "        probas = probas / probas.sum(axis=1, keepdims=True)\n",
    "        return probas\n",
    "    \n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "    def parallel_tree_predict_proba(trees, X):\n",
    "        with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "            preds = list(executor.map(lambda tree: predict_proba_tree(tree, X), trees))\n",
    "        return preds\n",
    "\n",
    "    _loadmodel.n_classes_ = 2\n",
    "    # _model.n_classes_ = 2\n",
    "    saved_estimators  =  _loadmodel.estimators_ \n",
    "    start = time.time()\n",
    "\n",
    "    for tree in saved_estimators:\n",
    "        if not hasattr(tree, \"monotonic_cst\"):\n",
    "            tree.monotonic_cst = None  # Set default value to None\n",
    "\n",
    "\n",
    "    tree_probas = parallel_tree_predict_proba(saved_estimators, input_np_format)\n",
    "    proba_pre = np.mean(tree_probas, axis=0)  \n",
    "    return proba_pre\n",
    "\n",
    "def evaluate_ensemble_model( X_test, y_test, model, CP_num, prediction_window, N, feature_map, model_name='', plot=True):\n",
    "\n",
    "    best_mod = model\n",
    "\n",
    "    saved_estimators = best_mod.estimators_\n",
    "\n",
    "    best_mod.n_classes_ = 2\n",
    "    best_mod.n_outputs_ = 1\n",
    "    best_mod.classes_ = np.array([ 0,1]) \n",
    "\n",
    "    for tree in saved_estimators:\n",
    "        if not hasattr(tree, \"monotonic_cst\"):\n",
    "            tree.monotonic_cst = None  # Set default value to None\n",
    "\n",
    "    X_test_input = X_test.values\n",
    "    predicted_proba = predict_proba_stack(best_mod, X_test_input)[:, 1]\n",
    "\n",
    "    # predicted_labels = best_mod.predict(X_test_input)\n",
    "    predicted_labels = None\n",
    "    auc, pre,sensitivity , specificity , ppv, npv = calculate_metrics(y_test, predicted_proba, predicted_labels)\n",
    "\n",
    "    add_results = ppv_sensitivity([0.9, 0.95], y_test, predicted_proba)\n",
    "    sensitivity_90, sensitivity_95 = add_results['Sensitivity']\n",
    "    ppv_90, ppv_95 = add_results['PPV']\n",
    "    plot = False\n",
    "    if plot: # plot prc curve, confusion matrix, feature importance using default method from each type of models itself. Could skip this plot. \n",
    "        print(f\"\\nDisplaying performance for CP {CP_num} with a {prediction_window}-year prediction window:\\n\")\n",
    "\n",
    "        prec, rec, threshold = precision_recall_curve(y_test, predicted_proba)\n",
    "        prc_df = pd.DataFrame({\"Recall\": rec, \"Precision\": prec})\n",
    "        ap_score = average_precision_score(y_test, predicted_proba)\n",
    "        base_ap_score = np.mean(y_test)\n",
    "\n",
    "        prc_plot = (\n",
    "            ggplot(prc_df, aes(\"Recall\", \"Precision\")) + \n",
    "            geom_line(color=\"#3C5488B2\") +\n",
    "            theme_bw() +\n",
    "            theme() +\n",
    "            coord_fixed() +\n",
    "            geom_hline(yintercept=base_ap_score, linetype=\"dashed\") +\n",
    "            labs(title=\"Precision-Recall Curve\") +\n",
    "            annotate(\"text\", x=0.15, y=1, label=f\"AP={ap_score:.2f}\", size=8) +\n",
    "            annotate(\"text\", x=0.3, y=0.95, label=f\"Chance Level AP={base_ap_score:.2f}\", size=8)\n",
    "            )\n",
    "        ax1 = pw.load_ggplot(prc_plot, figsize=(2.5, 2.5))\n",
    "\n",
    "\n",
    "        ax2 = pw.Brick(figsize=(2.5, 2.5))\n",
    "        cm = confusion_matrix(y_test, predicted_labels, labels=best_mod.classes_)\n",
    "        sns.heatmap(cm, annot=True, linewidth=1, cmap=\"GnBu\", fmt=\"g\",\n",
    "                    yticklabels=[\"Control\", \"Case\"], xticklabels=[\"Control\", \"Case\"], ax=ax2)\n",
    "        ax2.set_title(\"Confusion Matrix\")\n",
    "        ax2.set_xlabel(\"Predicted Label\")\n",
    "        ax2.set_ylabel(\"True Label\")\n",
    "\n",
    "        feature_importances = best_mod.feature_importances_\n",
    "        top_N_feature_index = np.argsort(feature_importances)[-N:]\n",
    "        top_N_feature_names = X_test.columns[top_N_feature_index]\n",
    "\n",
    "        def get_name(x):\n",
    "            if isinstance(x, str):\n",
    "                return feature_map.get(x.strip(), x)\n",
    "            else:\n",
    "                return feature_map.get(x, x)\n",
    "\n",
    "        top_N_feature_names = pd.Series(top_N_feature_names).apply(get_name)\n",
    "\n",
    "        top_N_importances = feature_importances[top_N_feature_index]\n",
    "\n",
    "        coef_plot_df = pd.DataFrame({\n",
    "            \"feature_name\": top_N_feature_names,\n",
    "            \"importance\": top_N_importances\n",
    "        })\n",
    "\n",
    "        coef_plot_df['feature_name'] = pd.Categorical(coef_plot_df['feature_name'], categories=coef_plot_df.sort_values('importance')['feature_name'])\n",
    "\n",
    "        feature_importance_plot = (\n",
    "            ggplot(coef_plot_df, aes(\"feature_name\", \"importance\")) +\n",
    "                geom_bar(stat=\"identity\", fill=\"#91D1C2B2\", color=\"black\") +\n",
    "                coord_flip() +\n",
    "                theme_bw() +\n",
    "                labs(x=\"\", y=\"Feature Importance\", title=f\"{model_name} Model (CP {CP_num} with a {prediction_window}-yr prediction window)\")\n",
    "            )\n",
    "        ax3 = pw.load_ggplot(feature_importance_plot, figsize=(5, 4))\n",
    "        ax_all = (ax1 | ax2)/ax3\n",
    "    else:\n",
    "        ax_all = None\n",
    "    results_list = [auc , pre, sensitivity , specificity , ppv, npv , sensitivity_90, sensitivity_95, ppv_90, ppv_95]\n",
    "\n",
    "    return ax_all , results_list\n",
    "\n",
    "\n",
    "def run_direct_evaluate_pipeline(X, y, all_map, model_type,  years=[0,1,2,5,10], pre_trained_model=None, score=None, f_reference=None, model_name=None, scalars=None, plot=False):\n",
    "    cp_year_results = {}\n",
    "    show_results_dict = {}\n",
    "    # shap_years = {}\n",
    "\n",
    "    for cp in [1]:\n",
    "        for prediction_window in reversed(years):\n",
    "            if pre_trained_model: # should always pass pretrained model inside\n",
    "                cv_results = []\n",
    "                for cv in range(1):  # test each cv from the pre-trained model or only test a part of cvs\n",
    "                    print('CV: ', cv, '| Prediction window: ', prediction_window, '| Model type: ', model_type)\n",
    "                    # model = pre_trained_model[prediction_window] \n",
    "                    # scalar = scalars[prediction_window] \n",
    "                    model = pre_trained_model\n",
    "                    scalar = scalars\n",
    "                    try:\n",
    "                        saved_model_features = f_reference[f'CP_{cp}_{prediction_window}_yr'].drop('person_id', axis=1).columns\n",
    "                    except:\n",
    "                        refer_cols = f_reference[f'CP_{cp}_{prediction_window}_yr']\n",
    "\n",
    "                        if not isinstance(refer_cols, list):\n",
    "                            refer_cols= f_reference[f'CP_{cp}_{prediction_window}_yr'].to_list()\n",
    "                        saved_model_features = [i for i in  refer_cols if i != 'person_id']\n",
    "                    # else:\n",
    "                    #     saved_model_features = model.feature_names_in_\n",
    "\n",
    "                    current_features = X[prediction_window].columns\n",
    "                    overlapping_cols = set(saved_model_features).intersection(set(current_features))\n",
    "                    missing_cols = set(saved_model_features) - set(current_features)\n",
    "                    print('Current cols\\t', len(current_features))\n",
    "                    print('Reference cols\\t', len(saved_model_features))\n",
    "                    print('Overalaping cols\\t', len(overlapping_cols))\n",
    "                    print('Missing_cols cols\\t', len(missing_cols), missing_cols)\n",
    "                    \n",
    "                    # f_input = X[prediction_window][list(overlapping_cols)]\n",
    "\n",
    "                    f_input = X[prediction_window].reindex(columns=saved_model_features, fill_value=0)\n",
    "\n",
    "                    # print(  f_input.dtypes)\n",
    "                    \n",
    "                    if  model_type == 'xgb':\n",
    "                        X_newnames = dict(zip(f_input.columns, range(len(f_input.columns))))\n",
    "                        X_newnames['age_at_prediction_window'] = 'age_at_prediction_window'\n",
    "                        f_input.columns = [X_newnames[i] for i in f_input.columns]\n",
    "                        # if xgboost, the mapping dictionarity will be converted, the key will be changed to the index of column\n",
    "                        featuremap2 = {}\n",
    "                        for k, v in X_newnames.items():\n",
    "                            j = all_map.get(k, k)\n",
    "                            featuremap2[v] = j\n",
    "                        all_map = featuremap2\n",
    "\n",
    "                    # f_input = preprocess_unmatched(f_input)\n",
    "                    f_input = preprocess_unmatched_scalar(f_input, scalar)\n",
    "\n",
    "                    # print(\"New data range (min, max):\")\n",
    "                    # print(f_input.min().min(), f_input.max().max())\n",
    "                    y_input = y[prediction_window]\n",
    "                    if model_type == 'rf' or model_type == 'xgb':\n",
    "                        axall, results_list = evaluate_ensemble_model(f_input, y_input, model, 1, prediction_window, 30, all_map, model_name=model_name, plot=plot)\n",
    "                    else:\n",
    "                        axall, results_list = evaluate_base_model(f_input, y_input, model, 1, prediction_window, 30, all_map, model_name=model_name, plot=plot)\n",
    "                    if plot: \n",
    "                        display(axall)\n",
    "#                     shapfi = shap_values(model, f_input, f_input, model_type, cols_names)\n",
    "#                     shap_years[f\"{str(cp)}_{str(prediction_window)}_{model_type}_{str(cv)}\"] = shapfi\n",
    "                    cp_year_results[f\"{str(prediction_window)}_{model_type}_{str(cv)}\"] = results_list # put all cv and prediction_window evaluation results to a dictionary\n",
    "                    cv_results.append(results_list) # meanwhile, show the results\n",
    "\n",
    "                showresults = pd.DataFrame(cv_results)\n",
    "                showresults.columns = ['auc', 'pre', 'sensitivity', 'specificity', 'ppv', 'npv', 'sensitivity_90', 'sensitivity_95', 'ppv_90', 'ppv_95']\n",
    "                showresults.loc['mean'] = showresults.mean()\n",
    "                showresults.loc['std'] = showresults.std()\n",
    "                display('Show results of cvs', showresults)\n",
    "                show_results_dict[f\"{str(cp)}_{str(prediction_window)}_{model_type}\"] = showresults\n",
    "\n",
    "    return show_results_dict, cp_year_results \n",
    "\n",
    "\n",
    "def reconstruct_rf(name, dirc='rf_chunks', modelkey=None, modelcv=None):\n",
    "    model_name = name\n",
    "    output_dir_model = os.path.join(dirc, model_name)\n",
    "    \n",
    "    if not os.path.exists(output_dir_model):\n",
    "        raise FileNotFoundError(f\"Error: The directory '{output_dir_model}' does not exist. Make sure the chunks are saved.\")\n",
    "    \n",
    "    chunk_files = sorted([os.path.join(output_dir_model, f) for f in os.listdir(output_dir_model) if f.endswith(\".pkl\") and ('chunk_' in f)])\n",
    "    key_chunk_files = []\n",
    "    for ifile in chunk_files:\n",
    "        if str(modelkey) + '_' + str(modelcv) in ifile: \n",
    "                key_chunk_files.append(ifile )\n",
    "    chunk_files = key_chunk_files\n",
    "\n",
    "    if len(chunk_files) == 0:\n",
    "        raise FileNotFoundError(f\"No chunk files found in '{output_dir_model}'. Ensure the chunks were saved correctly.\")\n",
    "    \n",
    "    reconstructed_rf = RandomForestClassifier()\n",
    "    reconstructed_rf.estimators_ = []\n",
    "    print(f\"Ori\", len(reconstructed_rf.estimators_))\n",
    "\n",
    "    for chunk_file in chunk_files:\n",
    "        chunk = joblib.load(chunk_file)\n",
    "        reconstructed_rf.estimators_.extend(chunk)\n",
    "        print(f\"Loaded {chunk_file}\", len(chunk), len(reconstructed_rf.estimators_))\n",
    "    \n",
    "    reconstructed_rf.n_estimators = len(reconstructed_rf.estimators_)\n",
    "\n",
    "    reconstructed_rf.modelname = name\n",
    "    reconstructed_rf.modelkey = modelkey\n",
    "    reconstructed_rf.modelcv = modelcv\n",
    "    \n",
    "    print(f\"Reconstructed RF model with {reconstructed_rf.n_estimators} estimators.\")\n",
    "    return reconstructed_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### directly evaluate existing models on unmatched testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_map = pickle.load(open('all_map.pkl', 'rb'))\n",
    "for i, v in all_map.items():\n",
    "    all_map[i] = i + ' ' + v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test previous downloaded model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "hold_out_portion = 0.5\n",
    "ratio = 10\n",
    "test_f = pickle.load( open(f'./MiddleFeatures/demo_unmatched_fs.pkl', 'rb'))\n",
    "test_t = pickle.load( open(f'./MiddleFeatures/test_t_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'rb'))\n",
    "test_e = pickle.load( open(f'./MiddleFeatures/test_e_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_cvs = [4, 0, 1, 1, 0]\n",
    "windows = [10, 5, 2, 1, 0]\n",
    "\n",
    "reference_cols = joblib.load('./rf_chunks/model_test_rf_all_feature/reference_cols_xgb_all_feature.pkl')\n",
    "scalarmodels = joblib.load('./rf_chunks/model_test_rf_all_feature/test_rf_scalar_all_feature.pkl')\n",
    "\n",
    "\n",
    "show_results_dict, results = {}, {}\n",
    "\n",
    "for inde in range(5):\n",
    "\n",
    "    print('Finetune for prediction window', windows[inde])\n",
    "\n",
    "    pretrainedRF = reconstruct_rf('model_test_rf_all_feature', modelkey=f'1_{windows[inde]}_rf', modelcv=best_cvs[inde])\n",
    "\n",
    "    pretrained_scalar = scalarmodels[f'1_{windows[inde]}_rf'][best_cvs[inde]] \n",
    "\n",
    "    show_results_dict[windows[inde]], results[windows[inde]] = run_direct_evaluate_pipeline(test_f, test_t, all_map, 'rf',  years=[windows[inde]],  pre_trained_model=pretrainedRF,\\\n",
    "                             score=None , model_name='all feature', scalars=pretrained_scalar, f_reference=reference_cols, plot=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(results, open(f'rf_chunks/model_test_rf_all_feature_direct/demoonlyage_rs_years_unmatched.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(show_results_dict, open(f'rf_chunks/model_test_rf_all_feature_direct/demoonlyage_showrs_years_unmatched.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrdPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

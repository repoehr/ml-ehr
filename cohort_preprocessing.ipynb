{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "a = time.time()\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from comorbidipy import comorbidity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "%load_ext Cython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pickle(file, folder):\n",
    "    df = pickle.load(open(f'{folder}/{file}', 'rb'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_data(site):\n",
    "    \n",
    "    demo_site = load_pickle(f'{site[:2]}demo_all.pkl', 'EHR')\n",
    "    print('Load demo data for site ', site, demo_site.shape)\n",
    "    dx_site = load_pickle(f'{site[:2]}dx_all.pkl', 'EHR')\n",
    "    print('Load dx data for site ', site, dx_site.shape)\n",
    "    rx_site = load_pickle(f'{site[:2]}rx_all.pkl', 'EHR')\n",
    "    print('Load rx data for site ', site, rx_site.shape)\n",
    "    lab_site = load_pickle(f'{site[:2]}lab_all.pkl', 'EHR')\n",
    "    print('Load lab data for site ', site, lab_site.shape)\n",
    "    vital_site = load_pickle(f'{site[:2]}vital_all.pkl', 'EHR')\n",
    "    print('Load vital data for site ', site, vital_site.shape)\n",
    "\n",
    "    person = set(demo_site.patid.unique().tolist()  +  dx_site.patid.unique().tolist() +  rx_site.patid.unique().tolist() +  lab_site.patid.unique().tolist() + vital_site.patid.unique().tolist())\n",
    "    enc  =set( demo_site.encounterid.unique().tolist()  +  dx_site.encounterid.unique().tolist() +  rx_site.encounterid.unique().tolist() +  lab_site.encounterid.unique().tolist() + vital_site.encounterid.unique().tolist())\n",
    "\n",
    "\n",
    "\n",
    "    return len(person), len(enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load in all of the data, already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def get_site_data(site):\n",
    "    \n",
    "    demo_site = load_pickle(f'{site[:2]}demo_all.pkl', 'EHR')\n",
    "    print('Load demo data for site ', site, demo_site.shape)\n",
    "    dx_site = load_pickle(f'{site[:2]}dx_all.pkl', 'EHR')\n",
    "    print('Load dx data for site ', site, dx_site.shape)\n",
    "    rx_site = load_pickle(f'{site[:2]}rx_all.pkl', 'EHR')\n",
    "    print('Load rx data for site ', site, rx_site.shape)\n",
    "    lab_site = load_pickle(f'{site[:2]}lab_all.pkl', 'EHR')\n",
    "    print('Load lab data for site ', site, lab_site.shape)\n",
    "    vital_site = load_pickle(f'{site[:2]}vital_all.pkl', 'EHR')\n",
    "    print('Load vital data for site ', site, vital_site.shape)\n",
    "\n",
    "    dx_site.drop('encounterid', axis=1, inplace=True)\n",
    "    rx_site.drop('encounterid', axis=1, inplace=True)\n",
    "    lab_site.drop('encounterid', axis=1, inplace=True)\n",
    "    vital_site.drop('encounterid', axis=1, inplace=True)\n",
    "\n",
    "    return demo_site, dx_site, rx_site, lab_site, vital_site\n",
    "\n",
    "\n",
    "def get_domains_for_40_persons(demo_site, dx_site, rx_site, lab_site, vital_site):\n",
    "    comparison_date = pd.Timestamp('1975-01-01') \n",
    "    demo_site = demo_site[demo_site['birth_date']<=comparison_date]\n",
    "    demo_40 = demo_site.patid.unique().tolist()\n",
    "    print('Find demo <= comparison date', comparison_date, len(demo_40))\n",
    "\n",
    "    print('Pick >=40 persons rows from demographics', demo_site.shape)\n",
    "\n",
    "    dx_site = dx_site[dx_site.patid.isin(demo_40)]\n",
    "    print('Pick >=40 persons rows from dx', dx_site.shape)\n",
    "\n",
    "    rx_site = rx_site[rx_site.patid.isin(demo_40)]\n",
    "    print('Pick >=40 persons rows from rx', rx_site.shape)\n",
    "\n",
    "    lab_site = lab_site[lab_site.patid.isin(demo_40)]\n",
    "    print('Pick >=40 persons rows from lab', lab_site.shape)\n",
    "\n",
    "    vital_site = vital_site[vital_site.patid.isin(demo_40)]\n",
    "    print('Pick >=40 persons rows from vital', vital_site.shape)\n",
    "    gc.collect()\n",
    "    return demo_site, dx_site, rx_site, lab_site, vital_site\n",
    "\n",
    "\n",
    "def get_all_dates(dx_site, rx_site, lab_site, vital_site):\n",
    "\n",
    "    file_dfs = [ dx_site, rx_site, lab_site, vital_site]\n",
    "    date_names = ['dx_date_fill', 'rx_start_date', 'specimen_date', 'measure_date']  \n",
    "\n",
    "\n",
    "    merged_ages_list = []\n",
    "\n",
    "    for i in range(len(date_names)):\n",
    "        print('df shape', file_dfs[i].shape)\n",
    "        if file_dfs[i].shape[0] > 0:\n",
    "            merged_ages_list.append(file_dfs[i][['patid', date_names[i]]].rename(columns={date_names[i]: \"ALL_AGES\"}))\n",
    "\n",
    "    merged_ages = pd.concat(merged_ages_list, ignore_index = True).drop_duplicates()\n",
    "\n",
    "    print('--Merge age rows', merged_ages.shape)\n",
    "\n",
    "    merged_ages = merged_ages.groupby('patid')['ALL_AGES'].apply(list).reset_index(name='ALL_AGES')\n",
    "\n",
    "    print(f\"--There are {len(merged_ages['patid'])} unique patients across all the files.\")\n",
    "\n",
    "    merged_ages = merged_ages.iloc[\n",
    "    ((merged_ages['ALL_AGES'].apply(lambda x: max(x) - min(x)).dt.days) >= 1).values, :]\n",
    "\n",
    "    all_patients = merged_ages['patid'].unique().tolist()\n",
    "    \n",
    "    print(f\"--There are {len(all_patients)} patients with at least 1 year of data available across all files.\")\n",
    "    return merged_ages\n",
    "\n",
    "def get_domains_for_1year_EHR_persons(ehr_1_year_persons,demo_site , dx_site, rx_site, lab_site, vital_site):\n",
    "\n",
    "    demo_site = demo_site[demo_site.patid.isin(ehr_1_year_persons)]\n",
    "    print('Pick >=1 year EHR  persons rows from demo', demo_site.shape)\n",
    "\n",
    "    dx_site = dx_site[dx_site.patid.isin(ehr_1_year_persons)]\n",
    "    print('Pick >=1 year EHR  persons rows from dx', dx_site.shape)\n",
    "\n",
    "    rx_site = rx_site[rx_site.patid.isin(ehr_1_year_persons)]\n",
    "    print('Pick >=1 year EHR  persons rows from rx', rx_site.shape)\n",
    "\n",
    "    lab_site = lab_site[lab_site.patid.isin(ehr_1_year_persons)]\n",
    "    print('Pick >=1 year EHR  persons rows from lab', lab_site.shape)\n",
    "\n",
    "    vital_site = vital_site[vital_site.patid.isin(ehr_1_year_persons)]\n",
    "    print('Pick >=1 year EHR  persons rows from vital', vital_site.shape)\n",
    "    gc.collect()\n",
    "    return demo_site, dx_site, rx_site, lab_site, vital_site\n",
    "\n",
    "\n",
    "def get_domains_for_dx_persons(demo_site, dx_site, rx_site, lab_site, vital_site):\n",
    "    dx_persons = dx_site.patid.unique().tolist()\n",
    "    print('Find dx_persons', len(dx_persons))\n",
    "\n",
    "    print('Pick dx persons rows in dx', dx_site.shape)\n",
    "\n",
    "    demo_site = demo_site[demo_site.patid.isin(dx_persons)]\n",
    "    print('Pick dx persons rows from demographics', demo_site.shape)\n",
    "\n",
    "    rx_site = rx_site[rx_site.patid.isin(dx_persons)]\n",
    "    print('Pick dx persons rows from rx', rx_site.shape)\n",
    "\n",
    "    lab_site = lab_site[lab_site.patid.isin(dx_persons)]\n",
    "    print('Pick dx persons rows from lab', lab_site.shape)\n",
    "\n",
    "    vital_site = vital_site[vital_site.patid.isin(dx_persons)]\n",
    "    print('Pick dx persons rows from vital', vital_site.shape)\n",
    "    gc.collect()\n",
    "    return demo_site, dx_site, rx_site, lab_site, vital_site\n",
    "\n",
    "\n",
    "\n",
    "# load in the ADRD diagnosis and medication codes from the Florida EHR paper\n",
    "ADRD_dx_med_codes = pd.read_csv(\"./ADRD_dx_med_codes.csv\")\n",
    "# display(ADRD_dx_med_codes)\n",
    "# get the ADRD diagnosis codes for cases\n",
    "ADRD_STRINGS = [\"Alzheimer's disease\", \"Vascular dementia\", \"Frontotemporal dementia\", \"Lewy Body Dementia\"]\n",
    "ADRD_STRINGS = '|'.join(ADRD_STRINGS)\n",
    "\n",
    "ADRD_ICD9 = ADRD_dx_med_codes[(ADRD_dx_med_codes['Code_type'] == 'ICD-9') & ADRD_dx_med_codes['Concept'].str.contains(ADRD_STRINGS)].Code.reset_index(drop=True)\n",
    "ADRD_ICD10 = ADRD_dx_med_codes[(ADRD_dx_med_codes['Code_type'] == 'ICD-10') & ADRD_dx_med_codes['Concept'].str.contains(ADRD_STRINGS)].Code.reset_index(drop=True)\n",
    "display('ADRD ICD9 and ICD10 codes:', ' '.join(ADRD_ICD9.tolist()), ' '.join(ADRD_ICD10.tolist()))\n",
    "\n",
    "# get the ADRD and other conditions to use as what a control patient should not have\n",
    "ADRD_AND_OTHER_CONDITIONS = ADRD_STRINGS +'|'+ '|'.join([\"Dementia\", \"Conditions cause dementia\"])\n",
    "\n",
    "ADRD_AND_OTHER_ICD9 = ADRD_dx_med_codes[(ADRD_dx_med_codes['Code_type'] == 'ICD-9') & ADRD_dx_med_codes['Concept'].str.contains(ADRD_AND_OTHER_CONDITIONS)].Code.reset_index(drop=True)\n",
    "ADRD_AND_OTHER_ICD10 = ADRD_dx_med_codes[(ADRD_dx_med_codes['Code_type'] == 'ICD-10') & ADRD_dx_med_codes['Concept'].str.contains(ADRD_AND_OTHER_CONDITIONS)].Code.reset_index(drop=True)\n",
    "display('ADRD and other dementia ICD9 and ICD10 codes:',' '.join(ADRD_AND_OTHER_ICD9.tolist()), ' '.join(ADRD_AND_OTHER_ICD10.tolist()))\n",
    "\n",
    "\n",
    "ANTI_DEMENTIA_RXCUI = ADRD_dx_med_codes[(ADRD_dx_med_codes['Concept'] == \"Anti-dementia medications\") & (ADRD_dx_med_codes['Code_type'] == 'RXCUI')].Code.reset_index(drop=True)\n",
    "\n",
    "ANTI_DEMENTIA_RXCUI_list = ANTI_DEMENTIA_RXCUI.tolist()\n",
    "print('ANTI_DEMENTIA_RXCUI_list:', ANTI_DEMENTIA_RXCUI_list)\n",
    "\n",
    "\n",
    "\n",
    "def get_dx_case_control(dx_site, rx_site, ADRD_ICD9, ADRD_ICD10, ADRD_AND_OTHER_ICD9, ADRD_AND_OTHER_ICD10, ANTI_DEMENTIA_RXCUI_list ):\n",
    "   \n",
    "    ############### CASES ##################\n",
    "    # get all of the rows from dx_enc with an ICD9_CODE or ICD10_CODE that matches the ADRD diagnosis codes\n",
    "    dx_site9 = dx_site[dx_site['dx_type']==9]\n",
    "    dx_site10 = dx_site[dx_site['dx_type']==10]\n",
    "    print('--dx_site9', dx_site9.shape)\n",
    "    print('--dx_site10', dx_site10.shape)\n",
    "\n",
    "    dx_site_adrd9 = dx_site9[dx_site9['dx'].isin(ADRD_ICD9) ]\n",
    "    dx_site_adrd10 = dx_site10[dx_site10['dx'].isin(ADRD_ICD10) ]\n",
    "    print('--dx_site_adrd9', dx_site_adrd9.shape)\n",
    "    print('--dx_site_adrd9', dx_site_adrd10.shape)\n",
    "\n",
    "    dx_site_adrd = pd.concat([dx_site_adrd9, dx_site_adrd10], axis=0)\n",
    "    print('--dx_site_adrd', dx_site_adrd.shape)\n",
    "\n",
    "    dx_cases = dx_site_adrd.patid.unique().tolist()\n",
    "\n",
    "    ############### CONTROLS ##################\n",
    "    dx_site_adrd_large9 = dx_site9[dx_site9['dx'].isin(ADRD_AND_OTHER_ICD9) ]\n",
    "    dx_site_adrd_large10 = dx_site10[dx_site10['dx'].isin(ADRD_AND_OTHER_ICD10) ]\n",
    "    print('--dx_site_adrd_large9', dx_site_adrd_large9.shape)\n",
    "    print('--dx_site_adrd_large10', dx_site_adrd_large10.shape)\n",
    "\n",
    "    dx_site_adrd_large = pd.concat([dx_site_adrd_large9, dx_site_adrd_large10], axis=0)\n",
    "    print('--dx_site_adrd_large', dx_site_adrd_large.shape)\n",
    "\n",
    "    dx_cases_large = dx_site_adrd_large.patid.unique().tolist()\n",
    "    dx_controls = list(set(dx_site['patid'].tolist()) - set(dx_cases_large)) \n",
    "    print(f\"Number of cases: {len(set(dx_cases))}\")  \n",
    "    print(f\"--Number of controls: {len(set(dx_controls))}\") \n",
    "\n",
    "    rx_site_adrd = rx_site[rx_site['rxnorm_cui'].isin(ANTI_DEMENTIA_RXCUI )]\n",
    "\n",
    "    med_cases = set(rx_site_adrd['patid'].tolist())  \n",
    "\n",
    "    dx_rx_controls = set(dx_controls) - med_cases #  controls based on dx patients who are no dx case or rx case\n",
    "\n",
    "    print(f\"Number of rx cases: {len(set(med_cases))}\")  \n",
    "    print('\\t--Intersection of rx and dx cases', len(set(dx_cases) & set(med_cases)))\n",
    "    print('\\t--Intersection of rx and dx large cases', len(set(dx_cases_large) & set(med_cases)))\n",
    "\n",
    "    print(f\"Number of dx rx controls: {len(set(dx_rx_controls))}\") \n",
    "\n",
    "    return dx_cases, dx_rx_controls, dx_site_adrd, rx_site_adrd\n",
    "\n",
    "\n",
    "def get_domains_for_cases_controls(demo_site, dx_site, rx_site, lab_site, vital_site, cases, controls):\n",
    "    cases_controls = set(cases) | set(controls)\n",
    "    demo_site = demo_site[demo_site.patid.isin(cases_controls)]\n",
    "    print('Pick cases_controls persons rows from demographics', demo_site.shape)\n",
    "\n",
    "    dx_site = dx_site[dx_site.patid.isin(cases_controls)]\n",
    "    print('Pick cases_controls persons rows from demographics', dx_site.shape)\n",
    "\n",
    "    rx_site = rx_site[rx_site.patid.isin(cases_controls)]\n",
    "    print('Pick cases_controls persons rows from rx', rx_site.shape)\n",
    "\n",
    "    lab_site = lab_site[lab_site.patid.isin(cases_controls)]\n",
    "    print('Pick cases_controls persons rows from lab', lab_site.shape)\n",
    "\n",
    "    vital_site = vital_site[vital_site.patid.isin(cases_controls)]\n",
    "    print('Pick cases_controls persons rows from vital', vital_site.shape)\n",
    "    gc.collect()\n",
    "    return demo_site, dx_site, rx_site, lab_site, vital_site\n",
    "\n",
    "\n",
    "def get_case_index_and_compare(dx_site_adrd, rx_site_adrd, all_dates, dx_cases):\n",
    "\n",
    "    cases_first_ADRD_df = dx_site_adrd.loc[dx_site_adrd.groupby('patid')['dx_date_fill'].idxmin()]\n",
    "    rx_site_adrd_in_dx = rx_site_adrd[rx_site_adrd.patid.isin(dx_cases)]\n",
    "    med_cases_first_ADRD_df = rx_site_adrd_in_dx.loc[rx_site_adrd_in_dx.groupby('patid')['rx_start_date'].idxmin()]\n",
    "\n",
    "    cases_first_ADRD_df = cases_first_ADRD_df.rename(columns={'dx_date_fill':'first_date'})\n",
    "    med_cases_first_ADRD_df = med_cases_first_ADRD_df.rename(columns={'rx_start_date':'first_date'})\n",
    "\n",
    "    merged_cases_index = pd.concat([cases_first_ADRD_df, med_cases_first_ADRD_df], axis=0)\n",
    "    merged_cases_index = merged_cases_index.groupby('patid')['first_date'].min().reset_index()\n",
    "    merged_cases_index = merged_cases_index.rename(columns={'first_date':'INDEX_DATE'})\n",
    "    print('--First occur dx rows', cases_first_ADRD_df.shape)\n",
    "    print('--First occur rx rows', med_cases_first_ADRD_df.shape)\n",
    "    print('--Index date for unique cases: ', merged_cases_index.patid.nunique())\n",
    "\n",
    "    merged_ages_cases = all_dates[all_dates['patid'].isin(dx_cases)] # have >1r ehr\n",
    "\n",
    "    merged_ages_cases['EARLIEST_DATE'] = merged_ages_cases['ALL_AGES'].apply(min)\n",
    "    merged_ages_cases['LAST_DATE'] = merged_ages_cases['ALL_AGES'].apply(max)\n",
    "\n",
    "    merged_cases_compare = pd.merge(merged_ages_cases, merged_cases_index, on=\"patid\", how = \"inner\")\n",
    "    print('Assert same:', merged_cases_compare.patid.nunique(), merged_cases_index.patid.nunique(), merged_ages_cases.patid.nunique())\n",
    "    return merged_cases_compare\n",
    "\n",
    "\n",
    "def get_control_dates(all_dates, dx_rx_controls ):\n",
    "\n",
    "    merged_controls_dates = all_dates[all_dates['patid'].isin(set(dx_rx_controls))].reset_index(drop=True)\n",
    "    print('--all date rows for controls:', merged_controls_dates.shape)\n",
    "    print('--all dates for controls unique :', merged_controls_dates.patid.nunique())\n",
    "\n",
    "    merged_controls_dates['EARLIEST_DATE'] = merged_controls_dates['ALL_AGES'].apply(min)\n",
    "    merged_controls_dates['LAST_DATE'] = merged_controls_dates['ALL_AGES'].apply(max)\n",
    "\n",
    "    assert (merged_controls_dates['LAST_DATE'] > merged_controls_dates['EARLIEST_DATE']).all(), \\\n",
    "        \"Error: All rows have no LAST_DATE greater than FIRST_DATE\"\n",
    "\n",
    "    merged_controls_dates['Last_EHR_minus_one_INDEX_DATE'] = merged_controls_dates['LAST_DATE'] - pd.DateOffset(years=1)\n",
    "\n",
    "    print('--control number', merged_controls_dates.patid.nunique())\n",
    "    return merged_controls_dates\n",
    "\n",
    "\n",
    "def get_cci_with_ICD(_input_dx, col, _caselist, icd, temp=True):\n",
    "\n",
    "    # for the cases, we are calculating the comorbidities before the first diagnosis of ADRD\n",
    "    _comorbidities_df = comorbidity(df = _input_dx.copy(), id = 'patid', code = col, age = None, icd=icd)\n",
    "\n",
    "    if temp: \n",
    "        # for the cases with no diagnoses before the first ADRD diagnosis, we will assign them a comorbidity score of 0 (and a value of 0 for each comorbidity)\n",
    "        _no_dx_before_first_ADRD = np.setdiff1d(_caselist, _input_dx['patid'].unique())\n",
    "\n",
    "        temp_df = pd.DataFrame(columns=_comorbidities_df.columns)\n",
    "        temp_df['patid'] = _no_dx_before_first_ADRD\n",
    "        temp_df.iloc[:, 1:] = 0.0\n",
    "\n",
    "        # concatenate the two dataframes\n",
    "        _comorbidities_df = pd.concat([_comorbidities_df, temp_df])\n",
    "    return _comorbidities_df\n",
    "\n",
    "\n",
    "def get_merged_cci(cci1, cc2): # we compute cci based on ICD10 and ICD9 respectively, here combine the results from two cci results. \n",
    "    cci = cci1.set_index('patid').combine(cc2.set_index('patid'), func=lambda s1, s2: s1.combine(s2, max),  fill_value=0)\n",
    "    weights = dict(\n",
    "                ami=0,\n",
    "                chf=2,\n",
    "                pvd=0,\n",
    "                cevd=0,\n",
    "                dementia=2,\n",
    "                copd=1,\n",
    "                rheumd=1,\n",
    "                pud=0,\n",
    "                mld=2,\n",
    "                diab=0,\n",
    "                diabwc=1,\n",
    "                hp=2,\n",
    "                rend=1,\n",
    "                canc=2,\n",
    "                msld=4,\n",
    "                metacanc=6,\n",
    "                aids=2,\n",
    "            )\n",
    "\n",
    "    cci_ = cci.iloc[:, :-1]\n",
    "    cci_['comorbidity_score'] = cci_.multiply(weights).sum(axis=1)\n",
    "\n",
    "    cci_['comorbidity_score'] = cci_['comorbidity_score'] - 2 * cci_['dementia']\n",
    "    cci_ = cci_.reset_index()\n",
    "    \n",
    "    return cci_\n",
    "\n",
    "def get_all_cci(dx_site, merged_cases_compare, merged_controls_dates, dx_cases, dx_rx_controls ):\n",
    "\n",
    "    dx_site9 = dx_site[dx_site['dx_type']==9]\n",
    "    dx_site10 = dx_site[dx_site['dx_type']==10]\n",
    "    print('--dx_site9 rows:', dx_site9.shape)\n",
    "    print('--dx_site10 rows:', dx_site10.shape)\n",
    "\n",
    "    cases_before_first_dx9 = pd.merge(dx_site9, merged_cases_compare[['patid', 'INDEX_DATE']], on='patid', how='right')\n",
    "    print('--merge case index and dx_site9 for unique cases:', cases_before_first_dx9.patid.nunique())\n",
    "    cases_before_first_dx10 = pd.merge(dx_site10, merged_cases_compare[['patid', 'INDEX_DATE']], on='patid', how='right')\n",
    "    print('--merge case index and dx_site10 for unique cases:', cases_before_first_dx10.patid.nunique())\n",
    "\n",
    "    cases_before_first_dx9 = cases_before_first_dx9[cases_before_first_dx9.apply(lambda x: x['dx_date_fill'] < x['INDEX_DATE'], axis=1)]\n",
    "    print('--dx 9 rows before case index:', cases_before_first_dx9.patid.nunique())\n",
    "    cases_before_first_dx10 = cases_before_first_dx10[cases_before_first_dx10.apply(lambda x: x['dx_date_fill'] < x['INDEX_DATE'], axis=1)]\n",
    "    print('--dx 10 rows before case index:', cases_before_first_dx10.patid.nunique())\n",
    "\n",
    "    cci_df_icd9 = get_cci_with_ICD(cases_before_first_dx9, 'dx', dx_cases, 'icd9')\n",
    "    cci_df_icd10 = get_cci_with_ICD(cases_before_first_dx10, 'dx', dx_cases, 'icd10')\n",
    "\n",
    "    cases_dx_comorbidities_df = get_merged_cci(cci_df_icd10, cci_df_icd9)\n",
    "\n",
    "    cases_dx_comorbidities_df['target'] = 1\n",
    "\n",
    "\n",
    "    controls_before_index9 = pd.merge(dx_site9, merged_controls_dates[['patid', 'Last_EHR_minus_one_INDEX_DATE']], on='patid', how='right')\n",
    "    controls_before_index10 = pd.merge(dx_site10, merged_controls_dates[['patid', 'Last_EHR_minus_one_INDEX_DATE']], on='patid', how='right')\n",
    "    print('\\n--merge control index and dx_site9 for unique controls:', controls_before_index9.patid.nunique())\n",
    "    print('--merge control index and dx_site10 for unique controls:', controls_before_index10.patid.nunique())\n",
    "\n",
    "    control_cci_df_icd9 = get_cci_with_ICD(controls_before_index9, 'dx', dx_rx_controls, 'icd9', temp=False) \n",
    "    control_cci_df_icd10 = get_cci_with_ICD(controls_before_index10, 'dx', dx_rx_controls, 'icd10', temp=False) \n",
    "\n",
    "    controls_dx_comorbidities_df = get_merged_cci(control_cci_df_icd9, control_cci_df_icd10)\n",
    "\n",
    "    controls_dx_comorbidities_df['target'] = 0\n",
    "\n",
    "    comorbidities = pd.concat([cases_dx_comorbidities_df, controls_dx_comorbidities_df])\n",
    "\n",
    "    comorbidities['comorbidity_score'] = comorbidities['comorbidity_score'].astype('float16')\n",
    "    print('Overall cci :', comorbidities.patid.nunique())\n",
    "\n",
    "    comorbidities = comorbidities[['patid', 'comorbidity_score', 'target']]\n",
    "    return comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_site_process(site):\n",
    "    demo_site, dx_site, rx_site, lab_site, vital_site = get_site_data(site)\n",
    "    rx_site['rxnorm_cui'] = rx_site['rxnorm_cui'].astype('str')\n",
    "    # filter <=1884, >=1year EHR, in dx_site \n",
    "    demo_site, dx_site, rx_site, lab_site, vital_site = get_domains_for_40_persons(demo_site, dx_site, rx_site, lab_site, vital_site)\n",
    "    all_dates = get_all_dates(dx_site, rx_site, lab_site, vital_site)\n",
    "    year1_ehr_persons = all_dates.patid.unique().tolist()\n",
    "    demo_site , dx_site, rx_site, lab_site, vital_site = get_domains_for_1year_EHR_persons(year1_ehr_persons, demo_site , dx_site, rx_site, lab_site, vital_site)\n",
    "    demo_site, dx_site, rx_site, lab_site, vital_site = get_domains_for_dx_persons(demo_site, dx_site, rx_site, lab_site, vital_site)\n",
    "\n",
    "    # define case/control\n",
    "    dx_cases, dx_rx_controls, dx_site_adrd, rx_site_adrd = get_dx_case_control(dx_site, rx_site, ADRD_ICD9, ADRD_ICD10, ADRD_AND_OTHER_ICD9, ADRD_AND_OTHER_ICD10, ANTI_DEMENTIA_RXCUI_list)\n",
    "    demo_site, dx_site, rx_site, lab_site, vital_site = get_domains_for_cases_controls(demo_site, dx_site, rx_site, lab_site, vital_site, dx_cases, dx_rx_controls)\n",
    "    \n",
    "    # process date\n",
    "    merged_cases_compare = get_case_index_and_compare(dx_site_adrd, rx_site_adrd, all_dates, dx_cases)\n",
    "    merged_controls_dates = get_control_dates(all_dates, dx_rx_controls)\n",
    "    comorbidity_scores = get_all_cci(dx_site, merged_cases_compare, merged_controls_dates, dx_cases, dx_rx_controls)\n",
    "    return dx_cases, dx_rx_controls, demo_site, dx_site, rx_site, lab_site, vital_site,merged_cases_compare, merged_controls_dates, comorbidity_scores # list of persons, domain data of cases and controls, index of cases and controls, comorbidity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract ehrs for checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in [ 'wcm', 'columbia', 'montefiore', 'mshs', 'nyu']:\n",
    "    print('Process ', site)\n",
    "    dx_cases, dx_rx_controls, demo_site, dx_site, rx_site, lab_site, vital_site,merged_cases_compare, merged_controls_dates, comorbidity_scores = each_site_process(site)\n",
    "    site_resources = [dx_cases, dx_rx_controls, demo_site, dx_site, rx_site, lab_site, vital_site,merged_cases_compare, merged_controls_dates, comorbidity_scores]\n",
    "    # pickle.dump(site_resources, open( f'./Middle/{site}_site_resources.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "hold_out_portion = 0.5\n",
    "dx_cases_all = []\n",
    "dx_rx_controls_all = []\n",
    "\n",
    "demo_site_all = []\n",
    "dx_site_all = []\n",
    "rx_site_all = []\n",
    "lab_site_all = []\n",
    "vital_site_all = []\n",
    "merged_cases_compare_all = []\n",
    "merged_controls_dates_all = []\n",
    "comorbidity_scores_all = []\n",
    "gc.collect()\n",
    "for site in ['wcm', 'columbia', 'nyu', 'mshs', 'montefiore']:\n",
    "    print('Load from site', site)\n",
    "    dx_cases, dx_rx_controls, demo_site, dx_site, rx_site, lab_site,\\\n",
    "        vital_site, merged_cases_compare, merged_controls_dates, \\\n",
    "            comorbidity_scores = pickle.load(open(f'./Middle/{site}_site_resources.pkl', 'rb'))\n",
    "    print(f'--Include cases from site {site}', len(dx_cases))  \n",
    "    print(f'--Include controls from site {site}', len(dx_rx_controls))    \n",
    "  \n",
    "    dx_cases_all.append(dx_cases)\n",
    "    dx_rx_controls_all.append(dx_rx_controls)\n",
    "    demo_site_all.append(demo_site) \n",
    "    dx_site_all.append(dx_site) \n",
    "    # rx_site_all.append(rx_site) \n",
    "    # lab_site_all.append(lab_site) \n",
    "    # vital_site_all.append(vital_site) \n",
    "    merged_cases_compare_all.append(merged_cases_compare) \n",
    "    merged_controls_dates_all.append(merged_controls_dates) \n",
    "    comorbidity_scores_all.append(comorbidity_scores) \n",
    "    del dx_cases, dx_rx_controls, demo_site, dx_site, rx_site, lab_site,\\\n",
    "        vital_site, merged_cases_compare, merged_controls_dates, \\\n",
    "            comorbidity_scores\n",
    "    print('Collect gabbage')\n",
    "    gcsum =  gc.collect()\n",
    "    print('--gc', gcsum)\n",
    "\n",
    "cases_all = [i for idlist in dx_cases_all for i in idlist]\n",
    "controls_all = [i for idlist in dx_rx_controls_all for i in idlist]\n",
    "\n",
    "print('All cases ', len(cases_all), len(set(cases_all)))\n",
    "print('All control ', len(controls_all), len(set(controls_all)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cases_all, open('./Middle/cases_all.pkl', 'wb'))\n",
    "pickle.dump(controls_all, open('./Middle/controls_all.pkl', 'wb'))\n",
    "\n",
    "merged_cases_compare_all_df = pd.concat(merged_cases_compare_all, axis=0)\n",
    "merged_controls_dates_all_df = pd.concat(merged_controls_dates_all, axis=0)\n",
    "print(\"--Merge cases compare shape: \", merged_cases_compare_all_df.shape)\n",
    "print(\"--Merge controls dates shape: \", merged_controls_dates_all_df.shape)\n",
    "\n",
    "pickle.dump(merged_cases_compare_all_df, open('./Middle/merged_cases_compare_all_df.pkl', 'wb'))\n",
    "pickle.dump(merged_controls_dates_all_df, open('./Middle/merged_controls_dates_all_df.pkl', 'wb'))\n",
    "\n",
    "# comorbidity_scores_all\n",
    "comorbidity_scores_all_df = pd.concat(comorbidity_scores_all, axis=0)\n",
    "pickle.dump(comorbidity_scores_all_df, open('./Middle/comorbidity_scores_all_df.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_cases_compare_all_df= pickle.load(open('./Middle/merged_cases_compare_all_df.pkl', 'rb'))\n",
    "merged_controls_dates_all_df = pickle.load( open('./Middle/merged_controls_dates_all_df.pkl', 'rb'))\n",
    "\n",
    "comorbidity_scores_all_df = pickle.load( open('./Middle/comorbidity_scores_all_df.pkl', 'rb'))\n",
    "\n",
    "cases_all = pickle.load( open('./Middle/cases_all.pkl', 'rb'))\n",
    "controls_all = pickle.load( open('./Middle/controls_all.pkl', 'rb'))\n",
    "\n",
    "demo_site_all_df = pickle.load( open('./MiddleFeatures/demo_site_all_df.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 random split for hold-out testing at 50%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "np.random.seed(52)\n",
    "random.seed(52)\n",
    "from sklearn.utils import check_random_state\n",
    "random_state = check_random_state(52)\n",
    "\n",
    "hold_out_control = np.random.choice(controls_all, int(hold_out_portion * (len(controls_all))), replace=False)\n",
    "hold_out_case = np.random.choice(cases_all, int(hold_out_portion * (len(cases_all))), replace=False)\n",
    "\n",
    "test_control = list(set(controls_all) - set(hold_out_control) )\n",
    "test_case = list(set(cases_all) - set(hold_out_case) )\n",
    "\n",
    "print('Hold out control', len(hold_out_control), 'Hold out case', len(hold_out_case))\n",
    "print('Hold out example', hold_out_control[-2:], hold_out_case[-2:])\n",
    "print('Test control', len(test_control), 'Test case', len(test_case))\n",
    "print( 'Test example', test_control[-2:], test_case[-2:])\n",
    "\n",
    "\n",
    "splits = (hold_out_case, hold_out_control, test_case, test_control)\n",
    "\n",
    "with open(f\"./Middle/splits/data_splits_portion_{str(hold_out_portion).split('.')[-1]}.pkl\", 'wb') as f:\n",
    "    pickle.dump(splits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_portion=0.5\n",
    "with open(f\"./Middle/splits/data_splits_portion_{str(hold_out_portion).split('.')[-1]}.pkl\", 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "hold_out_case, hold_out_control, test_case, test_control = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "matchcv_merged_cases_compare = merged_cases_compare_all_df[merged_cases_compare_all_df['patid'].isin(set(hold_out_case))]\n",
    "matchcv_merged_controls_ages = merged_controls_dates_all_df[merged_controls_dates_all_df['patid'].isin(set(hold_out_control))]\n",
    "print('Split date information of matchcv for: \\n\\tcase', matchcv_merged_cases_compare.shape, '\\n\\tcontrol', matchcv_merged_controls_ages.shape)\n",
    "\n",
    "test_merged_cases_compare = merged_cases_compare_all_df[merged_cases_compare_all_df['patid'].isin(set(test_case))]\n",
    "test_merged_controls_ages = merged_controls_dates_all_df[merged_controls_dates_all_df['patid'].isin(set(test_control))]\n",
    "print('Split date information of test data for: \\n\\tcase:', test_merged_cases_compare.shape, '\\n\\tcontrol', test_merged_controls_ages.shape)\n",
    "\n",
    "matchcv_demo_site_all_cases = demo_site_all_df[demo_site_all_df['patid'].isin(set(hold_out_case))]\n",
    "matchcv_demo_site_all_controls = demo_site_all_df[demo_site_all_df['patid'].isin(set(hold_out_control))]\n",
    "print('Split demo information of matchcv for: \\n\\tcase', matchcv_demo_site_all_cases.shape, '\\n\\tcontrol', matchcv_demo_site_all_controls.shape)\n",
    "\n",
    "comorbidity_scores_all_df = pd.concat(comorbidity_scores_all, axis=0)\n",
    "\n",
    "matchcv_comorbidity_scores_all_cases = comorbidity_scores_all_df[comorbidity_scores_all_df['patid'].isin(set(hold_out_case))]\n",
    "matchcv_comorbidity_scores_all_controls = comorbidity_scores_all_df[comorbidity_scores_all_df['patid'].isin(set(hold_out_control))]\n",
    "print('Split comorbidity information of matchcv for: \\n\\tcase', matchcv_comorbidity_scores_all_cases.shape, '\\n\\tcontrol', matchcv_comorbidity_scores_all_controls.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 get initial match for hold-out cases from hold-out controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def initial_match_and_closer_date(caselist, case_index_date_dict, case_birth_date_dict, pre_sliced_data):\n",
    "    index_limit = 6\n",
    "    birth_limit = 1\n",
    "\n",
    "\n",
    "    def get_possible_controls_and_nearest_date(caseid, col_all_control_info):\n",
    "\n",
    "        filtered_df = col_all_control_info[\n",
    "            (col_all_control_info[\"birth_date\"] >= precomputed_ranges[caseid][2]) &\n",
    "            (col_all_control_info[\"birth_date\"] <=  precomputed_ranges[caseid][3]) &\n",
    "             (col_all_control_info['ALL_AGES'] >=  precomputed_ranges[caseid][0]) & \n",
    "             (col_all_control_info['ALL_AGES'] <=  precomputed_ranges[caseid][1])\n",
    "    ]\n",
    "\n",
    "        filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "        filtered_df['abs_diff'] = (filtered_df['ALL_AGES'] -  precomputed_ranges[caseid][4]).abs()\n",
    "        nearest_dates_df = filtered_df.loc[\n",
    "            filtered_df.groupby('patid')['abs_diff'].idxmin()\n",
    "        ]\n",
    "        nearest_dates_df['caseid'] = caseid\n",
    "        nearest_dates_df = nearest_dates_df[['caseid','patid', 'ALL_AGES']].rename(columns={'patid':'controlid'})\n",
    "        del  filtered_df\n",
    "        return list(nearest_dates_df.itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "    precomputed_ranges = {\n",
    "        caseid: (\n",
    "            case_index_date_dict[caseid] - pd.DateOffset(months=index_limit),\n",
    "            case_index_date_dict[caseid] + pd.DateOffset(months=index_limit),\n",
    "            case_birth_date_dict[caseid] - pd.DateOffset(years=birth_limit),\n",
    "            case_birth_date_dict[caseid] + pd.DateOffset(years=birth_limit),\n",
    "            case_index_date_dict[caseid]\n",
    "        )\n",
    "        for caseid in tqdm(caselist, total=len(caselist))\n",
    "    }\n",
    "    print('Finish case range preparation!')\n",
    "\n",
    "    all_results = []\n",
    "    for caseid in  tqdm(caselist, total=len(caselist)):\n",
    "\n",
    "        # results = get_possible_controls_and_nearest_date(caseid, random.choice(pre_sliced_data) )\n",
    "        results = get_possible_controls_and_nearest_date(caseid, pre_sliced_data)\n",
    "\n",
    "        all_results.append(results)\n",
    "\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_index_date_dict = dict(matchcv_merged_cases_compare[['patid', 'INDEX_DATE']].values)\n",
    "case_birth_date_dict = dict(matchcv_demo_site_all_cases[['patid', 'birth_date']].values)\n",
    "\n",
    "birth_demo = matchcv_demo_site_all_controls[['patid', 'birth_date']]\n",
    "print('--control date for unique controls: ', matchcv_merged_controls_ages.patid.nunique())\n",
    "print('--control demo for unique controls: ', birth_demo.patid.nunique())\n",
    "\n",
    "all_control_info = birth_demo.merge(matchcv_merged_controls_ages, on='patid', how='inner')\n",
    "print('--merge control date with control demo for unique controls: ', all_control_info.patid.nunique())\n",
    "all_control_info = all_control_info[['patid', 'EARLIEST_DATE', 'birth_date', 'ALL_AGES']]\n",
    "all_control_info_explode =  all_control_info.explode('ALL_AGES')\n",
    "print('--exploded to rows: ', all_control_info_explode.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 call the function above to do initial match (case and control have similar birth date and nearby visit date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Finish control dates sampling preparation!')\n",
    "results = initial_match_and_closer_date(hold_out_case, case_index_date_dict, case_birth_date_dict, all_control_info_explode)\n",
    "print('average inital match control per case: ', sum([len(result) for result in results])/len(hold_out_case))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results, open('./Middle/results.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_melt = [tri for result in results for tri in result]\n",
    "case_control_initial = pd.DataFrame(results_melt)\n",
    "case_control_initial.columns=['caseid', 'controlid', 'nearest_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(case_control_initial, open('./Middle/case_control_initial.pkl', 'wb'))\n",
    "print(case_control_initial.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_control_initial = pickle.load( open('./Middle/case_control_initial.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 prepare for PS matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 function and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from random import sample \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def filter_ehr_enough_and_elder_control_with_closer_index(caseid_list, input_case_control_initial, all_control_info, window_length):\n",
    "    print('\\t--case-control-initial shape ', input_case_control_initial.shape)\n",
    "\n",
    "    case_control_initial = input_case_control_initial[input_case_control_initial['caseid'].isin(set(caseid_list))]\n",
    "    control_ehr = case_control_initial.merge(all_control_info[['patid', 'EARLIEST_DATE', 'birth_date']], left_on = 'controlid', right_on='patid', how='inner').drop('patid', axis=1)\n",
    "    print('\\t--merge case-control-initial with all_control_info shape ', control_ehr.shape,  '| unqiue controls:' ,control_ehr.controlid.nunique())\n",
    "\n",
    "    # filter >= length for control\n",
    "    control_ehr_length = control_ehr[control_ehr['EARLIEST_DATE'] +  pd.DateOffset(years=window_length + 1) <= control_ehr['nearest_date']] \n",
    "    print('\\t--filter with length of yeas:', window_length + 1, 'Resulting in',  control_ehr_length.shape,  '| unique control:', control_ehr_length.controlid.nunique())\n",
    "    # display(control_ehr_length.sort_values(by=['birth_date']))\n",
    "\n",
    "    # filter >=50 at index for control \n",
    "    control_ehr_elder = control_ehr_length[control_ehr_length['birth_date'] + pd.DateOffset(years=50) <= control_ehr_length['nearest_date']] \n",
    "    print('\\t--filter with elder 50, Resulting in',  control_ehr_elder.shape, '| unique control:',  control_ehr_elder.controlid.nunique())\n",
    "\n",
    "    # control_ehr_elder = control_ehr_elder[['caseid', 'controlid']].groupby('caseid')['controlid'].apply(list).reset_index()\n",
    "    # case_control_dict = dict(zip(control_ehr_elder['caseid'].values, control_ehr_elder[ 'controlid'].values))\n",
    "    # display(control_ehr_elder)\n",
    "    del control_ehr_length, control_ehr, case_control_initial\n",
    "    print('collect gc:',gc.collect())\n",
    "    return control_ehr_elder \n",
    "\n",
    "\n",
    "class Matcher_PS(object):\n",
    "\n",
    "    def __init__(self,  subfolder='middle_files'):\n",
    "        self.subfolder = subfolder\n",
    "\n",
    "    def get_psm_input(self, corm_score_case, corm_score_control, transformed_demo, initial_match_df):\n",
    "    \n",
    "        def prepare_psm_case_input( yearcase):\n",
    "            \n",
    "            casedemos = transformed_demo[transformed_demo['patid'].isin(yearcase)]\n",
    " \n",
    "            casecorms = corm_score_case[['patid', 'comorbidity_score']]\n",
    "\n",
    "            case_demo_corm = casedemos.merge(casecorms, on='patid', how='left')\n",
    "            assert case_demo_corm.patid.nunique() == casedemos.patid.nunique() \n",
    "\n",
    "            case_demo_corm['is_cases'] = 1\n",
    "            return case_demo_corm\n",
    "        \n",
    "\n",
    "        def prepare_psm_control_input( yearcontrol):\n",
    "            control_demos = transformed_demo[transformed_demo['patid'].isin(yearcontrol)]\n",
    "\n",
    "            control_cormdf = corm_score_control[['patid', 'comorbidity_score']]\n",
    "                \n",
    "            control_corm_demo = control_demos.merge(control_cormdf, on='patid', how='left')\n",
    "            assert control_demos.patid.nunique() == control_corm_demo.patid.nunique() \n",
    "\n",
    "            control_corm_demo['is_cases'] = 0\n",
    "            return control_corm_demo\n",
    "        \n",
    "        year_case = initial_match_df.caseid.unique().tolist()\n",
    "        year_control = initial_match_df.controlid.unique().tolist()\n",
    "\n",
    "        case_psminput = prepare_psm_case_input(year_case)\n",
    "        control_psminput = prepare_psm_control_input( year_control)\n",
    "        \n",
    "        cat_psminput = pd.concat([case_psminput, control_psminput], axis=0)\n",
    "        del case_psminput, control_psminput\n",
    "        return cat_psminput\n",
    "    \n",
    "    \n",
    "    def propensity_score(self, covariate_df, cols):\n",
    "        mod = LogisticRegression(class_weight=\"balanced\")\n",
    "        mod.fit(covariate_df[cols], covariate_df[\"is_cases\"])\n",
    "        covariate_df[\"propensity_score\"] = mod.predict_proba(covariate_df[cols])[:, 1]\n",
    "        return covariate_df\n",
    "    \n",
    "    \n",
    "    def match_case_to_controls_on_propensity_score(self, propensity_df, case_match_control_dict, max_matches, case_index_date_df, prior_cases):\n",
    "\n",
    "        cases_propensity_df = propensity_df[propensity_df[\"is_cases\"]==1].reset_index(drop=True)\n",
    "        controls_propensity_df = propensity_df[propensity_df[\"is_cases\"]==0].reset_index(drop=True)\n",
    "        \n",
    "        num_cases = cases_propensity_df.shape[0]\n",
    "\n",
    "        # case_str_ids = cases_propensity_df['patid'].astype(str).values\n",
    "\n",
    "        controls_id_to_index = dict(zip(controls_propensity_df[\"patid\"], controls_propensity_df.index.values)) # order is right\n",
    "        # map control int to index int\n",
    "\n",
    "        psm_match_dict = {}\n",
    "        marked_as_used = set()\n",
    "        \n",
    "        if prior_cases is not None:\n",
    "            sorted_keys = [key for key, value in sorted(case_match_control_dict.items() , key=lambda item: len(item[1])) if key not in prior_cases]\n",
    "            prior_ids = sorted(prior_cases, key=lambda item: len(case_match_control_dict[item]))\n",
    "\n",
    "            sorted_keys = prior_ids + sorted_keys\n",
    "        else:\n",
    "            sorted_keys = [key for key, value in sorted(case_match_control_dict.items() , key=lambda item: len(item[1]))]\n",
    "        print('sorted keys!')\n",
    "\n",
    "\n",
    "        controls_ps_values = controls_propensity_df['propensity_score'].values\n",
    "\n",
    "        nn = NearestNeighbors(n_neighbors=max_matches)\n",
    "\n",
    "\n",
    "        for counti,  case_str_id in tqdm(enumerate(sorted_keys), total=len(sorted_keys)):\n",
    "            potential_controls = case_match_control_dict[case_str_id] # read id\n",
    "            \n",
    "            if len(potential_controls) <= max_matches: \n",
    "                # print('--- case:', case_str_id)\n",
    "                print('ori potential < max:', case_str_id)\n",
    "\n",
    "\n",
    "            potential_controls = list( set(potential_controls) - marked_as_used)\n",
    "            # [pc for pc in potential_controls if pc not in marked_as_used]\n",
    "\n",
    "            if len(potential_controls) > 0:\n",
    "                potential_control_indices = np.vectorize(controls_id_to_index.get)(potential_controls)\n",
    "                # potential control id to control index, ordered by control propensity df \n",
    "                \n",
    "                if potential_control_indices.shape[0] > max_matches:\n",
    "                    # control_df_potential = controls_propensity_df.iloc[potential_control_indices]  # get by index\n",
    "                    control_df_potential = controls_ps_values[potential_control_indices]  # get by index\n",
    "                    # print(control_df_potential.shape)\n",
    "                    nn.fit(control_df_potential.reshape(control_df_potential.shape[0], -1))\n",
    "                    # nn.fit(control_df_potential['propensity_score'].values.reshape(control_df_potential.shape[0], -1))\n",
    "\n",
    "                    _, matched_controls_indices = nn.kneighbors(cases_propensity_df[cases_propensity_df['patid']== case_str_id]['propensity_score'].values.reshape( -1, 1))\n",
    "\n",
    "                    matched_potential_indices = potential_control_indices[matched_controls_indices[0]] # the position in index\n",
    "\n",
    "                    matched_potential_ids = controls_propensity_df.iloc[matched_potential_indices][\"patid\"].astype(str).values\n",
    "\n",
    "                    marked_as_used.update(matched_potential_ids)\n",
    "                \n",
    "                else: \n",
    "                    # print('--- case', case_str_id)\n",
    "                    # print('after removing the used: ', potential_control_indices.shape[0],'| used: ', len(marked_as_used))\n",
    "                    matched_potential_ids = potential_controls\n",
    "                    marked_as_used.update(matched_potential_ids)\n",
    "                psm_match_dict[case_str_id] = matched_potential_ids\n",
    "\n",
    "            else:\n",
    "                 psm_match_dict[case_str_id] = []\n",
    "\n",
    "            if counti % 5000 ==0:\n",
    "                gcsum = gc.collect()\n",
    "                print('Colelct', gcsum)\n",
    "\n",
    "\n",
    "        psm_match_df = pd.DataFrame.from_dict(psm_match_dict, orient=\"index\", columns=[f'psm_control_{str(i)}' for i in range(1, max_matches + 1)])\n",
    "        psm_match_df.index.name = \"case_id\"\n",
    "        psm_match_df.sort_index(inplace=True)\n",
    "        \n",
    "        index_dates = case_index_date_df[case_index_date_df[\"patid\"].isin(psm_match_df.index.values.astype(str))]\n",
    "        index_dates.sort_values(\"patid\", inplace=True)\n",
    "\n",
    "        psm_match_df.insert(0, \"case_index_date\", index_dates[\"INDEX_DATE\"].values)\n",
    "\n",
    "        return psm_match_df\n",
    "\n",
    "       \n",
    "    def get_matched_case_control_demo(self, corm_case, corm_control, transformed_demo, initial_match_dict, max_matches, index_case,  prior_cases=None):\n",
    "        # the input should be specific to the year \n",
    "        psm_input = self.get_psm_input(corm_case, corm_control, transformed_demo, initial_match_dict)\n",
    "\n",
    "        colsinput = [ i for i in psm_input.columns if i not in ['is_cases', 'patid'] ]\n",
    "\n",
    "        propensity_pred =  self.propensity_score(psm_input, cols=colsinput)\n",
    "\n",
    "        simple_match_dict = initial_match_dict.groupby(initial_match_dict['caseid'].map(str))['controlid'].apply(list).to_dict()\n",
    "        \n",
    "        propensity_match = self.match_case_to_controls_on_propensity_score\\\n",
    "        (propensity_pred, simple_match_dict, max_matches=max_matches, case_index_date_df=index_case, prior_cases=prior_cases)\n",
    "        \n",
    "\n",
    "        return propensity_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 perform matching at 1:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psm_match(input_case_corm, input_control_corm, democase, democontrol, ratio, initial_match, caseindex, all_control_dates, yearlist=[]):\n",
    "    \n",
    "    def prepare_demo(democase, democontrol):\n",
    "        democ = pd.concat([democase, democontrol], axis=0)\n",
    "        demo = democ.drop_duplicates(inplace=False)\n",
    "        # display(demo)\n",
    "        demo['birth_year'] = demo['birth_date'].dt.year\n",
    "        demo['sex_at_birth'] = demo['sex'].apply(lambda x: x if x in ['F', 'M'] else 'Other/unknown')\n",
    "        demo['race'] = demo['race'].apply\\\n",
    "        (lambda x: x if x in ['03', '02',   '05', '07', '06',   '04', '01'] else 'Other/unknown')\n",
    "        demo['eth'] = demo['hispanic'].apply\\\n",
    "        (lambda x: x if x in ['N', 'Y'] else 'Other/unknown')\n",
    "\n",
    "        demo_persons = demo['patid'].values.tolist()\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), ['birth_year']),\n",
    "            ('cat', OneHotEncoder(drop='first'), ['sex_at_birth', 'race', 'eth'])\n",
    "        ])\n",
    "\n",
    "        ft_demo =  preprocessor.fit_transform(demo)\n",
    "                    \n",
    "        num_features = ['birth_year']\n",
    "        cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(['sex_at_birth', 'race', 'eth'])\n",
    "        all_features = num_features + list(cat_features)\n",
    "        \n",
    "        if hasattr(ft_demo, \"toarray\"):\n",
    "            ft_demo = ft_demo.toarray()\n",
    "            \n",
    "        df_transformed = pd.DataFrame(ft_demo, columns=all_features)\n",
    "        \n",
    "        new_index = pd.Series(demo_persons, name='patid')\n",
    "\n",
    "        df_transformed['patid'] = new_index\n",
    "        \n",
    "        return  df_transformed\n",
    "\n",
    "    \n",
    "    controls_with_cci =  input_control_corm.patid.unique().tolist()\n",
    "    print('--Controls with cci', len(controls_with_cci))\n",
    "    df_transformed = prepare_demo(democase, democontrol) ## all demo of all hold-out individuals\n",
    "    case_with_demo = caseindex.merge(democase, on='patid', how='inner') # all hold-out case with all case index \n",
    "    print('Input case index, ', caseindex.shape, '| case demo', democase.shape, '| case with demo', case_with_demo.shape)\n",
    "    \n",
    "    obs_years_list = yearlist\n",
    "    ps_matcher = Matcher_PS(subfolder='Middle')\n",
    "    psm_match_collection = {}\n",
    "    match_control_index = {}\n",
    "    for year in reversed(obs_years_list):   \n",
    "        print('-------------- Year', year)\n",
    "        print('\\nStart with cases: ', case_with_demo.patid.nunique())\n",
    "\n",
    "        # select proper cases for the window\n",
    "        if year == 0: \n",
    "            case_with_demo_ehr = case_with_demo[case_with_demo['EARLIEST_DATE'] + pd.DateOffset(days=1 )<= case_with_demo['INDEX_DATE']]\n",
    "            print('--Case_with ehr length day >= ',  1, '| unique: ', case_with_demo_ehr.patid.nunique() )\n",
    "        else:\n",
    "            case_with_demo_ehr = case_with_demo[case_with_demo['EARLIEST_DATE'] + pd.DateOffset(years=year +1 )<= case_with_demo['INDEX_DATE']]\n",
    "            print('--Case_with ehr length year >= ', year + 1, '| unique: ', case_with_demo_ehr.patid.nunique() )\n",
    "\n",
    "        case_with_demo_elder = case_with_demo_ehr[case_with_demo_ehr['birth_date'] + pd.DateOffset(years=50)<= case_with_demo_ehr['INDEX_DATE']]\n",
    "        case_year =  case_with_demo_elder['patid'].unique().tolist()       \n",
    "        print('--Case_with index age >= 50', '| unique: ', len(case_year))\n",
    "\n",
    "\n",
    "        print('\\nStart with initial match rows', initial_match.shape )\n",
    "        initial_match_1 = initial_match[initial_match['caseid'].isin(set(case_year))]\n",
    "        print('--pick initial match rows within cases of the year', initial_match_1.shape)\n",
    "\n",
    "        filter_initial_match = filter_ehr_enough_and_elder_control_with_closer_index(case_year, initial_match_1, all_control_dates, year)\n",
    "        print('--pick initial match rows within control having enough EHR and elder, drop ', initial_match_1.shape[0]-filter_initial_match.shape[0])\n",
    "\n",
    "        cci_initial_match_filter = filter_initial_match[filter_initial_match['controlid'].isin(controls_with_cci)]\n",
    "        print('--pick initial match rows within control having cci, drop ', filter_initial_match.shape[0] - cci_initial_match_filter.shape[0])\n",
    "\n",
    "        match_control_index['match_year{}'.format(year)] = cci_initial_match_filter[['caseid',\t'controlid',\t'nearest_date']]\n",
    "\n",
    "\n",
    "        control_year = cci_initial_match_filter.controlid.unique().tolist()\n",
    "        case_year = cci_initial_match_filter.caseid.unique().tolist()\n",
    "\n",
    "        corm_case_year = input_case_corm[input_case_corm['patid'].isin(case_year)]\n",
    "        corm_control_year = input_control_corm[input_control_corm['patid'].isin(control_year)]\n",
    "        print('\\nPick corm rows for cases and controls of the year', corm_case_year.shape ,corm_control_year.shape )\n",
    "\n",
    "        print('\\nCase of the year: ', len(case_year), '| Control of the year', len(control_year))\n",
    "\n",
    "        _psm_control_case_df = ps_matcher.get_matched_case_control_demo\\\n",
    "        (corm_case_year, corm_control_year, df_transformed, cci_initial_match_filter, max_matches=ratio,\\\n",
    "         index_case=caseindex)\n",
    "        \n",
    "        psm_match_collection['match_year{}'.format(year)] = _psm_control_case_df\n",
    "\n",
    "        display(_psm_control_case_df.isnull().sum())\n",
    "    \n",
    "    return psm_match_collection, match_control_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "psm1, match_control_index1 = get_psm_match(matchcv_comorbidity_scores_all_cases, matchcv_comorbidity_scores_all_controls, matchcv_demo_site_all_cases,\\\n",
    "                     matchcv_demo_site_all_controls, 10, case_control_initial, matchcv_merged_cases_compare[['patid', 'EARLIEST_DATE','INDEX_DATE']], \\\n",
    "                        all_control_info[['patid'\t,'EARLIEST_DATE',\t'birth_date']], yearlist=[0, 1, 2, 5, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(psm1 , open( './PSM_results/years_psm_ratio10.pkl', 'wb'))\n",
    "pickle.dump(match_control_index1 , open( './PSM_results/years_match_control_index.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is Done for PSM matching!\n",
    "\n",
    "below compute control index date, save to psm_match dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_control_index1 = pickle.load(  open( './PSM_results/years_match_control_index.pkl', 'rb'))\n",
    "\n",
    "psm1 = pickle.load( open( './PSM_results/years_psm_ratio10.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_control_index(row, control_col, _nearest_age_dict):\n",
    "    key = (row.name, row[control_col])\n",
    "    value = _nearest_age_dict.get(key, None)\n",
    "    # print(key)\n",
    "    return value\n",
    "psm_match_years_with_control_index = {}\n",
    "\n",
    "for year, psm_match in psm1.items():\n",
    "    print(year)\n",
    "    psm_match_copy = psm_match.copy()\n",
    "\n",
    "    ori_cols =  psm_match_copy.columns\n",
    "\n",
    "    controlindexyear = match_control_index1[year]\n",
    "\n",
    "    dict_date = dict(zip(zip(controlindexyear['caseid'], controlindexyear['controlid']), controlindexyear['nearest_date']))\n",
    "    print('dict obtained: ', len(dict_date))\n",
    "\n",
    "    \n",
    "    for control_col in [c  for c in ori_cols if 'psm_control_' in c]: \n",
    "        print('--', control_col)\n",
    "        decide_control_index = psm_match_copy.apply(lambda row: get_control_index(row, control_col, dict_date), axis=1)\n",
    "\n",
    "        psm_match_copy[control_col + '_index'] = decide_control_index\n",
    "\n",
    "    psm_match_years_with_control_index[year] = psm_match_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(psm_match_years_with_control_index, open( './PSM_results/years_psm_ratio10_index_date.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrdPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

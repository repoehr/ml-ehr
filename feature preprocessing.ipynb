{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import dill\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from lightgbm import LGBMClassifier\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "dx_cases_all = []\n",
    "dx_rx_controls_all = []\n",
    "\n",
    "demo_site_all = []\n",
    "dx_site_all = []\n",
    "rx_site_all = []\n",
    "lab_site_all = []\n",
    "vital_site_all = []\n",
    "\n",
    "for site in ['wcm', 'columbia', 'nyu', 'mshs', 'montefiore']:\n",
    "    print('Load from site', site)\n",
    "    dx_cases, dx_rx_controls, demo_site, dx_site, rx_site, lab_site,\\\n",
    "        vital_site, merged_cases_compare, merged_controls_dates, \\\n",
    "            comorbidity_scores = pickle.load(open(f'./Middle/{site}_site_resources.pkl', 'rb'))\n",
    "    print(f'--Include cases from site {site}', len(dx_cases))  \n",
    "    print(f'--Include controls from site {site}', len(dx_rx_controls))    \n",
    "  \n",
    "    dx_cases_all.append(dx_cases)\n",
    "    dx_rx_controls_all.append(dx_rx_controls)\n",
    "    \n",
    "    demo_site_all.append(demo_site) \n",
    "    dx_site_all.append(dx_site) \n",
    "    rx_site_all.append(rx_site) \n",
    "    lab_site_all.append(lab_site) \n",
    "    vital_site_all.append(vital_site) \n",
    "    \n",
    "\n",
    "    del dx_cases, dx_rx_controls, demo_site, dx_site, rx_site, lab_site,\\\n",
    "        vital_site, merged_cases_compare, merged_controls_dates, \\\n",
    "            comorbidity_scores\n",
    "    gc.collect()\n",
    "    \n",
    "cases_all = [i for idlist in dx_cases_all for i in idlist]\n",
    "controls_all = [i for idlist in dx_rx_controls_all for i in idlist]\n",
    "\n",
    "print('All cases ', len(cases_all), len(set(cases_all)))\n",
    "print('All control ', len(controls_all), len(set(controls_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_site_all_df = pd.concat(demo_site_all, axis=0)\n",
    "dx_site_all_df = pd.concat(dx_site_all, axis=0)\n",
    "rx_site_all_df = pd.concat(rx_site_all, axis=0)\n",
    "lab_site_all_df = pd.concat(lab_site_all, axis=0)\n",
    "vital_site_all_df = pd.concat(vital_site_all, axis=0)\n",
    "\n",
    "\n",
    "del demo_site_all, dx_site_all, rx_site_all, lab_site_all, vital_site_all\n",
    "print('DF for domains demo: ', demo_site_all_df.shape, '\\n\\t', demo_site_all_df.patid.nunique())\n",
    "print('DF for domains dx: ' , dx_site_all_df.shape, '\\n\\t', dx_site_all_df.patid.nunique())\n",
    "print('DF for domains rx: ', rx_site_all_df.shape, '\\n\\t', rx_site_all_df.patid.nunique())\n",
    "print('DF for domains lab: ', lab_site_all_df.shape, '\\n\\t', lab_site_all_df.patid.nunique())\n",
    "print('DF for domains vital: ', vital_site_all_df.shape, '\\n\\t', vital_site_all_df.patid.nunique())\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "pickle.dump(demo_site_all_df, open( './MiddleFeatures/demo_site_all_df.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(dx_site_all_df, open( './MiddleFeatures/dx_site_all_df.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(rx_site_all_df, open( './MiddleFeatures/rx_site_all_df.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(lab_site_all_df, open( './MiddleFeatures/lab_site_all_df.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(vital_site_all_df, open( './MiddleFeatures/vital_site_all_df.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 using ICD to phecode mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## phecode \n",
    "to_Phewas = pd.read_csv(f\"./icd2phecode.csv\", sep = ',', dtype={'ICD': str, 'Phecode': str}).rename(columns={'Phecode': 'phecode'})\n",
    "\n",
    "ICD9_to_Phewas = to_Phewas[to_Phewas.Flag == 9]\n",
    "ICD10_to_Phewas = to_Phewas[to_Phewas.Flag == 10]\n",
    "\n",
    "ICD9_to_Phewas = ICD9_to_Phewas[['ICD', 'phecode']]  \n",
    "ICD10_to_Phewas = ICD10_to_Phewas[['ICD', 'phecode']]  \n",
    "\n",
    "\n",
    "ICD9_dict = dict(zip(ICD9_to_Phewas['ICD'], ICD9_to_Phewas['phecode'] ))\n",
    "ICD10_dict = dict(zip(ICD10_to_Phewas['ICD'], ICD10_to_Phewas['phecode'] ))\n",
    "\n",
    "\n",
    "\n",
    "# ======\n",
    "# defining list of ADRD\n",
    "ADRD_dx_med_codes = pd.read_csv(\"./ADRD_dx_med_codes.csv\")\n",
    "ADRD_dx_med_codes.loc[(ADRD_dx_med_codes['Description']==\"Pick's disease\") & (ADRD_dx_med_codes['Code']==\"33111\"), 'Code'] = '331.11'\n",
    "\n",
    "ADRD_STRINGS = [\"Alzheimer's disease\", \"Vascular dementia\", \"Frontotemporal dementia\", \"Lewy Body Dementia\"]\n",
    "ADRD_STRINGS = '|'.join(ADRD_STRINGS)\n",
    "\n",
    "ADRD_ICD9 = ADRD_dx_med_codes[(ADRD_dx_med_codes['Code_type'] == 'ICD-9') & ADRD_dx_med_codes['Concept'].str.contains(ADRD_STRINGS)] \n",
    "ADRD_ICD10 = ADRD_dx_med_codes[(ADRD_dx_med_codes['Code_type'] == 'ICD-10') & ADRD_dx_med_codes['Concept'].str.contains(ADRD_STRINGS)] \n",
    "\n",
    "ADRD_ICD9_phecodes = ADRD_ICD9.merge(ICD9_to_Phewas, left_on = 'Code', right_on = 'ICD', how='left'  )['phecode'].tolist()\n",
    "ADRD_ICD10_phecodes = ADRD_ICD10.merge(ICD10_to_Phewas, left_on = 'Code', right_on = 'ICD', how='left'  )['phecode'].tolist()\n",
    "\n",
    "# display(ADRD_ICD10_dict)\n",
    "print('ADRD_ICD9_phecodes: ', ADRD_ICD9_phecodes)\n",
    "print('ADRD_ICD10_phecodes: ', ADRD_ICD10_phecodes)\n",
    "\n",
    "\n",
    "# ======\n",
    "# defining list of ADRD and other dementia\n",
    "ADRD_AND_OTHER_CONDITIONS = ADRD_STRINGS +'|'+ '|'.join([\"Dementia\", \"Conditions cause dementia\"])\n",
    "\n",
    "ADRD_AND_OTHER_ICD9 = ADRD_dx_med_codes[(ADRD_dx_med_codes['Code_type'] == 'ICD-9') & ADRD_dx_med_codes['Concept'].str.contains(ADRD_AND_OTHER_CONDITIONS)] \n",
    "ADRD_AND_OTHER_ICD10 = ADRD_dx_med_codes[(ADRD_dx_med_codes['Code_type'] == 'ICD-10') & ADRD_dx_med_codes['Concept'].str.contains(ADRD_AND_OTHER_CONDITIONS)] \n",
    "\n",
    "ADRD_AND_OTHER_ICD9_PHECODES = ADRD_AND_OTHER_ICD9.merge(ICD9_to_Phewas, left_on = 'Code', right_on = 'ICD', how='left'  )['phecode'].tolist()\n",
    "ADRD_AND_OTHER_ICD10_PHECODES = ADRD_AND_OTHER_ICD10.merge(ICD10_to_Phewas, left_on = 'Code', right_on = 'ICD', how='left'  )['phecode'].tolist()\n",
    "\n",
    "print('ADRD_AND_OTHER_ICD9_PHECODES: ', ADRD_AND_OTHER_ICD9_PHECODES)\n",
    "print('ADRD_AND_OTHER_ICD10_PHECODES: ', ADRD_AND_OTHER_ICD10_PHECODES)\n",
    "\n",
    "\n",
    "\n",
    "# ======\n",
    "# defining list of medication rxcui\n",
    "ANTI_DEMENTIA_RXCUI = ADRD_dx_med_codes[(ADRD_dx_med_codes['Concept'] == \"Anti-dementia medications\") & (ADRD_dx_med_codes['Code_type'] == 'RXCUI')].Code.reset_index(drop=True)\n",
    "\n",
    "ANTI_DEMENTIA_RXCUI_list = ANTI_DEMENTIA_RXCUI.tolist()\n",
    "print('ANTI_DEMENTIA_RXCUI_list:', ANTI_DEMENTIA_RXCUI_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dx to phecode \n",
    "dx_enc_df9 = dx_site_all_df[dx_site_all_df.dx_type==9]\n",
    "dx_enc_df10 = dx_site_all_df[dx_site_all_df.dx_type==10]\n",
    "print('Get df9 ', dx_enc_df9.shape)\n",
    "print('Get df10 ', dx_enc_df10.shape)\n",
    "\n",
    "dx_enc_df9['phecode'] = dx_enc_df9['dx'].map(ICD9_dict ) \n",
    "dx_enc_df10['phecode'] = dx_enc_df10['dx'].map(ICD10_dict ) \n",
    "print('Map 9 to phecode: ', dx_enc_df9.shape)\n",
    "print('Map 10 to phecode: ', dx_enc_df10.shape)\n",
    "\n",
    "print(f\"-We have non-na {dx_enc_df9[~dx_enc_df9.phecode.isna()].shape[0]} phecode Rows in dx9\")\n",
    "print(f\"-We have non-na {dx_enc_df10[~dx_enc_df10.phecode.isna()].shape[0]} phecode Rows in dx10\")\n",
    "\n",
    "\n",
    "dx_enc_phe = pd.concat([dx_enc_df9, dx_enc_df10], axis=0)\n",
    "print('--Mapping dx_enc_df to phecode based on ICD9 and ICD10 columns', dx_enc_phe.shape)\n",
    "print(f\"--{round(100*(dx_enc_phe['phecode'].isna().sum() / dx_enc_phe.shape[0]), 2)}% of encounters have finally a null Phecode\")\n",
    "print(f\"--We have {dx_enc_phe[~dx_enc_phe.phecode.isna()].phecode.nunique()} unique Phecodes\")\n",
    "\n",
    "\n",
    "print('Before dropping null phecode', dx_enc_phe.shape)\n",
    "dx_enc_phe = dx_enc_phe[~dx_enc_phe['phecode'].isna()] \n",
    "print('Drop null phecode, finally', dx_enc_phe.shape)  \n",
    "\n",
    "dx_enc_phe = dx_enc_phe.drop_duplicates()\n",
    "print('Drop duplicates, finally', dx_enc_phe.shape)  \n",
    "\n",
    "del dx_enc_df9, dx_enc_df10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(dx_enc_phe, open( './MiddleFeatures/processed_dx_enc_phe.pkl', 'wb'))\n",
    "del dx_enc_phe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dx_enc_phe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proprocess medication data, RXCUI to ingredient level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function\n",
    "def get_med_ingredients_multi_aou(rxcui_list, ohsu_ing_dict, existing_dict):\n",
    "    NaN_counts = 0\n",
    "    ohsu_ing_dict = {}\n",
    "    nan_list = []\n",
    "    \n",
    "    for rxcui in tqdm(rxcui_list):\n",
    "        if rxcui in existing_dict:\n",
    "            continue\n",
    "        try:\n",
    "            rxcui_ingredient = urllib.request.urlopen(f\"https://rxnav.nlm.nih.gov/REST/rxcui/{rxcui}/related.json?tty=IN\").read().decode()\n",
    "            rxcui_ingredient = json.loads(rxcui_ingredient)\n",
    "            rxcui_ingredient = [item['rxcui'] for item in rxcui_ingredient['relatedGroup']['conceptGroup'][0]['conceptProperties']]\n",
    "        except:\n",
    "            ohsu_ing_dict[rxcui] = np.nan\n",
    "#                 print('nan ing', rxcui)\n",
    "            NaN_counts+=1\n",
    "            nan_list.append(rxcui)\n",
    "            continue\n",
    "        ohsu_ing_dict[rxcui] = rxcui_ingredient\n",
    "    return ohsu_ing_dict, NaN_counts\n",
    "\n",
    "\n",
    "if 'rxcui_ing' not in rx_site_all_df.columns:\n",
    "\n",
    "    exsiting_ing_dict = pickle.load(open('ing_dict_multi.pkl', 'rb'))\n",
    "\n",
    "    int_exsiting_ing_dict = {}\n",
    "    for k,v in exsiting_ing_dict.items():\n",
    "        try:\n",
    "            intk = int(k)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        int_exsiting_ing_dict[intk] =  v\n",
    "\n",
    "    print('\\nExisting ing dict from ohsu and all of us: ', len(exsiting_ing_dict))\n",
    "\n",
    "    print('--Convert from existing dictionary to int dictionary: ', len(int_exsiting_ing_dict))\n",
    "    rx_site_all_df['rxnorm_cui'] = rx_site_all_df['rxnorm_cui'].astype(int)\n",
    "\n",
    "    unique_rxcui = rx_site_all_df.rxnorm_cui.unique().tolist()\n",
    "    insight_ing_dict = {}\n",
    "\n",
    "    # print(type(unique_rxcui[0]))\n",
    "    insight_ing_dict, na_counts = get_med_ingredients_multi_aou(unique_rxcui, insight_ing_dict, int_exsiting_ing_dict)\n",
    "\n",
    "    print('--get insight_ing dict / New na_counts', len(insight_ing_dict), na_counts)\n",
    "\n",
    "    insight_ing_dict = insight_ing_dict | int_exsiting_ing_dict\n",
    "    print('Update to current insight_ing_dict', len(insight_ing_dict))\n",
    "\n",
    "    rx_site_all_df['rxcui_ing'] = rx_site_all_df['rxnorm_cui'].apply(lambda x: insight_ing_dict[x])\n",
    "    print('\\n--Mapped to rxcui ing df', rx_site_all_df.shape)\n",
    "    print(f\"--{round(100*(rx_site_all_df['rxcui_ing'].isna().sum() / rx_site_all_df.shape[0]), 2)}% of encounters have finally a null rxcui_ing\")\n",
    "\n",
    "\n",
    "    rx_site_all_df = rx_site_all_df.dropna(subset=['rxcui_ing'],inplace=False)\n",
    "    print('--removing null rxcui ing, now: ', rx_site_all_df.shape)\n",
    "\n",
    "    rx_site_all_df = rx_site_all_df.explode('rxcui_ing')\n",
    "    rx_site_all_df['rxcui_ing'] = rx_site_all_df['rxcui_ing'].astype(int)\n",
    "    print('--exploded to', rx_site_all_df.shape)\n",
    "    rx_site_all_df = rx_site_all_df.dropna(subset=['rxcui_ing'],inplace=False)\n",
    "    print('----removing null rxcui ing, now:', rx_site_all_df.shape)\n",
    "    rx_site_all_df = rx_site_all_df.drop_duplicates()\n",
    "    print('----removing duplicates, now:', rx_site_all_df.shape)\n",
    "\n",
    "    print(f\"We have {rx_site_all_df.rxcui_ing.nunique()} unique rxcui_ing\")\n",
    "\n",
    "else:\n",
    "    print('Already having this ingredient column.')\n",
    "\n",
    "\n",
    "# rx_site_all_df = rx_site_all_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rx_site_all_df, open( './MiddleFeatures/processed_rx_ing.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rx_site_all_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lab_site_all_df.head())\n",
    "lab_site_all_df.groupby('lab_loinc')['result_unit'].apply(set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_codes_df = pd.read_excel('Collected_lab_tests_for_ADRD_evaluation_M_online.xlsx', skiprows=4)\n",
    "c_codes_df.columns = ['Class',\t'Code',\t'Name'\t,'Acceptable Sample Source'\t,'Source Suitable',\t'Rangelow',\t'Rangehigh',\t'Unit of Ranges',\t'Range Specs',\t'Range Applicable',\t'Range reference', ' Corresponding test in paper 1', 'Note']\n",
    "c_codes = c_codes_df['Code'].tolist() # it is str\n",
    "\n",
    "print(f'There are {c_codes_df.Class.nunique()} unique lab test names in the codes')\n",
    "print(f'There are {c_codes_df.Code.nunique()} unique lab test loincs in the codes')\n",
    "\n",
    "c_codes_df.loc[:, 'Class'] = c_codes_df['Class'].str.lower()\n",
    "c_codes_df.loc[:, 'Unit of Ranges'] = c_codes_df['Unit of Ranges'].str.lower()\n",
    "display(c_codes_df.head(1))\n",
    "# for i in range(c_codes_df.shape[0]):\n",
    "#     display(c_codes_df[i:i+1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merge_chunks = []\n",
    "chunk_size = 5000000\n",
    "removelist = ['ni', 'ot', 'nan']\n",
    "\n",
    "for start in tqdm(range(0, len(lab_site_all_df), chunk_size)):\n",
    "    print('Chunk:', start)\n",
    "\n",
    "    end = min(start + chunk_size, len(lab_site_all_df))\n",
    "    chunk = lab_site_all_df.iloc[start:end]\n",
    "    chunk['result_unit'] = chunk['result_unit'].str.lower()\n",
    "    print('to lower case')\n",
    "\n",
    "    print('--bef: ', chunk.shape)\n",
    "\n",
    "    chunk = chunk[~chunk['result_unit'].isin(set(removelist))]\n",
    "    print('--after removal unselect units: ', chunk.shape)\n",
    " \n",
    "    # chunk = chunk[~chunk['result_unit'].isin(['ni'])]\n",
    "    # print('--after removal unselect units ni: ', chunk.shape)\n",
    "\n",
    "    # chunk = chunk[~chunk['result_unit'].isin(['ot'])]\n",
    "    # print('--after removal unselect units ot: ', chunk.shape)\n",
    "\n",
    "    # chunk = chunk[~chunk['result_unit'].isin(['nan'])]\n",
    "    # print('--after removal unselect units nan: ', chunk.shape)\n",
    "\n",
    "    chunk_lab_merge = chunk.merge(c_codes_df[['Code', 'Class', 'Name', 'Rangelow', 'Rangehigh', 'Unit of Ranges',]], left_on='lab_loinc', right_on = 'Code', how='left').drop('Code', axis=1)\n",
    "\n",
    "    merge_chunks.append(chunk_lab_merge)\n",
    "    del chunk\n",
    "\n",
    "# chunk_lab_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_merge = pd.concat(merge_chunks, axis=0)\n",
    "lab_merge.loc[:, 'Class'] = lab_merge['Class'].str.strip().str.lower()\n",
    "lab_merge = lab_merge.drop(['norm_range_low', 'norm_range_high', 'Name'], axis=1)\n",
    "pickle.dump(lab_merge, open('MiddleFeatures/lab_merge.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_merge = pickle.load(open('MiddleFeatures/lab_merge.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use unit conversion if needed\n",
    "\n",
    "conversion_factors_dict_update3 = {\n",
    "# \"alanine aminotransferase [alt]\": {\n",
    "#     \"[iu]/l\": 1, \"[u]/l\": 1, \"iu/l\": 1,  \"u/l\":1, \"range_unit\": \"u/l\"\n",
    "# },\n",
    "\"aspartate aminotransferase [ast]\": {\n",
    "    \"[iu]/l\": 1, \"[u]/l\": 1, \"iu/l\": 1,  \"u/l\":1,  \"range_unit\": \"u/l\"\n",
    "},\n",
    "\"c reactive protein\": {\n",
    "    \"mg/dl\": 1, \"mg/l\": 0.1, \"range_unit\": \"mg/dl\"   \n",
    "},# 0.1 \n",
    "\"free t3, serum\": {\n",
    "      \"ng/l\": 1,   \"pg/ml\": 1, \"range_unit\": \"ng/dl\"   \n",
    "}, # no need of unit conversion due to record error\n",
    "\"glucose\": {\n",
    "    \"mg/dl\": 0.0555, \"mmol/l\":1, \"range_unit\": \"mmol/l\"   \n",
    "}, # need to convert to mmol/l need to convert to range_unit\n",
    "\"hemoglobin\": {\n",
    "    \"g/l\": 0.1, \"g/dl{calc}\": 1, \"g/dl\": 1, \"range_unit\": \"g/dl\"   \n",
    "}, # check\n",
    "\"high-sensitivity c-reactive protein\": {\n",
    "    \"mg/dl\": 10, \"mg/l\": 1, \"range_unit\": \"mg/l\"  \n",
    "}, # seems right conversion\n",
    "\"immature granulocytes, ig\": {\n",
    "    \"10*3/ul\": 1,  \"k/ul\": 1,  \"10*3/mm3\": 1, \"/ul\": 0.001, \"range_unit\": \"k/ul\"  # nL 转换为 k/uL\n",
    "}, # \n",
    "\n",
    "\"lymphocytes\": {\n",
    "    \"10*3/ul\": 1, \"10*9/l\": 1,  \"k/ul\": 1,  \\\n",
    "        \"k/mm3\": 1, \"10*3/mm3\": 1, \"{cells}/ul\": 0.001, \"/ul\": 0.001, \"range_unit\": \"k/ul\"  # nL 转换为 k/uL\n",
    "},# checked\n",
    "\"platelet count\": {\n",
    "    \"10*3/ul\": 1,  \"k/ul\" :1, \"range_unit\": \"k/ul\"  \n",
    "},\n",
    "\"rbc count\": {\n",
    "     \"10*6/ul\": 1, \"10*3/mm3\": 1, \"m/ul\": 1, \"million/ul\":1, \"range_unit\": \"million/ul\"  # nL 转换为 million/uL\n",
    "},# # no need of unit conversion due to record error\n",
    "\"sedimentation rate\": {\n",
    "    \"mm/h\": 1, \"range_unit\": \"mm/h\",\n",
    "},\n",
    "\"serum albumin\": {\n",
    "    \"g/dl{calc}\": 1, \"g/dl\": 1, \"ug/mg\": 0.0001,  \"g/l\": 0.1, \"range_unit\": \"g/dl\" \n",
    "}, # check record wrong  0.0001, 0.01 # only convert part\n",
    "\"serum alkaline phosphatase\": {\n",
    "   \"[u]/l\": 1, \"iu/l\": 1, \"u/l\":1, \"range_unit\": \"u/l\"  \n",
    "},  \n",
    "# \"serum calcium\": {\n",
    "#     \"mg/dl\": 1, \"range_unit\": \"mg/dl\"\n",
    "# },\n",
    "\"serum calcium, ionized\": {\n",
    "    \"mmol/l\": 1, \"mg/dl\": 0.25, \"range_unit\": \"mmol/l\"   \n",
    "}, \n",
    "\"serum magnesium\": {\n",
    "    \"10*-3.eq/l\":1.2155, \"mg/dl\": 1, \"range_unit\": \"mg/dl\"   \n",
    "},\n",
    "\"serum sodium\": {\n",
    "    \"mmol/l\": 1,               \n",
    "    \"10*-6.eq/l\": 1,      \n",
    "    \"mg/dl\": 1,                    \n",
    "    \"range_unit\": \"mmol/l\"\n",
    "}, #no need of unit conversion due to record error\n",
    "\"total white blood cell count\": {\n",
    "    \"10*3/ul\": 1,   \n",
    "    \"k/ul\":1,\n",
    "    \"range_unit\": \"k/ul\"\n",
    "}, \n",
    "\"urine urea nitrogen\": {\n",
    "    \"g/(24.h)\": 1,             \n",
    "    \"mg/dl\": 1,             \n",
    "    \"grams/24h\": 1,  \n",
    "    \"range_unit\": \"grams/24h\"\n",
    "}, #no need of unit conversion due to record error\n",
    "\"red cell distribution width\":{'%':1}  \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_gender_specific_range(range_str, gender):\n",
    "    # if range_str is None:\n",
    "    #     return np.nan\n",
    "    # if  '/' in range_str:\n",
    "    # if isinstance(range_str, str):\n",
    "    try:\n",
    "        parts = dict(item.split(':') for item in range_str.split('/'))\n",
    "\n",
    "        range_f = float(parts.get(gender[0], np.nan))\n",
    "        return range_f\n",
    "    except:\n",
    "        return np.nan\n",
    "    # else:\n",
    "    #     return float(range_str)\n",
    "\n",
    "cfactorlist = set(conversion_factors_dict_update3.keys())\n",
    "\n",
    "def evaluate_lab_values(df, conversion_factors_dict, gender_context):\n",
    "    chunk_size = 500000\n",
    "    \n",
    "    processed_chunks = []\n",
    "\n",
    "    for start in tqdm(range(0, len(df), chunk_size)):\n",
    "        print('------ Chunk', start)\n",
    "\n",
    "\n",
    "        end = min(start + chunk_size, len(df))\n",
    "        chunk = df.iloc[start:end] \n",
    "        # chunk.loc[:, 'Class'] = chunk['Class'].str.strip()\n",
    "            \n",
    "        chunk['cfactor'] = 1.0\n",
    "        mask = chunk.Class.isin(cfactorlist)\n",
    "        chunk.loc[mask, 'cfactor'] = chunk.loc[mask].apply(\n",
    "            lambda row: conversion_factors_dict.get(row['Class'], {}).get(row['result_unit'], 1), axis=1\n",
    "        )\n",
    "\n",
    "        chunk['Rangelow'] = chunk['Rangelow'].fillna('').astype(str)\n",
    "        chunk['Rangelow_f'] = chunk['Rangelow']\n",
    "        mask = chunk['Rangelow'].str.contains('/', na=False)\n",
    "\n",
    "        chunk.loc[mask, 'Rangelow_f'] = chunk.loc[mask].apply(\n",
    "            lambda row: extract_gender_specific_range(row['Rangelow'], gender_context.get(row['patid'])), axis=1\n",
    "        ) \n",
    "        chunk.drop('Rangelow', axis=1, inplace=True)\n",
    "        chunk['Rangelow_f'] = pd.to_numeric(chunk['Rangelow_f'], errors='coerce')\n",
    "\n",
    "        chunk['Rangehigh'] = chunk['Rangehigh'].fillna('').astype(str)\n",
    "\n",
    "        chunk['Rangehigh_f'] = chunk['Rangehigh']\n",
    "        mask2 = chunk['Rangehigh'].str.contains('/' , na=False)\n",
    "\n",
    "        chunk.loc[mask2, 'Rangehigh_f'] = chunk.loc[mask2].apply(\n",
    "            lambda row: extract_gender_specific_range(row['Rangehigh'], gender_context.get(row['patid'])), axis=1\n",
    "        ) \n",
    "        chunk.drop('Rangehigh',axis=1, inplace=True)\n",
    "        chunk['Rangehigh_f'] = pd.to_numeric(chunk['Rangehigh_f'], errors='coerce')\n",
    "\n",
    "\n",
    "        chunk['cvalue'] =  np.multiply(chunk['result_num'].values,  chunk['cfactor'].values)\n",
    "        chunk.dropna(subset='cvalue', inplace=True)\n",
    "        # display(chunk.dtypes)\n",
    "        conditions = [\n",
    "            (chunk['cvalue'] < chunk['Rangelow_f']),\n",
    "            (chunk['cvalue'] > chunk['Rangehigh_f']),\n",
    "            ((chunk['cvalue'] >= chunk['Rangelow_f']) & (chunk['cvalue'] <= chunk['Rangehigh_f']))\n",
    "        ]\n",
    "        choices = ['ablow', 'abhigh', 'nor']\n",
    "\n",
    "        chunk['flag'] = np.select(conditions, choices, default=None)\n",
    "\n",
    "        print('\\t\\tafter condition matching, nan flag count: ', chunk['flag'].isna().sum())\n",
    "        \n",
    "        chunk.loc[chunk['Rangelow_f'].isnull() & (chunk['cvalue'] > chunk['Rangehigh_f']), 'flag'] = 'abhigh'\n",
    "        chunk.loc[chunk['Rangelow_f'].isnull() & (chunk['cvalue'] <= chunk['Rangehigh_f']), 'flag'] = 'nor'\n",
    "        chunk.loc[chunk['Rangehigh_f'].isnull() & (chunk['cvalue'] < chunk['Rangelow_f']), 'flag'] = 'ablow'\n",
    "        chunk.loc[chunk['Rangehigh_f'].isnull() & (chunk['cvalue'] >= chunk['Rangelow_f']), 'flag'] = 'nor'\n",
    "        \n",
    "        print('\\t\\tafter adjustment, nan flag count: ', chunk['flag'].isna().sum())\n",
    "\n",
    "        chunk = chunk.dropna(subset=['flag'])\n",
    "\n",
    "        processed_chunks.append(chunk)\n",
    "        if start % chunk_size == 30:\n",
    "            print( gc.collect())\n",
    "    processed_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "    del processed_chunks\n",
    "    return processed_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gender_context = dict(zip(demo_site_all_df['patid'].tolist(), demo_site_all_df['sex'].tolist()))\n",
    "gender_context = {k:v for k, v in gender_context.items() if v in ['F', 'M']}\n",
    "print('Female/male context for patient number: ', len(gender_context))\n",
    "\n",
    "gender_context_ce = {k: ('Female' if v == 'F' else 'Male') for k, v in gender_context.items()}\n",
    "\n",
    "print('Gender context for patient number: ', len(gender_context_ce), gender_context_ce)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "processed_lab =  evaluate_lab_values(lab_merge, conversion_factors_dict_update3, gender_context_ce) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "\n",
    "processed_lab['z_score'] = processed_lab.groupby(['lab_loinc'])['cvalue'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "\n",
    "processed_lab_z = processed_lab[(processed_lab['z_score'] >= -3) & (processed_lab['z_score'] <= 3)]\n",
    "del processed_lab\n",
    "gc.collect()\n",
    "print('--after removing z-score outliers ', processed_lab_z.shape)\n",
    "\n",
    "#processed_lab_z = processed_lab_z.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(processed_lab_z, open( './MiddleFeatures/processed_lab_flag.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "part_processed_lab = processed_lab_z[['patid', 'lab_loinc', 'specimen_date', 'Class', 'cvalue', 'flag' ]]\n",
    "\n",
    "befsize = part_processed_lab.shape[0]\n",
    "part_processed_lab = part_processed_lab.drop_duplicates()\n",
    "del processed_lab_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(part_processed_lab, open( './MiddleFeatures/part_processed_lab_flag.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Vital signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  BMI, SBP and SDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_ht_z = vital_site_all_df[~vital_site_all_df['ht'].isna()]\n",
    "ht_z = stats.zscore(vital_ht_z['ht'] )\n",
    "vital_ht_z = vital_ht_z[(ht_z >= -3) & (ht_z <= 3)]\n",
    "print('remove by zscore')\n",
    "\n",
    "vital_wt_z = vital_site_all_df[~vital_site_all_df['wt'].isna()]\n",
    "wt_z = stats.zscore(vital_wt_z['wt'] )\n",
    "vital_wt_z = vital_wt_z[(wt_z >= -3) & (wt_z <= 3)]\n",
    "print('remove by zscore')\n",
    "\n",
    "\n",
    "vital_dis_z = vital_site_all_df[~vital_site_all_df['diastolic'].isna()]\n",
    "dis_z = stats.zscore(vital_dis_z['diastolic'] )\n",
    "vital_dis_z = vital_dis_z[(dis_z >= -3) & (dis_z <= 3)]\n",
    "print('remove by zscore')\n",
    "\n",
    "\n",
    "vital_sys_z = vital_site_all_df[~vital_site_all_df['systolic'].isna()]\n",
    "sys_z = stats.zscore(vital_sys_z['systolic'] )\n",
    "vital_sys_z = vital_sys_z[(sys_z >= -3) & (sys_z <= 3)]\n",
    "print('remove by zscore')\n",
    "\n",
    "vital_all = pd.concat([vital_ht_z, vital_wt_z, vital_dis_z, vital_sys_z], axis=0)\n",
    "print('--Ori vital data: ', vital_site_all_df.shape)\n",
    "vital_all = vital_all.drop_duplicates()\n",
    "\n",
    "print('--after dropping duplicates and zscore: ', vital_all.shape)\n",
    "print('\\t--dropped duplicates: ', 1 - vital_all.shape[0] / vital_site_all_df.shape[0])\n",
    "\n",
    "befsize = vital_all.shape[0]\n",
    "vital_all = vital_all.dropna(subset=['ht', 'wt','diastolic', 'systolic' ], how='all')\n",
    "\n",
    "print('--after dropping all-nan rows : ', vital_all.shape)\n",
    "print('\\t--dropped duplicates rate: ', 1 - vital_all.shape[0] / befsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vital_all, open( './MiddleFeatures/processed_vital_continuous.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_all = pickle.load( open( './MiddleFeatures/processed_vital_continuous.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_date</th>\n",
       "      <th>ht</th>\n",
       "      <th>wt</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>134628321</td>\n",
       "      <td>2.463501e+07</td>\n",
       "      <td>3.187888e+07</td>\n",
       "      <td>7.811443e+07</td>\n",
       "      <td>7.811443e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019-08-13 14:52:55.797454848</td>\n",
       "      <td>6.549759e+01</td>\n",
       "      <td>6.935339e+02</td>\n",
       "      <td>7.317310e+01</td>\n",
       "      <td>1.270109e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1996-03-26 00:00:00</td>\n",
       "      <td>-6.900000e+01</td>\n",
       "      <td>-8.390000e+02</td>\n",
       "      <td>-2.670000e+02</td>\n",
       "      <td>-1.710000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2018-02-14 00:00:00</td>\n",
       "      <td>6.280000e+01</td>\n",
       "      <td>1.520000e+02</td>\n",
       "      <td>6.500000e+01</td>\n",
       "      <td>1.130000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-04-27 00:00:00</td>\n",
       "      <td>6.500000e+01</td>\n",
       "      <td>1.858000e+02</td>\n",
       "      <td>7.300000e+01</td>\n",
       "      <td>1.250000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2021-12-14 00:00:00</td>\n",
       "      <td>6.800000e+01</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>8.100000e+01</td>\n",
       "      <td>1.400000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-06-28 00:00:00</td>\n",
       "      <td>9.890000e+02</td>\n",
       "      <td>4.537600e+03</td>\n",
       "      <td>9.370000e+03</td>\n",
       "      <td>1.207600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.092819e+00</td>\n",
       "      <td>1.063803e+03</td>\n",
       "      <td>1.365469e+01</td>\n",
       "      <td>2.039898e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        measure_date            ht            wt  \\\n",
       "count                      134628321  2.463501e+07  3.187888e+07   \n",
       "mean   2019-08-13 14:52:55.797454848  6.549759e+01  6.935339e+02   \n",
       "min              1996-03-26 00:00:00 -6.900000e+01 -8.390000e+02   \n",
       "25%              2018-02-14 00:00:00  6.280000e+01  1.520000e+02   \n",
       "50%              2020-04-27 00:00:00  6.500000e+01  1.858000e+02   \n",
       "75%              2021-12-14 00:00:00  6.800000e+01  2.550000e+02   \n",
       "max              2023-06-28 00:00:00  9.890000e+02  4.537600e+03   \n",
       "std                              NaN  5.092819e+00  1.063803e+03   \n",
       "\n",
       "          diastolic      systolic  \n",
       "count  7.811443e+07  7.811443e+07  \n",
       "mean   7.317310e+01  1.270109e+02  \n",
       "min   -2.670000e+02 -1.710000e+02  \n",
       "25%    6.500000e+01  1.130000e+02  \n",
       "50%    7.300000e+01  1.250000e+02  \n",
       "75%    8.100000e+01  1.400000e+02  \n",
       "max    9.370000e+03  1.207600e+04  \n",
       "std    1.365469e+01  2.039898e+01  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital_all.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

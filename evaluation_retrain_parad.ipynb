{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import dill\n",
    "import pickle\n",
    "import warnings\n",
    "import urllib.request\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import patchworklib as pw\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    average_precision_score, precision_recall_curve,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    recall_score, precision_score,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names, but DecisionTreeClassifier was fitted without feature names\")\n",
    "\n",
    "def preprocess(xtrain, xtest ):\n",
    "    cols_to_impute = [col for col in xtrain.columns if  col == \"age_at_prediction_window\"]\n",
    "\n",
    "    if len(cols_to_impute) == 0:\n",
    "        print('\\tpreprocess column', None)\n",
    "        return xtrain, xtest, None\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(xtrain[cols_to_impute])\n",
    "    print('\\tpreprocess column', cols_to_impute)\n",
    "    xtrain[cols_to_impute] = scaler.transform(xtrain[cols_to_impute])\n",
    "    if xtest is not None:\n",
    "        xtest[cols_to_impute] = scaler.transform(xtest[cols_to_impute])\n",
    "\n",
    "    return xtrain, xtest, scaler\n",
    "\n",
    "   \n",
    "def preprocess_scalar(xtrain, xtest, scalar ):\n",
    "    cols_to_impute = [col for col in xtrain.columns if  col == \"age_at_prediction_window\"]\n",
    "\n",
    "    if len(cols_to_impute) == 0:\n",
    "        print('\\tpreprocess column', None)\n",
    "        return xtrain\n",
    "    \n",
    "    xtrain[cols_to_impute] = scalar.transform(xtrain[cols_to_impute])\n",
    "    if xtest is not None:\n",
    "        xtest[cols_to_impute] = scalar.transform(xtest[cols_to_impute])\n",
    "\n",
    "    return xtrain, xtest\n",
    "\n",
    "# metrics of 6 \n",
    "def calculate_metrics(y_true, y_pred_prob, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    avpre = average_precision_score(y_true, y_pred_prob)\n",
    "\n",
    "    # tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    # sensitivity = recall_score(y_true, y_pred)\n",
    "    # specificity = tn / (tn + fp)\n",
    "    # ppv = precision_score(y_true, y_pred)\n",
    "    # npv = tn / (tn + fn)\n",
    "    return auc, avpre, 0.1, 0.1, 0.1, 0.1\n",
    "\n",
    "# ppv  \n",
    "def ppv_sensitivity(specificity_levels, _y_true, _y_pred_proba):\n",
    "    add_sensitivity_results = []\n",
    "    add_ppv_results =  []\n",
    "    add_fpr, add_tpr, add_thresholds = roc_curve(_y_true, _y_pred_proba)\n",
    "    _results = {}\n",
    "    for specificity in specificity_levels:\n",
    "\n",
    "        _threshold_index = np.where(add_fpr <= (1 - specificity))[0][-1]\n",
    "        _threshold = add_thresholds[_threshold_index]\n",
    "\n",
    "        # Sensitivity (True Positive Rate)\n",
    "        _sensitivity = add_tpr[_threshold_index]\n",
    "        add_sensitivity_results.append( _sensitivity)\n",
    "\n",
    "        # Positive Predictive Value (PPV)\n",
    "        _y_pred_binary = (_y_pred_proba >= _threshold).astype(int)\n",
    "        _ppv = precision_score(_y_true, _y_pred_binary)\n",
    "        add_ppv_results.append(_ppv)\n",
    "\n",
    "    # Add metrics to results\n",
    "    _results['Sensitivity'] = add_sensitivity_results\n",
    "    _results['PPV'] = add_ppv_results\n",
    "    return _results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "def evaluate_base_model(X_test, y_test, model, CP_num, prediction_window, N, feature_map, model_name, plot=True):\n",
    "    best_mod = model\n",
    "\n",
    "    predicted_proba = best_mod.predict_proba(X_test)[:, 1]\n",
    "    predicted_labels = best_mod.predict(X_test)\n",
    "\n",
    "    auc, pre,sensitivity , specificity , ppv, npv = calculate_metrics(y_test, predicted_proba, predicted_labels)\n",
    "    \n",
    "    add_results = ppv_sensitivity([0.9, 0.95], y_test, predicted_proba)\n",
    "    sensitivity_90, sensitivity_95 = add_results['Sensitivity']\n",
    "    ppv_90, ppv_95 = add_results['PPV']\n",
    "    \n",
    "    if plot:\n",
    "        print(f\"\\nDisplaying performance for CP {CP_num} with a {prediction_window}-year prediction window:\\n\")\n",
    "\n",
    "        prec, rec, threshold = precision_recall_curve(y_test, predicted_proba)\n",
    "        prc_df = pd.DataFrame({\"Recall\": rec, \"Precision\": prec})\n",
    "        ap_score = average_precision_score(y_test, predicted_proba)\n",
    "        base_ap_score = np.mean(y_test)\n",
    "        prc_plot = (\n",
    "            ggplot(prc_df, aes(\"Recall\", \"Precision\")) + \n",
    "            geom_line(color=\"#3C5488B2\") +\n",
    "            theme_bw() +\n",
    "            theme() +\n",
    "            coord_fixed() +\n",
    "            geom_hline(yintercept=base_ap_score, linetype=\"dashed\") +\n",
    "            labs(title=\"Precision-Recall Curve\") +\n",
    "            annotate(\"text\", x=0.15, y=1, label=f\"AP={ap_score:.2f}\", size=8) +\n",
    "            annotate(\"text\", x=0.3, y=0.95, label=f\"Chance Level AP={base_ap_score:.2f}\", size=8)\n",
    "            )\n",
    "        ax1 = pw.load_ggplot(prc_plot, figsize=(2.5, 2.5))\n",
    "    \n",
    "    \n",
    "        ax2 = pw.Brick(figsize=(2.5, 2.5))\n",
    "        cm = confusion_matrix(y_test, predicted_labels, labels=best_mod.classes_)\n",
    "        sns.heatmap(cm, annot=True, linewidth=1, cmap=\"GnBu\", fmt=\"g\",\n",
    "                    yticklabels=[\"Control\", \"Case\"], xticklabels=[\"Control\", \"Case\"], ax=ax2)\n",
    "        ax2.set_title(\"Confusion Matrix\")\n",
    "        ax2.set_xlabel(\"Predicted Label\")\n",
    "        ax2.set_ylabel(\"True Label\")\n",
    "    \n",
    "        coefs = best_mod.coef_[0]\n",
    "        top_N_feature_index = np.argsort(abs(coefs))[-N:]\n",
    "        top_N_feature_names = X_test.columns[top_N_feature_index]\n",
    "\n",
    "        def get_name(x):\n",
    "            if isinstance(x, str):\n",
    "                return feature_map.get(x.strip(), x)\n",
    "            else:\n",
    "                return feature_map.get(x, x)\n",
    "        top_N_feature_names = pd.Series(top_N_feature_names).apply(get_name)\n",
    "\n",
    "\n",
    "        top_N_coefs_abs = abs(coefs)[top_N_feature_index]\n",
    "        top_N_coefs = coefs[top_N_feature_index]\n",
    "        coef_plot_df = pd.DataFrame({\"feature_name\": top_N_feature_names,\n",
    "                                \"abs_coef\": top_N_coefs_abs,\n",
    "                                \"coef\": top_N_coefs})\n",
    "        coef_plot_df['feature_name'] = pd.Categorical(coef_plot_df['feature_name'], categories=coef_plot_df.sort_values('abs_coef')['feature_name'])\n",
    "\n",
    "        feature_importance_plot = (\n",
    "            ggplot(coef_plot_df, aes(\"feature_name\", \"coef\")) +\n",
    "                geom_bar(stat=\"identity\", fill=\"#91D1C2B2\", color=\"black\") +\n",
    "                coord_flip() +\n",
    "                theme_bw() +\n",
    "                labs(x=\"\", y=\"Feature Coefficients\", title=f\"{model_name} Model (CP {CP_num} with a {prediction_window}-yr prediction window)\")\n",
    "            )\n",
    "        ax3 = pw.load_ggplot(feature_importance_plot, figsize=(5, 4))\n",
    "        ax_all = (ax1 | ax2)/ax3\n",
    "    else:\n",
    "        ax_all = None\n",
    "    results_list = [auc , pre, sensitivity , specificity , ppv, npv , sensitivity_90, sensitivity_95, ppv_90, ppv_95]\n",
    "\n",
    "    return ax_all, results_list\n",
    "\n",
    "\n",
    "def evaluate_ensemble_model( X_test,  y_test, model, CP_num, prediction_window, N, feature_map, model_name='', plot=True, pretrained_model=None, X_test_format=None):\n",
    "    \n",
    "    best_mod = model\n",
    "    X_test_input = X_test.values\n",
    "    if pretrained_model is not None:\n",
    "        X_test_format_input = X_test_format.values\n",
    "\n",
    "        predicted_proba = predict_proba_joint(model, pretrained_model, X_test_input, X_test_format_input )[:, 1]\n",
    "        predicted_labels = predict_joint(model, pretrained_model, X_test_input, X_test_format_input)\n",
    "    else:\n",
    "        # print(X_test_input.shape, best_mod)\n",
    "        predicted_proba = best_mod.predict_proba(X_test_input)\n",
    "        # print(predicted_proba, predicted_proba.shape)\n",
    "        predicted_proba = predicted_proba[:, 1]\n",
    "        # predicted_labels = best_mod.predict( X_test_input)\n",
    "\n",
    "    predicted_labels=None\n",
    "    auc, pre,sensitivity , specificity , ppv, npv = calculate_metrics(y_test, predicted_proba, predicted_labels)\n",
    "#     numbers = [float(\"{:.5f}\".format(num)) for num in [auc, pre,sensitivity, specificity, ppv, npv]]\n",
    "#     print(numbers)\n",
    "    add_results = ppv_sensitivity([0.9, 0.95], y_test, predicted_proba)\n",
    "    sensitivity_90, sensitivity_95 = add_results['Sensitivity']\n",
    "    ppv_90, ppv_95 = add_results['PPV']\n",
    "    plot= False\n",
    "    if plot: # plot prc curve, confusion matrix, feature importance using default method from each type of models itself. Could skip this plot. \n",
    "        print(f\"\\nDisplaying performance for CP {CP_num} with a {prediction_window}-year prediction window:\\n\")\n",
    "\n",
    "        prec, rec, threshold = precision_recall_curve(y_test, predicted_proba)\n",
    "        prc_df = pd.DataFrame({\"Recall\": rec, \"Precision\": prec})\n",
    "        ap_score = average_precision_score(y_test, predicted_proba)\n",
    "        base_ap_score = np.mean(y_test)\n",
    "\n",
    "        prc_plot = (\n",
    "            ggplot(prc_df, aes(\"Recall\", \"Precision\")) + \n",
    "            geom_line(color=\"#3C5488B2\") +\n",
    "            theme_bw() +\n",
    "            theme() +\n",
    "            coord_fixed() +\n",
    "            geom_hline(yintercept=base_ap_score, linetype=\"dashed\") +\n",
    "            labs(title=\"Precision-Recall Curve\") +\n",
    "            annotate(\"text\", x=0.15, y=1, label=f\"AP={ap_score:.2f}\", size=8) +\n",
    "            annotate(\"text\", x=0.3, y=0.95, label=f\"Chance Level AP={base_ap_score:.2f}\", size=8)\n",
    "            )\n",
    "        ax1 = pw.load_ggplot(prc_plot, figsize=(2.5, 2.5))\n",
    "\n",
    "\n",
    "        ax2 = pw.Brick(figsize=(2.5, 2.5))\n",
    "        cm = confusion_matrix(y_test, predicted_labels, labels=best_mod.classes_)\n",
    "        sns.heatmap(cm, annot=True, linewidth=1, cmap=\"GnBu\", fmt=\"g\",\n",
    "                    yticklabels=[\"Control\", \"Case\"], xticklabels=[\"Control\", \"Case\"], ax=ax2)\n",
    "        ax2.set_title(\"Confusion Matrix\")\n",
    "        ax2.set_xlabel(\"Predicted Label\")\n",
    "        ax2.set_ylabel(\"True Label\")\n",
    "\n",
    "        feature_importances = best_mod.feature_importances_\n",
    "        top_N_feature_index = np.argsort(feature_importances)[-N:]\n",
    "        top_N_feature_names = X_test.columns[top_N_feature_index]\n",
    "\n",
    "        def get_name(x):\n",
    "            if isinstance(x, str):\n",
    "                return feature_map.get(x.strip(), x)\n",
    "            else:\n",
    "                return feature_map.get(x, x)\n",
    "\n",
    "        top_N_feature_names = pd.Series(top_N_feature_names).apply(get_name)\n",
    "\n",
    "        top_N_importances = feature_importances[top_N_feature_index]\n",
    "\n",
    "        coef_plot_df = pd.DataFrame({\n",
    "            \"feature_name\": top_N_feature_names,\n",
    "            \"importance\": top_N_importances\n",
    "        })\n",
    "\n",
    "        coef_plot_df['feature_name'] = pd.Categorical(coef_plot_df['feature_name'], categories=coef_plot_df.sort_values('importance')['feature_name'])\n",
    "\n",
    "        feature_importance_plot = (\n",
    "            ggplot(coef_plot_df, aes(\"feature_name\", \"importance\")) +\n",
    "                geom_bar(stat=\"identity\", fill=\"#91D1C2B2\", color=\"black\") +\n",
    "                coord_flip() +\n",
    "                theme_bw() +\n",
    "                labs(x=\"\", y=\"Feature Importance\", title=f\"{model_name} Model (CP {CP_num} with a {prediction_window}-yr prediction window)\")\n",
    "            )\n",
    "        ax3 = pw.load_ggplot(feature_importance_plot, figsize=(5, 4))\n",
    "        ax_all = (ax1 | ax2)/ax3\n",
    "    else:\n",
    "        ax_all = None\n",
    "    results_list = [auc , pre, sensitivity , specificity , ppv, npv , sensitivity_90, sensitivity_95, ppv_90, ppv_95]\n",
    "    print(results_list)\n",
    "    return ax_all , results_list\n",
    "\n",
    "\n",
    "def format_input(_current_feature, prediction_window, f_reference):\n",
    "    cp = 1\n",
    "    try:\n",
    "        saved_model_features = f_reference[f'CP_{cp}_{prediction_window}_yr'].drop('person_id', axis=1).columns\n",
    "    except:\n",
    "        refer_cols = f_reference[f'CP_{cp}_{prediction_window}_yr']\n",
    "\n",
    "        if not isinstance(refer_cols, list):\n",
    "            refer_cols= f_reference[f'CP_{cp}_{prediction_window}_yr'].to_list()\n",
    "        saved_model_features = [i for i in  refer_cols if i != 'person_id']\n",
    "    \n",
    "    current_features = _current_feature.columns\n",
    "    overlapping_cols = set(saved_model_features).intersection(set(current_features))\n",
    "    missing_cols = set(saved_model_features) - set(current_features)\n",
    "    print('Current cols\\t', len(current_features))\n",
    "    print('Reference cols\\t', len(saved_model_features))\n",
    "    print('Overalaping cols\\t', len(overlapping_cols))\n",
    "    print('Missing_cols cols\\t', len(missing_cols), missing_cols)\n",
    "\n",
    "    f_input = _current_feature.reindex(columns=saved_model_features, fill_value=0)\n",
    "    assert 'patid' not in f_input.columns\n",
    "    return f_input\n",
    "\n",
    "# def parallel_forest_predict_proba(forest, X, n_jobs=-1):\n",
    "#     def predict_proba_tree(tree, X_subset):\n",
    "#         leaf_ids = tree.apply(X_subset)\n",
    "        \n",
    "#         leaf_values = tree.tree_.value  # Shape: (n_nodes, n_classes)\n",
    "        \n",
    "#         probas = leaf_values[leaf_ids][:, 0]   \n",
    "        \n",
    "#         probas = probas / probas.sum(axis=1, keepdims=True)\n",
    "#         return probas\n",
    "\n",
    "#     tree_probas = Parallel(n_jobs=n_jobs)(\n",
    "#         delayed(predict_proba_tree)(tree, X) for tree in forest.estimators_\n",
    "#     )\n",
    "#     # print('tree', len(tree_probas))\n",
    "#     # avg_probas = np.mean(tree_probas, axis=0)  # Shape: (n_samples, n_classes)\n",
    "#     all_probas = np.sum(tree_probas, axis=0)  # Shape: (n_samples, n_classes)\n",
    "#     # print(all_probas.shape)\n",
    "#     # avg_probas = all_probas / len(forest.estimators_)\n",
    "#     return all_probas\n",
    "\n",
    "\n",
    "def predict_joint(_model, _loadmodel, input_np, input_np_format):\n",
    "\n",
    "    try:\n",
    "        check_is_fitted(_model, \"estimators_\")\n",
    "        estimators = _model.estimators_\n",
    "    except Exception as e:\n",
    "        print(\"Model is not fitted or estimators_ not initialized:\", e)\n",
    "        return None\n",
    "    \n",
    "    saved_estimators  =  _loadmodel.estimators_ \n",
    "    for tree in saved_estimators:\n",
    "        if not hasattr(tree, \"monotonic_cst\"):\n",
    "            tree.monotonic_cst = None  # Set default value to None\n",
    "\n",
    "    saved_predictions = [ tree.predict(input_np_format) for tree in saved_estimators]\n",
    "    predictions = [tree.predict(input_np) for tree in estimators]\n",
    "    all_predictions = np.array( saved_predictions + predictions)\n",
    "\n",
    "    if all_predictions.dtype != np.int64:\n",
    "        all_predictions = all_predictions.astype(int)\n",
    "\n",
    "    majority_vote = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_predictions)\n",
    "    return majority_vote\n",
    "\n",
    "\n",
    "def evaluate_params(random_params, inner_cv, X_train, y_train, pretrained_model=None, X_train_format=None, random_seed=42):\n",
    "    inner_scores = []\n",
    "    # print('evaluate params...', random_params)\n",
    "    for inner_train_idx, inner_valid_idx in inner_cv.split(X_train, y_train):\n",
    "        X_inner_train, X_inner_valid = (\n",
    "            X_train.iloc[inner_train_idx],\n",
    "            X_train.iloc[inner_valid_idx],\n",
    "        )\n",
    "        y_inner_train, y_inner_valid = (\n",
    "            y_train[inner_train_idx],\n",
    "            y_train[inner_valid_idx],\n",
    "        )\n",
    "\n",
    "        model = RandomForestClassifier(random_state=random_seed, n_jobs=-1, **random_params)\n",
    "        model.fit(X_inner_train, y_inner_train)\n",
    "\n",
    "        X_inner_valid_np = X_inner_valid.values\n",
    "        if pretrained_model is None:\n",
    "            y_pred = model.predict( X_inner_valid_np)\n",
    "        else:\n",
    "            # should not run \n",
    "            X_inner_train_format, X_inner_valid_format = (\n",
    "            X_train_format.iloc[inner_train_idx],\n",
    "            X_train_format.iloc[inner_valid_idx],\n",
    "        )\n",
    "            X_inner_valid_format_np = X_inner_valid_format.values\n",
    "            y_pred = predict_joint(model, pretrained_model, X_inner_valid_np, X_inner_valid_format_np)\n",
    "            \n",
    "            del X_train_format, X_inner_valid_format, X_inner_train_format, X_inner_valid_format_np\n",
    "        \n",
    "        score = f1_score(y_inner_valid, y_pred, average=\"macro\")\n",
    "        # score = roc_auc_score(y_inner_valid, y_pred)\n",
    "        # use fi-score to train the models\n",
    "        inner_scores.append(score)\n",
    "\n",
    "    avg_inner_score = np.mean(inner_scores)\n",
    "    print('Done evaluating params...', random_params)\n",
    "\n",
    "    del X_train,   X_inner_train, X_inner_valid,  y_inner_train, y_inner_valid\n",
    "    \n",
    "    gc.collect()\n",
    "    return avg_inner_score, random_params\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import math\n",
    "\n",
    "\n",
    "def build_RF_model(random_seed):\n",
    "    estimator = RandomForestClassifier(class_weight='balanced', random_state=random_seed, warm_start=False)\n",
    "\n",
    "    max_depth =  [int(x) for x in np.linspace(3, 20, 5, dtype=int)]\n",
    "    # max_depth.append(None)\n",
    "    print('max_depth option:', max_depth)\n",
    "\n",
    "    param_dict = { \n",
    "        \"n_estimators\": [1000, 3000, 5000], \n",
    "        \"max_depth\": max_depth,\n",
    "        \"min_samples_split\": [5,10, 40],\n",
    "        \"max_features\": ['sqrt']\n",
    "    }\n",
    "\n",
    "    return estimator, param_dict\n",
    "\n",
    "\n",
    "def nested_cv_pipeline_parallel(\n",
    "    X_input,\n",
    "    y,\n",
    "    model_type,\n",
    "    cp,\n",
    "    prediction_window,\n",
    "    featuremap,\n",
    "    random_seed=99,\n",
    "    score=None,\n",
    "    pretrained_model=None,\n",
    "    pretrained_scalar=None,\n",
    "    reference_cols=None,\n",
    "):\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "    assert model_type == \"rf\"\n",
    "\n",
    "    if model_type == \"rf\":\n",
    "        _, param_grid = build_RF_model(random_seed)\n",
    "        X = X_input\n",
    "    outer_cv_splits = list(outer_cv.split(X, y))\n",
    "\n",
    "    outer_results = pd.DataFrame(columns=[\"cv\", \"auc\", \"pre\", \"sensitivity\", \"specificity\", \"ppv\", \"npv\", \"sensitivity_90\", \"sensitivity_95\", \"ppv_90\", \"ppv_95\"])\n",
    "    outer_best_models = []\n",
    "    standardizer_models = []\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    set_n_jobs = max(1, math.floor(cores*0.05))\n",
    "    print('set_njobs', set_n_jobs)\n",
    "\n",
    "    \n",
    "    import itertools\n",
    "    all_combi = list(itertools.product(param_grid['n_estimators'], param_grid['max_depth'],param_grid['min_samples_split'],param_grid['max_features'] ))\n",
    "\n",
    "    for cv_, (train_idx, test_idx) in enumerate(outer_cv_splits):\n",
    "        print(f\"---Outer CV Fold {cv_}---\")\n",
    "        # X_format = format_input(X, prediction_window, reference_cols)\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        # X_train_format, X_test_format = X_format.iloc[train_idx], X_format.iloc[test_idx]\n",
    "\n",
    "        X_train, X_test, scalar = preprocess(X_train, X_test)\n",
    "\n",
    "        standardizer_models.append(scalar)\n",
    "        print('Input shape', X_train.shape, X_test.shape)\n",
    "\n",
    "        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "\n",
    "        results = Parallel(n_jobs=1)(\n",
    "            delayed(evaluate_params)(\n",
    "                {'n_estimators': all_combi[er][0],'max_depth': all_combi[er][1], 'min_samples_split': all_combi[er][2], 'max_features': all_combi[er][3]} ,  # Randomly sample parameters\n",
    "                inner_cv,\n",
    "                X_train.copy(),\n",
    "                y_train,\n",
    "                random_seed=random_seed,\n",
    "            )\n",
    "            for er in range(30)  # Test 20 random parameter settings\n",
    "        )\n",
    "\n",
    "        # Extract best parameters and score\n",
    "        best_score, best_params = max(results, key=lambda x: x[0])\n",
    "        print(f\"Best Parameters for Outer Fold {cv_}: {best_params}\")\n",
    "        print(f\"Best Inner CV Score for Outer Fold {cv_}: {best_score:.4f}\")\n",
    "\n",
    "        final_model = RandomForestClassifier(random_state=random_seed, n_jobs=-1, **best_params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        axall, results_list = evaluate_ensemble_model(\n",
    "            X_test,\n",
    "            y_test,\n",
    "            final_model,\n",
    "            cp,\n",
    "            prediction_window,\n",
    "            N=30,\n",
    "            feature_map=featuremap,\n",
    "            model_name=\"Random Forest\",\n",
    "            # pretrained_model=pretrained_model,\n",
    "            # X_test_format=X_test_format,\n",
    "        )\n",
    "        display(axall)\n",
    "        outer_results.loc[cv_] = [int(cv_)] + results_list\n",
    "        outer_best_models.append(final_model)\n",
    "\n",
    "    mean_values = outer_results.mean().tolist()\n",
    "    std_values = outer_results.std().tolist()\n",
    "    outer_results.loc[\"mean\"] = [\"-\"] + mean_values[1:]\n",
    "    outer_results.loc[\"std\"] = [\"-\"] + std_values[1:]\n",
    "    display(outer_results)\n",
    "\n",
    "    return outer_results, outer_best_models, standardizer_models\n",
    "\n",
    "\n",
    "%config Application.warn_no_config=True\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def run_retrain_matched_pipeline(X, y, all_map, model_type, cps=[1], years=[0], score=None, pretrained_model=None, pretrained_scalar=None, reference_cols=None):\n",
    "    cp_year_results = {}\n",
    "    cp_year_models = {}\n",
    "    cp_year_scalars = {}\n",
    "\n",
    "    for cp in cps:\n",
    "        for prediction_window in reversed(years):\n",
    "            start = time.time()\n",
    "            print(f\"\\nRunning pipeline for CP {cp} with {prediction_window}-year prediction window...\\n\")\n",
    "            xinput = X[prediction_window]\n",
    "\n",
    "            if 'patid' in xinput.columns:\n",
    "                print('drop patid')\n",
    "                f_input = xinput.drop('patid', axis=1)\n",
    "\n",
    "                outer_results, outer_best_models, standardizer_models = nested_cv_pipeline_parallel(f_input, y[prediction_window], model_type, cp, prediction_window,all_map,\\\n",
    "                                                                                            score=score, pretrained_model=pretrained_model, pretrained_scalar=pretrained_scalar, \\\n",
    "                                                                                                reference_cols=reference_cols)\n",
    "                cp_year_results[prediction_window] = outer_results\n",
    "                cp_year_models[prediction_window] = outer_best_models\n",
    "                cp_year_scalars[prediction_window] = standardizer_models\n",
    "\n",
    "                print('Time eplapse', time.time() - start )\n",
    "    return cp_year_results, cp_year_models, cp_year_scalars\n",
    " \n",
    "\n",
    "all_map = pickle.load(open('all_map.pkl', 'rb'))\n",
    "for i, v in all_map.items():\n",
    "    all_map[i] = i + ' ' + v\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retrain on matched training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_portion =0.5\n",
    "ratio = 10\n",
    "matched_f = pickle.load( open(f'./MiddleFeatures/demo_matched_fs.pkl', 'rb'))\n",
    "matched_t = pickle.load( open(f'./MiddleFeatures/matched_t_drop_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'rb'))\n",
    "matched_e = pickle.load( open(f'./MiddleFeatures/matched_e_drop_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'rb'))\n",
    " \n",
    "all_map = pickle.load(open('all_map.pkl', 'rb'))\n",
    "for i, v in all_map.items():\n",
    "    all_map[i] = i + ' ' + v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = reversed([ 0,1,2,5,10])\n",
    "import time\n",
    "\n",
    "all_rs_years, all_model_test_years, all_model_scalar_years = {},{},{}\n",
    "for window in windows:\n",
    "    print(window)\n",
    "    all_rs_years[window], all_model_test_years[window], all_model_scalar_years[window]  = run_retrain_matched_pipeline(matched_f, matched_t, all_map, 'rf', cps=[1], years=[window],  score=None,\\\n",
    "                    pretrained_model=None, pretrained_scalar=None, reference_cols=None)\n",
    "    \n",
    "    print('Save all year models together: ', windows)\n",
    "    # save it all\n",
    "    # save it all\n",
    "    pickle.dump(all_model_test_years, open(f'rf_chunks/model_test_rf_all_feature_retrain/demo_all_uncommon_model_test_years_till{window}.pkl', 'wb'))\n",
    "    pickle.dump(all_model_scalar_years, open(f'rf_chunks/model_test_rf_all_feature_retrain/demo_all_uncommon_model_scalar_years_till{window}.pkl', 'wb'))\n",
    "    pickle.dump(all_rs_years, open(f'rf_chunks/model_test_rf_all_feature_retrain/demo_all_uncommon_rs_years_till{window}.pkl', 'wb'))\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hold-out testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "def run_retrain_evaluate_pipeline(X, y, all_map, model_type,  years=[0,1,2,5,10], pre_trained_model=None, score=None, f_reference=None, model_name=None, pre_trained_scalar=None, plot=False, finetuned_scalars=None, finetuned_models=None):\n",
    "    cp_year_results = {}\n",
    "    show_results_dict = {}\n",
    "    shap_years = {}\n",
    "\n",
    "    for cp in [1]:\n",
    "        for prediction_window in reversed(years):\n",
    "            if 1: # no pretrained model needed\n",
    "                cv_results = []\n",
    "                cv_num = 5\n",
    "                xinput = X[prediction_window]\n",
    "                for cv in range(cv_num):  # test each cv from the pre-trained model or only test a part of cvs\n",
    "                    print('CV: ', cv, '| Prediction window: ', prediction_window, '| Model type: ', model_type)\n",
    "                    if cv_num > 1: \n",
    "                        # model = pre_trained_model[prediction_window][cv]\n",
    "                        # pretrained_scalar = scalars[prediction_window][cv]\n",
    "                        # no pretrained model\n",
    "                        scalar = finetuned_scalars[prediction_window][cv]\n",
    "                        model = finetuned_models[prediction_window][cv]\n",
    "\n",
    "\n",
    "                    f_input = xinput.drop('patid', axis=1)\n",
    "                    print('ori f-input', f_input.shape)\n",
    "                    f_input =  f_input.reindex(columns=model.feature_names_in_, fill_value=0)\n",
    "                    print('after reindex', f_input.shape)\n",
    "                    f_input, _ = preprocess_scalar(f_input, None, scalar)\n",
    "                    print('after scalar', f_input.shape)\n",
    "\n",
    "                    y_input = y[prediction_window]\n",
    "                    \n",
    "                    if model_type == 'rf' or model_type == 'xgb':\n",
    "                        axall, results_list = evaluate_ensemble_model(f_input, y_input, model, 1, prediction_window, 30, all_map, model_name=model_name, plot=plot, pretrained_model=None, X_test_format=None)\n",
    "\n",
    "                    if plot: \n",
    "                        display(axall)\n",
    "\n",
    "                    cp_year_results[f\"{str(prediction_window)}_{model_type}_{str(cv)}\"] = results_list \n",
    "                    cv_results.append(results_list)  \n",
    "\n",
    "                showresults = pd.DataFrame(cv_results)\n",
    "                showresults.columns = ['auc', 'pre', 'sensitivity', 'specificity', 'ppv', 'npv', 'sensitivity_90', 'sensitivity_95', 'ppv_90', 'ppv_95']\n",
    "                showresults.loc['mean'] = showresults.mean()\n",
    "                showresults.loc['std'] = showresults.std()\n",
    "                display('Show results of cvs', showresults)\n",
    "                show_results_dict[f\"{str(cp)}_{str(prediction_window)}_{model_type}\"] = showresults\n",
    "\n",
    "    return show_results_dict, cp_year_results \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  test on uncommon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_map = pickle.load(open('all_map.pkl', 'rb'))\n",
    "for i, v in all_map.items():\n",
    "    all_map[i] = i + ' ' + v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_portion = 0.5\n",
    "\n",
    "unmatched_f = pickle.load( open(f'./MiddleFeatures/test_f_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'rb'))\n",
    "unmatched_t = pickle.load( open(f'./MiddleFeatures/test_t_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'rb'))\n",
    "unmatched_e = pickle.load( open(f'./MiddleFeatures/test_e_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "model_test_years_save = pickle.load( open(f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_model_test_years.pkl', 'rb'))\n",
    "model_scalar_years_save = pickle.load( open(f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_model_scalar_years.pkl', 'rb'))\n",
    "\n",
    "# showresults, results = {}, {}\n",
    "for i in [10, 5, 2, 1, 0]:\n",
    "    model_test_years = model_test_years_save[i] \n",
    "    model_scalar_years = model_scalar_years_save[i] \n",
    "    showresults, results = run_retrain_evaluate_pipeline(unmatched_f, unmatched_t, all_map, 'rf',  years=[i],  pre_trained_model=None,\\\n",
    "                                score='f1_macro' , model_name='all feature', pre_trained_scalar=None, f_reference= None, finetuned_models=model_test_years, finetuned_scalars=model_scalar_years)\n",
    "\n",
    "\n",
    "    note = str(i)\n",
    "    print('save to ', f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_showrs_years_unmatched_year{note}.pkl')\n",
    "    pickle.dump(results, open(f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_rs_years_unmatched_year{note}.pkl', 'wb'))\n",
    "    print('save to ', f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_showrs_years_unmatched_year{note}.pkl')\n",
    "    pickle.dump(showresults, open(f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_showrs_years_unmatched_year{note}.pkl', 'wb'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('save to ', f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_showrs_years_unmatched.pkl')\n",
    "pickle.dump(results, open(f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_rs_years_unmatched.pkl', 'wb'))\n",
    "print('save to ', f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_showrs_years_unmatched.pkl')\n",
    "pickle.dump(showresults, open(f'rf_chunks/model_test_rf_all_feature_retrain/all_uncommon_showrs_years_unmatched.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrdPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

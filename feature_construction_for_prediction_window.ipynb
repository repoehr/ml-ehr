{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import dill\n",
    "import pickle\n",
    "import warnings\n",
    "import urllib.request\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_portion = 0.5\n",
    "ratio = 10\n",
    "psm_match_years = pickle.load( open(f\"PSM_results/years_psm_ratio10.pkl\", 'rb'))\n",
    "psm_match_years_with_dates = pickle.load( open(f\"PSM_results/years_psm_ratio10_index_date.pkl\", 'rb'))\n",
    "\n",
    "data_splits = pickle.load(open(f'Middle/splits/data_splits_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'rb'))\n",
    "\n",
    "hold_out_case, hold_out_control, test_case, test_control = data_splits\n",
    "\n",
    "print('Hold_out_case:', hold_out_case.shape, hold_out_case[:2])\n",
    "print('Hold_out_control:', hold_out_control.shape, hold_out_control[:2])    \n",
    "print('Test_case:', len(test_case), test_case[:2])\n",
    "print('Test_control:', len(test_control), test_control[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_ages_index = pickle.load(open('Middle/merged_cases_compare_all_df.pkl', 'rb'))\n",
    "control_ages = pickle.load(open('Middle/merged_controls_dates_all_df.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 load feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxdata = pickle.load(open('./MiddleFeatures/processed_dx_enc_phe.pkl', 'rb'))\n",
    "rxdata = pickle.load(open('./MiddleFeatures/processed_rx_ing.pkl', 'rb'))\n",
    "labdata = pickle.load(open('./MiddleFeatures/part_processed_lab_flag.pkl', 'rb'))\n",
    "vitaldata = pickle.load(open('./MiddleFeatures/processed_vital_continuous.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADRD_and_dementia_strings related ICD9 and ICD10 codes 118\n",
      "26 {'433.5', '290.3', '401.3', '433', '295.3', '290.2', '349', '290.1', '348', '331.1', '290.16', '317.1', '332', '291.4', '292.2', '433.2', '290.12', '433.32', '433.12', '316', '290.13', '290.11', '433.3', '290', '331.9', '331'}\n"
     ]
    }
   ],
   "source": [
    "def convert_codelist():      # maybe useless\n",
    "    ADRD_codes = pd.read_csv(f\"ADRD_dx_med_codes.csv\")\n",
    "    ADRD_codes.loc[ADRD_codes[\"Code\"] == \"33111\", \"Code\"] = \"331.11\"\n",
    "    ADRD_and_dementia_strings = [\"Alzheimer's disease\", \"Vascular dementia\", \"Frontotemporal dementia\", \"Lewy Body Dementia\", \"Dementia\", \"Conditions cause dementia\"]\n",
    "    ADRD_and_dementia_strings = '|'.join(ADRD_and_dementia_strings)\n",
    "\n",
    "    ADRD_and_dementia_ICD9 = ADRD_codes[(ADRD_codes['Code_type'] == 'ICD-9') & ADRD_codes['Concept'].str.contains(ADRD_and_dementia_strings)] \n",
    "    ADRD_and_dementia_ICD10 = ADRD_codes[(ADRD_codes['Code_type'] == 'ICD-10') & ADRD_codes['Concept'].str.contains(ADRD_and_dementia_strings)] \n",
    "    print('ADRD_and_dementia_strings related ICD9 and ICD10 codes', ADRD_and_dementia_ICD10.shape[0] +   ADRD_and_dementia_ICD9.shape[0])\n",
    "    pd.set_option('display.max_rows', None)  # Show all rows\n",
    "    pd.set_option('display.max_columns', None)  # Show all columns\n",
    "\n",
    "    ICD_to_Phewas = pd.read_csv(f\"Phecode_map_v1_2_icd9_icd10cm_09_30_2024.csv\", dtype={'ICD': str, 'Phecode': str})\n",
    "    ICD9_to_Phewas = ICD_to_Phewas[ICD_to_Phewas['Flag']==9]\n",
    "    ICD10_to_Phewas = ICD_to_Phewas[ICD_to_Phewas['Flag']==10]\n",
    "    \n",
    "    ADRD_and_dementia_ICD9_phecodes_ = ADRD_and_dementia_ICD9.merge(ICD9_to_Phewas[[\"ICD\", \"Phecode\"]], left_on='Code', right_on='ICD', how='left')\n",
    "    ADRD_and_dementia_ICD10_phecodes_ = ADRD_and_dementia_ICD10.merge(ICD10_to_Phewas[[\"ICD\", \"Phecode\"]], left_on='Code', right_on='ICD', how='left')\n",
    "\n",
    "    # display(ADRD_and_dementia_ICD9_phecodes_)\n",
    "    # display(ADRD_and_dementia_ICD10_phecodes_)\n",
    "\n",
    "    ADRD_and_dementia_ICD9_phecodes = ADRD_and_dementia_ICD9_phecodes_.Phecode.unique().tolist()\n",
    "    ADRD_and_dementia_ICD10_phecodes = ADRD_and_dementia_ICD10_phecodes_.Phecode.unique().tolist()\n",
    " \n",
    "    ICD9_codes = ADRD_and_dementia_ICD9_phecodes_.ICD.tolist()\n",
    "    ICD10_codes = ADRD_and_dementia_ICD10_phecodes_.ICD.tolist()\n",
    "    \n",
    "    pd.set_option('display.max_rows', 100)  # Show all rows\n",
    "\n",
    "    return ICD9_codes, ICD10_codes, ADRD_and_dementia_ICD9_phecodes, ADRD_and_dementia_ICD10_phecodes\n",
    "\n",
    "code9, code10, phelist9, phelist10 = convert_codelist()\n",
    "print(len(set(phelist9 + phelist10)), set(phelist9 + phelist10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phecode_9_10 = set(phelist9 + phelist10)\n",
    "\n",
    "dxdata_noadrd = dxdata[~dxdata['phecode'].str.startswith(tuple(phecode_9_10))]\n",
    "print('--Before :', dxdata.shape)\n",
    "print('--Drop ADRD diagnosis phecodes from dxdata:', dxdata_noadrd.shape)\n",
    "print('--Drop ratio:', 1- dxdata_noadrd.shape[0] / dxdata.shape[0])\n",
    "del dxdata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADRD_dx_med_codes = pd.read_csv(\"./ADRD_dx_med_codes.csv\")\n",
    "\n",
    "ANTI_DEMENTIA_RXCUI = ADRD_dx_med_codes[(ADRD_dx_med_codes['Concept'] == \"Anti-dementia medications\") & (ADRD_dx_med_codes['Code_type'] == 'RXCUI')].Code.reset_index(drop=True)\n",
    "print('Before', rxdata.shape)\n",
    "\n",
    "ANTI_DEMENTIA_RXCUI_list = ANTI_DEMENTIA_RXCUI.unique().tolist()\n",
    "ANTI_DEMENTIA_RXCUI_codes = [int(i) for i in ANTI_DEMENTIA_RXCUI_list]\n",
    "\n",
    "rxdata['rxcui_ing'] = pd.to_numeric(rxdata['rxcui_ing'], errors='coerce')\n",
    "rxdata['rxcui_ing'] = rxdata['rxcui_ing'].astype(int)\n",
    "\n",
    "rxdata_noadrd = rxdata[~rxdata.rxcui_ing.isin(ANTI_DEMENTIA_RXCUI_codes)]\n",
    "\n",
    "print('--Drop Anti-dementia medications from rxdata ingredient:', rxdata_noadrd.shape)\n",
    "print('--Drop ratio:', 1 - rxdata_noadrd.shape[0] / rxdata.shape[0])\n",
    "del rxdata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--all noduplicate lab', labdata.shape )\n",
    "labdata_common = labdata[~labdata['Class'].isin(['free t3, serum', 'urine urea nitrogen'])]\n",
    "\n",
    "print('--Removing uncommon lab', labdata_common.shape )\n",
    "print('--Drop ratio:', 1- labdata_common.shape[0] / labdata.shape[0])\n",
    "\n",
    "del labdata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vitaldata = vitaldata[(vitaldata[['ht', 'wt', 'diastolic', 'systolic']]>0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_span(df): \n",
    "    df[\"record_date\"] = pd.to_datetime(df[\"record_date\"])\n",
    "    min_date = df[\"record_date\"].dt.date.min()\n",
    "    max_date = df[\"record_date\"].dt.date.max()\n",
    "    return ((max_date - min_date).days)/365\n",
    "\n",
    "\n",
    "def get_early_date(df, col='record_date'): \n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "    min_date = df[col].dt.date.min()\n",
    "    return min_date\n",
    "\n",
    "\n",
    "def process_features(input_cases, input_controls, years, input_dxdata, input_rxdata, input_labdata, input_vitaldata, common):\n",
    "\n",
    "    def _process_dx(diagnosis, all_patient_ids, person_id_to_index_date_map, date_offset, common):\n",
    "        dx_tmp = diagnosis[diagnosis[\"patid\"].isin(all_patient_ids)].reset_index(drop=True)\n",
    "        print(\"\\n#### Processing the diganosis data ...\", dx_tmp.shape)\n",
    "    \n",
    "\n",
    "        dx_tmp = dx_tmp[dx_tmp[\"dx_date_fill\"] <= ( dx_tmp[\"patid\"].map(person_id_to_index_date_map) - date_offset)].reset_index(drop=True)\n",
    "        print('\\t--Dropped EHR after offset, now rows and unique people: ', dx_tmp.shape, dx_tmp['patid'].nunique() )\n",
    "        if common:\n",
    "\n",
    "            dx_tmp_common = dx_tmp.groupby('phecode')['patid'].nunique().reset_index(name='count')\n",
    "            min_limit = int(len(set(all_patient_ids)) * 0.001)\n",
    "            if min_limit <2:\n",
    "                min_limit = 2\n",
    "                print('\\t--DXdata set limit to 2')\n",
    "\n",
    "            dx_tmp_common = dx_tmp_common[dx_tmp_common['count'] >= min_limit]\n",
    "            phe_keep = dx_tmp_common['phecode'].to_list()\n",
    "            \n",
    "            dx_tmp = dx_tmp[dx_tmp['phecode'].isin(phe_keep)]\n",
    "            print('\\t-- Only keep {} common phecode with occurrence >'.format(len(phe_keep)), min_limit, ', remains rows:' , dx_tmp.shape)\n",
    "        print('\\tGet first date...')\n",
    "        dx_first = dx_tmp.rename(columns={\"dx_date_fill\": \"record_date\"})\n",
    "        dx_first = dx_first.groupby(\"patid\")['record_date'].apply(min).reset_index(name=\"early\")\n",
    "                \n",
    "        return dx_first, dx_tmp\n",
    "\n",
    "\n",
    "    def _process_rx(medication, all_patient_ids, person_id_to_index_date_map, date_offset, common):\n",
    "        rx_tmp = medication[medication[\"patid\"].isin(all_patient_ids)].reset_index(drop=True)\n",
    "        print(\"\\n#### Processing the medication data ...\", rx_tmp.shape)\n",
    "\n",
    "        rx_tmp = rx_tmp[rx_tmp[\"rx_start_date\"] <= ((rx_tmp[\"patid\"].map(person_id_to_index_date_map)) - date_offset)].reset_index(drop=True)\n",
    "        print('\\t--Dropped EHR after offset, now ', rx_tmp.shape, rx_tmp['patid'].nunique())\n",
    "        if common:\n",
    "            rx_tmp_common = rx_tmp.groupby('rxcui_ing')['patid'].nunique().reset_index(name='count')\n",
    "            min_limit = int(len(set(all_patient_ids)) * 0.001)\n",
    "            if min_limit < 2:\n",
    "                min_limit = 2\n",
    "                print('\\t--RXdata set limit to 2')\n",
    "            rx_tmp_common = rx_tmp_common[rx_tmp_common['count'] >= min_limit]\n",
    "            rxc_keep = rx_tmp_common['rxcui_ing'].to_list()\n",
    "            \n",
    "            rx_tmp = rx_tmp[rx_tmp['rxcui_ing'].isin(rxc_keep)]\n",
    "            print('\\t-- Only keep {} common rxcui_ing with occurrence >='.format(len(rxc_keep)), min_limit, ', remains rows:' , rx_tmp.shape)\n",
    "\n",
    "        print('\\tGet first date...')\n",
    "        rx_first = rx_tmp.rename(columns={\"rx_start_date\": \"record_date\"})\n",
    "        rx_first = rx_first.groupby(\"patid\")['record_date'].apply(min).reset_index(name=\"early\")\n",
    "        return rx_first, rx_tmp\n",
    "\n",
    "    \n",
    "    def _process_ms_common(measurement, all_patient_ids, person_id_to_index_date_map, date_offset, common):\n",
    "\n",
    "        measurement_tmp = measurement\n",
    "        measurement_tmp = measurement_tmp[measurement_tmp[\"patid\"].isin(all_patient_ids)].reset_index(drop=True)\n",
    "        print(\"\\n#### Processing the measurement data ...\", measurement_tmp.shape)\n",
    "\n",
    "        measurement_tmp = measurement_tmp[measurement_tmp[\"specimen_date\"] <= ((measurement_tmp[\"patid\"].map(person_id_to_index_date_map)) - date_offset)].reset_index(drop=True)\n",
    "        print('\\t--Dropped EHR after offset, now ', measurement_tmp.shape,  measurement_tmp['patid'].nunique())\n",
    "\n",
    "        if common:\n",
    "            measurement_tmp_common = measurement_tmp.groupby('Class')['patid'].nunique().reset_index(name='count')\n",
    "            \n",
    "            min_limit = int(len(set(all_patient_ids)) * 0.001)\n",
    "            measurement_tmp_common = measurement_tmp_common[measurement_tmp_common['count'] >= min_limit]\n",
    "\n",
    "            lab_keep = measurement_tmp_common['Class'].to_list()\n",
    "            measurement_tmp = measurement_tmp[measurement_tmp['Class'].isin(lab_keep)]\n",
    "            print('\\t-- Only keep {} common lab classes with occurrence >'.format(len(lab_keep)), min_limit, ', remains rows:' , measurement_tmp.shape)\n",
    "\n",
    "        print('\\tGetting recent measurements......')\n",
    "        measurement_tmp = measurement_tmp.sort_values(by=[\"patid\", \"Class\", \"specimen_date\"], ascending=[True, True, False])\n",
    "        measurement_tmp = measurement_tmp.groupby([\"patid\", \"Class\"]).first().reset_index()\n",
    "        print('\\t-- Recent measurements', measurement_tmp.shape)\n",
    "        measurement_tmp_format = measurement_tmp[['patid', 'specimen_date', 'Class', 'flag']]\n",
    "\n",
    "        return measurement_tmp_format\n",
    "    \n",
    "\n",
    "    def _process_vital_common(vitals, all_patient_ids, person_id_to_index_date_map, date_offset, common):\n",
    "         \n",
    "        range_dict = {\n",
    "            'sbp': {'low':None, 'high':  120},\n",
    "            'dbp': {'low':None, 'high': 80},\n",
    "            'bmi':{'low':18.5, 'high':25}\n",
    "        }        \n",
    "\n",
    "        vitals = vitals[vitals[\"patid\"].isin(all_patient_ids)].reset_index(drop=True)\n",
    "        print(\"\\n#### Processing the vital data ...\", vitals.shape)\n",
    "\n",
    "        vitals = vitals[vitals[\"measure_date\"] <= ((vitals[\"patid\"].map(person_id_to_index_date_map)) - date_offset)].reset_index(drop=True)\n",
    "        print('\\t--Dropped EHR after offset, now ', vitals.shape,  vitals['patid'].nunique())\n",
    "        \n",
    "        vital_cols = []\n",
    "        for col in ['ht', 'wt' , 'systolic', 'diastolic']:\n",
    "            # print('--process each vital, ', col)\n",
    "            vital_col = vitals[~vitals[col].isna()][['patid', \t'measure_date'\t, col]]\n",
    "            # print('----each vital, ' , col, vital_col.shape)\n",
    "\n",
    "            vital_col = vital_col.sort_values(by=[\"patid\", \"measure_date\"], ascending=[True, False])\n",
    "            # print('----sort by date')\n",
    "\n",
    "            vital_col = vital_col.groupby([\"patid\"]).first().reset_index()    \n",
    "            print('----keep last date', col, vital_col.shape)\n",
    "\n",
    "            vital_cols.append(vital_col)\n",
    "\n",
    "\n",
    "        # bmi computation\n",
    "        ht_dict = dict(zip(vital_cols[0]['patid'], vital_cols[0]['ht']))\n",
    "\n",
    "        wt_dict = dict(zip(vital_cols[1]['patid'], vital_cols[1]['wt']))\n",
    "        wtdate_dict = dict(zip(zip(vital_cols[1]['patid'], vital_cols[1]['wt']), vital_cols[1]['measure_date']))\n",
    "\n",
    "        bmi_list = []\n",
    "\n",
    "        print('--computing bmi')\n",
    "        for idx in tqdm(all_patient_ids):\n",
    "            ht_idx = ht_dict.get(idx, None)\n",
    "            wt_idx = wt_dict.get(idx, None)\n",
    "\n",
    "            if (ht_idx is not None) and (wt_idx is not None):\n",
    "                if ht_idx > 0 and wt_idx > 0:\n",
    "                    record_date_ = wtdate_dict.get((idx, wt_idx), None) \n",
    "                    bmiidx = (wt_idx * 0.4536 if wt_idx>77 else wt_idx) /( ( ht_idx  * 2.54 / 100 ) **2)\n",
    "                    bmi_list.append([idx, record_date_, bmiidx ])\n",
    "        vital_bmi = pd.DataFrame(bmi_list, columns=['patid', 'measure_date',  'value'])\n",
    "        vital_bmi['Class'] = 'bmi'\n",
    "        vital_bmi['flag'] = vital_bmi['value'].apply(lambda x: 'ablow' if x < range_dict['bmi']['low'] else ('abhigh' if x> range_dict['bmi']['high'] else 'nor'))\n",
    "        \n",
    "        vitals_bmi_format = vital_bmi[['patid', 'measure_date', 'Class', 'flag']].rename(columns={'measure_date':'specimen_date'})\n",
    "        print('-- build bmi df', vitals_bmi_format.shape)\n",
    "\n",
    "\n",
    "        def determine_flag(row, _vital_range, cat ):\n",
    "            ranges = _vital_range.get(cat, None)\n",
    "\n",
    "            if ranges and pd.notnull(ranges['low']) and pd.notnull(ranges['high']):\n",
    "                if ranges['low'] <= row['value_as_number'] <= ranges['high']:\n",
    "                    return 'nor'\n",
    "                elif row['value_as_number'] < ranges['low']:\n",
    "                    return 'ablow'\n",
    "                elif row['value_as_number'] > ranges['high']:\n",
    "                    return 'abhigh'\n",
    "            if ranges and pd.isnull(ranges['low']):\n",
    "                if row['value_as_number'] <= ranges['high']:\n",
    "                    return 'nor'\n",
    "                else:\n",
    "                    return 'abhigh'\n",
    "                \n",
    "            if ranges and pd.isnull(ranges['high']):\n",
    "                if row['value_as_number'] >= ranges['low']:\n",
    "                    return 'nor'\n",
    "                else:\n",
    "                    return 'ablow'    \n",
    "\n",
    "        # sbp\n",
    "        vitals_sbp = vital_cols[2].rename(columns={'systolic':'value_as_number'})\n",
    "        vitals_sbp['flag'] = vitals_sbp.apply(lambda row: determine_flag(row, range_dict, cat ='sbp'), axis=1)\n",
    "        vitals_sbp['Class'] = 'sbp'\n",
    "\n",
    "        vitals_dbp = vital_cols[3].rename(columns={'diastolic':'value_as_number'})\n",
    "        vitals_dbp['flag'] = vitals_dbp.apply(lambda row: determine_flag(row, range_dict, cat ='dbp'), axis=1)\n",
    "        vitals_dbp['Class'] = 'dbp'\n",
    "\n",
    "        vitals_bp = pd.concat([vitals_sbp, vitals_dbp], axis=0)\n",
    "        print('-- build bp df', vitals_bp.shape)\n",
    "\n",
    "        # display(vitals_bp)\n",
    "        vitals_bp_format = vitals_bp[['patid', 'measure_date', 'Class',  'flag']].rename(columns={'measure_date':'specimen_date'})\n",
    "\n",
    "        return vitals_bmi_format, vitals_bp_format\n",
    "        \n",
    "\n",
    "    def _process_vital_ms_common(measurement,vitals, all_patient_ids, person_id_to_index_date_map, date_offset, common):\n",
    "        \n",
    "        mea_tmp = _process_ms_common(measurement, all_patient_ids, person_id_to_index_date_map, date_offset, common)\n",
    "        \n",
    "        vital_bmi, vital_bp = _process_vital_common(vitals, all_patient_ids, person_id_to_index_date_map, date_offset, common)\n",
    "\n",
    "        all_tmp = pd.concat([mea_tmp, vital_bmi, vital_bp], axis=0, ignore_index=True)\n",
    "        \n",
    "        print('\\tGet first date...') # this only computes the abnormal as signs of meas_first, \n",
    "        meas_first = all_tmp.rename(columns={\"specimen_date\": \"record_date\"})\n",
    "\n",
    "        meas_first = meas_first.groupby(\"patid\").apply(lambda x: get_early_date(x)).reset_index(name=\"early\")\n",
    "        return meas_first, all_tmp\n",
    "    \n",
    "    def _construct_feature_matrix4(cases, controls, diagnosis, medication, measurement=None, vital_data=None, demographics=None, prediction_window=None, CP_num=None, further_lblist=None, common=False):\n",
    "        print(f\"----****Constructing the feature matrix for computable phenotype {CP_num} for a {prediction_window}-year prediction window ...\")\n",
    "\n",
    "        if isinstance(cases, pd.DataFrame) and isinstance(controls, pd.DataFrame): # the case index dataframe | the control index dataframe \n",
    "            cases_tmp = cases \n",
    "            controls_tmp = controls\n",
    "            all_patient_ids =set(cases['patid'].tolist() + controls_tmp['patid'].tolist() )\n",
    "\n",
    "            cases_to_index_date_map = cases_tmp.set_index(\"patid\").to_dict()[\"INDEX_DATE\"]  \n",
    "            controls_to_index_date_map = controls_tmp.set_index(\"patid\").to_dict()[\"INDEX_DATE\"] \n",
    "    \n",
    "\n",
    "        print('\\tall patients: ', len(all_patient_ids))   \n",
    "        person_id_to_index_date_map = cases_to_index_date_map | controls_to_index_date_map\n",
    "       \n",
    "        date_offset = pd.DateOffset(years = prediction_window) if prediction_window > 0 else  pd.DateOffset(days = 1)\n",
    "        date_offset_early = pd.DateOffset(years = prediction_window+1)\n",
    "\n",
    "\n",
    "        print(f\"\\nThere are a total of {cases_tmp.shape[0] + controls_tmp.shape[0]} samples, with {cases_tmp.shape[0]} cases and {controls_tmp.shape[0]} controls.\")\n",
    "        print('\\t-- define date offset', date_offset)\n",
    "        print('\\t-- define date offset_early', date_offset_early, '\\n')\n",
    "        \n",
    "        dx_first, dx_tmp = _process_dx(diagnosis, all_patient_ids, person_id_to_index_date_map, date_offset, common)\n",
    "        rx_first, rx_tmp = _process_rx(medication, all_patient_ids, person_id_to_index_date_map, date_offset, common)\n",
    "\n",
    "        ms_first, ms_tmp = _process_vital_ms_common(measurement, vital_data, all_patient_ids, person_id_to_index_date_map, date_offset, common)\n",
    "        # display(dx_first, rx_first, ms_first)\n",
    "        # display(dx_tmp, rx_tmp, ms_tmp)\n",
    "        early_all = pd.concat([dx_first, rx_first, ms_first])\n",
    "\n",
    "        early_all['early'] =  pd.to_datetime(early_all['early'])\n",
    "        early_all = early_all.groupby('patid', as_index=False)['early'].min()\n",
    "\n",
    "        early_all_index  = early_all[early_all[\"early\"] <= ((early_all[\"patid\"].map(person_id_to_index_date_map)) - date_offset_early)] # .reset_index(drop=True)\n",
    "        early_ids = early_all_index['patid'].unique().tolist()\n",
    "        print('|| Early patients: ', early_all_index['patid'].nunique())\n",
    "        del early_all, dx_first, rx_first, ms_first, early_all_index\n",
    "        gc.collect()\n",
    "        return early_ids, dx_tmp, rx_tmp, ms_tmp\n",
    "    \n",
    "    \n",
    "    def _pivot_feature_matrix4(early_ids, cases, controls, dx_tmp, rx_tmp, measurement_tmp=None, demographics=None, prediction_window=None, CP_num=1):\n",
    "    \n",
    "        if isinstance(cases, pd.DataFrame) and isinstance(controls, pd.DataFrame): # the case index dataframe | the control index dataframe \n",
    "            cases_tmp = cases\n",
    "            controls_tmp = controls\n",
    "            all_patient_ids =set(cases_tmp['patid'].tolist() + controls_tmp['patid'].tolist() )\n",
    "            \n",
    "            cases_to_index_date_map = cases_tmp.set_index(\"patid\").to_dict()[\"INDEX_DATE\"] # modified Nov2\n",
    "            controls_to_index_date_map = controls_tmp.set_index(\"patid\").to_dict()[\"INDEX_DATE\"] \n",
    "    \n",
    "        all_patient_ids = list(set(all_patient_ids) & set(early_ids)) #[p for p in all_patient_ids if p in early_ids] # here use the early_ids obtained from the _construct_feature_matrix4 function\n",
    "        print('\\nRestrict all patients to only include Early patients: ', len(all_patient_ids)) # remove patients that dont have more than 1 year observation window  \n",
    "\n",
    "        person_id_to_index_date_map = cases_to_index_date_map | controls_to_index_date_map\n",
    "\n",
    "        # date_offset = prediction_window \n",
    "        date_offset = pd.DateOffset(years = prediction_window) if prediction_window > 0 else  pd.DateOffset(days = 1)\n",
    " \n",
    "        print('\\t-- date offset', date_offset)\n",
    "\n",
    "        dx_tmp = dx_tmp[dx_tmp['patid'].isin(early_ids)]\n",
    "        rx_tmp = rx_tmp[rx_tmp['patid'].isin(early_ids)]\n",
    "        measurement_tmp = measurement_tmp[measurement_tmp['patid'].isin(early_ids)] # no lab data in ohsu\n",
    "        print('|| After early patients selection:', dx_tmp.shape, rx_tmp.shape, measurement_tmp.shape, '\\n')\n",
    "\n",
    "        dx_tmp = dx_tmp[[\"patid\", \"phecode\"]]\n",
    "        dx_tmp = dx_tmp.pivot_table(index=\"patid\", columns=\"phecode\", values=\"phecode\", aggfunc={\"phecode\": \"first\"}).reset_index().rename_axis(None, axis=1)\n",
    "        print('#### Pivot to dx feature dataframe', dx_tmp.shape)\n",
    "\n",
    "        dx_tmp = pd.merge(dx_tmp, pd.DataFrame({\"patid\":list(set(all_patient_ids))}), on=\"patid\", how=\"outer\")\n",
    "        print('\\t-- outer merge with patients with no dx records', dx_tmp.shape) # if no diagnosis for one id, fill the row with 0\n",
    "        dx_tmp[dx_tmp.columns[1:]] = dx_tmp[dx_tmp.columns[1:]].notna().astype(int) \n",
    "        print('\\t-- dx_tmp column examples', dx_tmp.columns[1:10])\n",
    "        dx_tmp = dx_tmp.rename(columns={col: str(col)+ \"_dx\" for col in dx_tmp.columns[1:]}) # add the _dx as the suffix for diagnosis columns\n",
    "        print(f\"-- The dimension of the processed diagnosis data is {dx_tmp.shape}.\\n\")\n",
    "        gc.collect()\n",
    "        rx_tmp = rx_tmp[[\"patid\", \"rxcui_ing\"]]\n",
    "        rx_tmp = rx_tmp.pivot_table(index=\"patid\", columns=\"rxcui_ing\", values=\"rxcui_ing\", aggfunc={\"rxcui_ing\": \"first\"}).reset_index().rename_axis(None, axis=1)\n",
    "        print('#### Pivot to rx feature dataframe', rx_tmp.shape)\n",
    "\n",
    "        rx_tmp = pd.merge(rx_tmp, pd.DataFrame({\"patid\":list(set(all_patient_ids))}), on=\"patid\", how=\"outer\")\n",
    "        print('\\t-- outer merge with patients with no rx records', rx_tmp.shape)\n",
    "        rx_tmp[rx_tmp.columns[1:]] = rx_tmp[rx_tmp.columns[1:]].notna().astype(int)\n",
    "        print('\\t-- rx_tmp column examples', rx_tmp.columns[1:10])\n",
    "        rx_tmp = rx_tmp.rename(columns={col: str(col)+ \"_rx\" for col in rx_tmp.columns[1:]}) # # add the _rx as the suffix for medication columns\n",
    "        print(f\"-- The dimension of the processed medication data is {rx_tmp.shape}.\\n\")\n",
    "        gc.collect()\n",
    "\n",
    "        measurement_tmp = measurement_tmp[[\"patid\", \"Class\", 'flag']]\n",
    "        measurement_tmp['Class_flag'] = measurement_tmp['Class'] + '_' + measurement_tmp['flag']\n",
    "\n",
    "        measurement_tmp['value'] = 1   \n",
    "#         measurement_tmp = measurement_tmp.pivot_table(index='person_id', columns='Class_flag', values='value', fill_value=0)\n",
    "        measurement_tmp = measurement_tmp.pivot_table(index=\"patid\", columns=\"Class_flag\", values=\"value\", aggfunc={\"Class_flag\": \"first\"}).reset_index().rename_axis(None, axis=1)\n",
    "        print('#### Pivot to ms feature dataframe', measurement_tmp.shape)\n",
    "        measurement_tmp = pd.merge(measurement_tmp, pd.DataFrame({\"patid\":list(set(all_patient_ids))}), on=\"patid\", how=\"outer\")\n",
    "        print('\\t-- outer merge with patients with no measurement_tmp records', measurement_tmp.shape)\n",
    "        measurement_tmp[measurement_tmp.columns[1:]] = measurement_tmp[measurement_tmp.columns[1:]].notna().astype(int)\n",
    "        print('\\t-- measurement_tmp column examples', measurement_tmp.columns[1:10])\n",
    "        measurement_tmp = measurement_tmp.rename(columns={col: str(col)+\"_measurement\" for col in measurement_tmp.columns[1:]})\n",
    "        print(f\"-- The dimension of the processed measurement data is {measurement_tmp.shape}.\\n\")\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Generating the final feature matrix ...\")\n",
    "        all_dfs = [dx_tmp, rx_tmp, measurement_tmp]\n",
    "\n",
    "        features_df = reduce(lambda left, right: pd.merge(left, right, on=['patid'], how='outer'), all_dfs) \n",
    "\n",
    "        del all_dfs\n",
    "        gc.collect()\n",
    "        print(f\"The dimension of the feature matrix is {features_df.shape}.\\n\")\n",
    "        # compute age_at_prediction_window column as the feature, \n",
    "        # besides this age_at_prediction, there is no demographics in ohsu, so here the _px is not combined with demographics \n",
    "\n",
    "        features_df[\"age_at_prediction_window\"] = features_df[\"patid\"].map(person_id_to_index_date_map) - date_offset # index age - prediction_window = age_at_prediction\n",
    "             \n",
    "        targets = np.where(features_df[\"patid\"].isin(cases_tmp[\"patid\"]), 1, 0) # set the label for case and control\n",
    "        print('\\tCases: ', targets.sum(), 'Controls: ', len(targets) - targets.sum(), 'Ratio: ', (len(targets) - targets.sum())/targets.sum())\n",
    "        object_cols = [col for col in features_df.dtypes[features_df.dtypes == 'object'].index if col != \"patid\"]\n",
    "        for col in object_cols:\n",
    "            features_df[col] = pd.to_numeric(features_df[col], errors=\"coerce\")\n",
    "        gc.collect()\n",
    "        print(\"--------------------------- Done -------------------------------------\\n\")\n",
    "        return features_df, targets\n",
    "\n",
    "    \n",
    "    _early_patient_ids, _dx_tmp, _rx_tmp, _ms_tmp = _construct_feature_matrix4\\\n",
    "    (input_cases, input_controls, input_dxdata, input_rxdata, input_labdata, input_vitaldata, prediction_window=years[0], CP_num=1, common=common)\n",
    "    # pivot feature to matrix\n",
    "    X, y = _pivot_feature_matrix4\\\n",
    "    (_early_patient_ids, input_cases, input_controls,\\\n",
    "        _dx_tmp, _rx_tmp, _ms_tmp, None,\\\n",
    "        years[0], 1)\n",
    "    return X, y, _early_patient_ids\n",
    "\n",
    "\n",
    "def further_filter_controls(_feature, _target, _early, input_psm, years):\n",
    "    _feature_drop = {}\n",
    "    _target_drop = {}\n",
    "    _early_drop = {}\n",
    "    \n",
    "    for prediction_window in reversed(years):\n",
    "        print('Dealing with prediction window: ', prediction_window)\n",
    "        px = _feature[prediction_window]\n",
    "        tx = _target[prediction_window]\n",
    "        ex = _early[prediction_window] \n",
    "\n",
    "        print('\\tBefore the processing, total ids:',  len(tx), '| cases:', tx.sum(), '| control:',len(tx)-tx.sum(), '| Control/Case ratio:',(len(tx)-tx.sum())/tx.sum())\n",
    "\n",
    "        psm = input_psm[f'match_year{prediction_window}']\n",
    "\n",
    "        ori_case = psm.index.tolist()\n",
    "        # compare the original cases and the early_ids, to find the cases that are removed \n",
    "        # case_to_remove =  [i for i in ori_case if i not in set(ex)]\n",
    "        case_to_remove = list(set(ori_case) - set(ex))\n",
    "\n",
    "        print('\\tRemoved cases:', case_to_remove)\n",
    "\n",
    "        if len(case_to_remove) > 0:\n",
    "            loc_case = psm.loc[case_to_remove]\n",
    "\n",
    "            psmcol = [c for c in loc_case.columns if c.startswith('psm_control_') and 'index' not in c] # select the matched control columns\n",
    "            loc_control = loc_case.loc[:, psmcol].values # search the columns to find controls in the psm data\n",
    "            control_to_remove_ = [item for sublist in loc_control for item in sublist] # fine the controls of the cases (that will be removed)\n",
    "            control_to_remove  = [i for i in control_to_remove_ if i is not None]\n",
    "            print('\\t\\tWill remove controls:', len(control_to_remove))\n",
    "            \n",
    "            idx_control_to_remove = px[px['patid'].isin(control_to_remove)] # find the row index of the controls \n",
    "            idx_control_to_remove = idx_control_to_remove.index.tolist()\n",
    "\n",
    "            px_drop = px.drop(idx_control_to_remove, inplace=False, axis=0)\n",
    "            tx_drop = np.array([ ti for i , ti in enumerate(tx) if i not in idx_control_to_remove])\n",
    "            ex_drop = [ ei for i , ei in enumerate(ex) if ei not in control_to_remove]\n",
    "            print('\\tAfter the processing: total ids:', px_drop.shape, '| cases:', tx_drop.sum(), '| control:',len(tx_drop)-tx_drop.sum(), '| Control/Case ratio:',(len(tx_drop)-tx_drop.sum())/tx_drop.sum())\n",
    "\n",
    "        else:\n",
    "            # if no case is removed afte early_ids computation, this function is designed to have no use. \n",
    "            px_drop = px\n",
    "            tx_drop = tx\n",
    "            \n",
    "            ex_drop = ex\n",
    "\n",
    "        _target_drop[prediction_window] = tx_drop\n",
    "        _early_drop[prediction_window] = ex_drop\n",
    "        _feature_drop[prediction_window] = px_drop\n",
    "        # there is no demographics in ohsu, so here the _px is not combined with demographics \n",
    "        print('------------\\n')\n",
    "    return _feature_drop, _target_drop, _early_drop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_f = {}\n",
    "matched_t = {}\n",
    "matched_e = {}\n",
    "\n",
    "\n",
    "for year in reversed( [0, 1, 2, 5, 10]):\n",
    "# for year in [10]:\n",
    "    psm_match = psm_match_years_with_dates[f'match_year{year}']\n",
    "\n",
    "    case_id_year = psm_match.index.tolist()  \n",
    "    case_date_info = cases_ages_index[cases_ages_index['patid'].isin(case_id_year)]  \n",
    "    # get the case index date in the psm file \n",
    "\n",
    "    control_columns = [col for col in psm_match.columns if 'psm_control_' in col]\n",
    "    control_and_index_list = []\n",
    "    for match_number in range(10):\n",
    "        control_and_index = psm_match.loc[:, ['psm_control_' + str(match_number+1), 'psm_control_' + str(match_number+1) + '_index']].reset_index(drop=True)\n",
    "        control_and_index.columns = ['patid', 'INDEX_DATE']\n",
    "\n",
    "        control_and_index_list.append(control_and_index)\n",
    "\n",
    "\n",
    "    control_index_info = pd.concat(control_and_index_list, axis=0)\n",
    "\n",
    "    control_index_info = control_index_info.dropna(subset=['patid'])\n",
    "    print('--control_date_info shape', control_index_info.shape)\n",
    "    control_id_year = control_index_info.patid.unique().tolist()\n",
    "\n",
    "    print('Case for this year:', len(case_id_year), '| Control for this year:', len(control_id_year), '| Total ids:', len(set(case_id_year + control_id_year)))\n",
    "\n",
    "    ## common is false\n",
    "    f, t, e = process_features(case_date_info, control_index_info, [year], dxdata_noadrd, rxdata_noadrd, labdata_common, vitaldata, common=False)\n",
    "    matched_f[year] = f \n",
    "    matched_t[year] = t \n",
    "    matched_e[year] = e \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchf_drop, matcht_drop, matche_drop = further_filter_controls(matched_f, matched_t, matched_e, psm_match_years_with_dates, [10, 5, 2, 1, 0 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(matchf_drop, open(f'./MiddleFeatures/matched_f_drop_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'wb'))\n",
    "pickle.dump(matcht_drop, open(f'./MiddleFeatures/matched_t_drop_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'wb'))\n",
    "pickle.dump(matche_drop, open(f'./MiddleFeatures/matched_e_drop_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_portion =0.5\n",
    "ratio = 10\n",
    "pickle.dump(matched_f, open(f'./MiddleFeatures/matched_f_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(matched_t, open(f'./MiddleFeatures/matched_t_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'wb'))\n",
    "pickle.dump(matched_e, open(f'./MiddleFeatures/matched_e_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_f = {}\n",
    "unmatched_t = {}\n",
    "unmatched_e = {}\n",
    "\n",
    "for year in [0, 1, 2 ,5, 10]:\n",
    "\n",
    "    test_case_date = cases_ages_index[cases_ages_index['patid'].isin(test_case)]  # select from 50% testing data\n",
    "    required_date_offset =  year + 1\n",
    "    test_case_date = test_case_date[test_case_date['EARLIEST_DATE'] + pd.DateOffset(years =  required_date_offset) <= test_case_date['INDEX_DATE']] # the test case should have at least 1 year in observation window\n",
    "    test_case_id = test_case_date['patid'].unique().tolist()\n",
    "\n",
    "    test_control_date = control_ages[control_ages['patid'].isin(test_control)] \n",
    "    test_control_date = test_control_date[test_control_date['EARLIEST_DATE'] +  pd.DateOffset(years =  required_date_offset) <= test_control_date['Last_EHR_minus_one_INDEX_DATE']] # for testing data, control is not matched so the index date is Last_EHR_minus_one_INDEX_DATE\n",
    "    test_control_date = test_control_date.rename(columns={'Last_EHR_minus_one_INDEX_DATE':'INDEX_DATE'})\n",
    "    test_control_id = test_control_date['patid'].unique().tolist()\n",
    "\n",
    "    print('Case for this year:', len(test_case_id), '| Control for this year:', len(test_control_id), '| Total ids:', len(set(test_case_id + test_control_id)))\n",
    "\n",
    "    f, t, e = process_features(test_case_date, test_control_date, [year], dxdata_noadrd, rxdata_noadrd, labdata_common, vitaldata, common=False)\n",
    "    unmatched_f[year] = f \n",
    "    unmatched_t[year] = t \n",
    "    unmatched_e[year] = e \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del labdata_common, dxdata_noadrd, rxdata_noadrd, vitaldata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_portion =0.5\n",
    "ratio = 10\n",
    "pickle.dump(unmatched_f, open(f'./MiddleFeatures/test_f_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unmatched_t, open(f'./MiddleFeatures/test_t_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'wb'))\n",
    "pickle.dump(unmatched_e, open(f'./MiddleFeatures/test_e_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import dill\n",
    "import pickle\n",
    "import warnings\n",
    "import urllib.request\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import patchworklib as pw\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    average_precision_score, precision_recall_curve,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    recall_score, precision_score,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names, but DecisionTreeClassifier was fitted without feature names\")\n",
    "\n",
    "def preprocess(xtrain, xtest ):\n",
    "    cols_to_impute = [col for col in xtrain.columns if  col == \"age_at_prediction_window\"]\n",
    "\n",
    "    if len(cols_to_impute) == 0:\n",
    "        print('\\tpreprocess column', None)\n",
    "        return xtrain, xtest, None\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(xtrain[cols_to_impute])\n",
    "    print('\\tpreprocess column', cols_to_impute)\n",
    "    xtrain[cols_to_impute] = scaler.transform(xtrain[cols_to_impute])\n",
    "    if xtest is not None:\n",
    "        xtest[cols_to_impute] = scaler.transform(xtest[cols_to_impute])\n",
    "\n",
    "    return xtrain, xtest, scaler\n",
    "\n",
    "   \n",
    "def preprocess_scalar(xtrain, xtest, scalar ):\n",
    "    cols_to_impute = [col for col in xtrain.columns if  col == \"age_at_prediction_window\"]\n",
    "\n",
    "    if len(cols_to_impute) == 0:\n",
    "        print('\\tpreprocess column', None)\n",
    "        return xtrain\n",
    "    \n",
    "    xtrain[cols_to_impute] = scalar.transform(xtrain[cols_to_impute])\n",
    "    if xtest is not None:\n",
    "        xtest[cols_to_impute] = scalar.transform(xtest[cols_to_impute])\n",
    "\n",
    "    return xtrain, xtest\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_prob, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    avpre = average_precision_score(y_true, y_pred_prob)\n",
    "\n",
    "    return auc, avpre, None, None, None, None\n",
    "# ppv  \n",
    "\n",
    "def ppv_sensitivity(specificity_levels, _y_true, _y_pred_proba):\n",
    "    add_sensitivity_results = []\n",
    "    add_ppv_results =  []\n",
    "    add_fpr, add_tpr, add_thresholds = roc_curve(_y_true, _y_pred_proba)\n",
    "    _results = {}\n",
    "    for specificity in specificity_levels:\n",
    "\n",
    "        _threshold_index = np.where(add_fpr <= (1 - specificity))[0][-1]\n",
    "        _threshold = add_thresholds[_threshold_index]\n",
    "\n",
    "        # Sensitivity (True Positive Rate)\n",
    "        _sensitivity = add_tpr[_threshold_index]\n",
    "        add_sensitivity_results.append( _sensitivity)\n",
    "\n",
    "        # Positive Predictive Value (PPV)\n",
    "        _y_pred_binary = (_y_pred_proba >= _threshold).astype(int)\n",
    "        _ppv = precision_score(_y_true, _y_pred_binary)\n",
    "        add_ppv_results.append(_ppv)\n",
    "\n",
    "    # Add metrics to results\n",
    "    _results['Sensitivity'] = add_sensitivity_results\n",
    "    _results['PPV'] = add_ppv_results\n",
    "    return _results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reconstruct_rf(name, dirc='rf_chunks', modelkey=None, modelcv=None):\n",
    "    model_name = name\n",
    "    output_dir_model = os.path.join(dirc, model_name)\n",
    "    \n",
    "    if not os.path.exists(output_dir_model):\n",
    "        raise FileNotFoundError(f\"Error: The directory '{output_dir_model}' does not exist. Make sure the chunks are saved.\")\n",
    "    \n",
    "    chunk_files = sorted([os.path.join(output_dir_model, f) for f in os.listdir(output_dir_model) if f.endswith(\".pkl\") and ('chunk_' in f)])\n",
    "    key_chunk_files = []\n",
    "    for ifile in chunk_files:\n",
    "        if str(modelkey) + '_' + str(modelcv) in ifile: \n",
    "                key_chunk_files.append(ifile )\n",
    "    chunk_files = key_chunk_files\n",
    "\n",
    "    if len(chunk_files) == 0:\n",
    "        raise FileNotFoundError(f\"No chunk files found in '{output_dir_model}'. Ensure the chunks were saved correctly.\")\n",
    "    \n",
    "    reconstructed_rf = RandomForestClassifier()\n",
    "    reconstructed_rf.estimators_ = []\n",
    "    print(f\"Ori\", len(reconstructed_rf.estimators_))\n",
    "\n",
    "    for chunk_file in chunk_files:\n",
    "        chunk = joblib.load(chunk_file)\n",
    "        reconstructed_rf.estimators_.extend(chunk)\n",
    "        print(f\"Loaded {chunk_file}\", len(chunk), len(reconstructed_rf.estimators_))\n",
    "    \n",
    "    reconstructed_rf.n_estimators = len(reconstructed_rf.estimators_)\n",
    "\n",
    "    reconstructed_rf.modelname = name\n",
    "    reconstructed_rf.modelkey = modelkey\n",
    "    reconstructed_rf.modelcv = modelcv\n",
    "    \n",
    "    print(f\"Reconstructed RF model with {reconstructed_rf.n_estimators} estimators.\")\n",
    "    return reconstructed_rf\n",
    "\n",
    "\n",
    "def build_RF_model(random_seed):\n",
    "    estimator = RandomForestClassifier(class_weight='balanced', random_state=random_seed, warm_start=False)\n",
    "\n",
    "    max_depth =  [int(x) for x in np.linspace(3, 20, 5, dtype=int)]\n",
    "    # max_depth.append(None)\n",
    "    print('max_depth option:', max_depth)\n",
    "\n",
    "    param_dict = { \n",
    "        \"n_estimators\": [1000,3000,5000], ## ori 1000 3000\n",
    "        \"max_depth\": max_depth,\n",
    "        \"min_samples_split\": [5, 10, 40],\n",
    "        \"max_features\": ['sqrt']\n",
    "    }\n",
    "\n",
    "    return estimator, param_dict\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_base_model(X_test, y_test, model, CP_num, prediction_window, N, feature_map, model_name, plot=True):\n",
    "    best_mod = model\n",
    "\n",
    "    predicted_proba = best_mod.predict_proba(X_test)[:, 1]\n",
    "    predicted_labels = best_mod.predict(X_test)\n",
    "\n",
    "    auc, pre,sensitivity , specificity , ppv, npv = calculate_metrics(y_test, predicted_proba, predicted_labels)\n",
    "    \n",
    "    add_results = ppv_sensitivity([0.9, 0.95], y_test, predicted_proba)\n",
    "    sensitivity_90, sensitivity_95 = add_results['Sensitivity']\n",
    "    ppv_90, ppv_95 = add_results['PPV']\n",
    "    \n",
    "    if plot:\n",
    "        print(f\"\\nDisplaying performance for CP {CP_num} with a {prediction_window}-year prediction window:\\n\")\n",
    "\n",
    "        prec, rec, threshold = precision_recall_curve(y_test, predicted_proba)\n",
    "        prc_df = pd.DataFrame({\"Recall\": rec, \"Precision\": prec})\n",
    "        ap_score = average_precision_score(y_test, predicted_proba)\n",
    "        base_ap_score = np.mean(y_test)\n",
    "        prc_plot = (\n",
    "            ggplot(prc_df, aes(\"Recall\", \"Precision\")) + \n",
    "            geom_line(color=\"#3C5488B2\") +\n",
    "            theme_bw() +\n",
    "            theme() +\n",
    "            coord_fixed() +\n",
    "            geom_hline(yintercept=base_ap_score, linetype=\"dashed\") +\n",
    "            labs(title=\"Precision-Recall Curve\") +\n",
    "            annotate(\"text\", x=0.15, y=1, label=f\"AP={ap_score:.2f}\", size=8) +\n",
    "            annotate(\"text\", x=0.3, y=0.95, label=f\"Chance Level AP={base_ap_score:.2f}\", size=8)\n",
    "            )\n",
    "        ax1 = pw.load_ggplot(prc_plot, figsize=(2.5, 2.5))\n",
    "    \n",
    "    \n",
    "        ax2 = pw.Brick(figsize=(2.5, 2.5))\n",
    "        cm = confusion_matrix(y_test, predicted_labels, labels=best_mod.classes_)\n",
    "        sns.heatmap(cm, annot=True, linewidth=1, cmap=\"GnBu\", fmt=\"g\",\n",
    "                    yticklabels=[\"Control\", \"Case\"], xticklabels=[\"Control\", \"Case\"], ax=ax2)\n",
    "        ax2.set_title(\"Confusion Matrix\")\n",
    "        ax2.set_xlabel(\"Predicted Label\")\n",
    "        ax2.set_ylabel(\"True Label\")\n",
    "    \n",
    "        coefs = best_mod.coef_[0]\n",
    "        top_N_feature_index = np.argsort(abs(coefs))[-N:]\n",
    "        top_N_feature_names = X_test.columns[top_N_feature_index]\n",
    "\n",
    "        def get_name(x):\n",
    "            if isinstance(x, str):\n",
    "                return feature_map.get(x.strip(), x)\n",
    "            else:\n",
    "                return feature_map.get(x, x)\n",
    "        top_N_feature_names = pd.Series(top_N_feature_names).apply(get_name)\n",
    "\n",
    "\n",
    "        top_N_coefs_abs = abs(coefs)[top_N_feature_index]\n",
    "        top_N_coefs = coefs[top_N_feature_index]\n",
    "        coef_plot_df = pd.DataFrame({\"feature_name\": top_N_feature_names,\n",
    "                                \"abs_coef\": top_N_coefs_abs,\n",
    "                                \"coef\": top_N_coefs})\n",
    "        coef_plot_df['feature_name'] = pd.Categorical(coef_plot_df['feature_name'], categories=coef_plot_df.sort_values('abs_coef')['feature_name'])\n",
    "\n",
    "        feature_importance_plot = (\n",
    "            ggplot(coef_plot_df, aes(\"feature_name\", \"coef\")) +\n",
    "                geom_bar(stat=\"identity\", fill=\"#91D1C2B2\", color=\"black\") +\n",
    "                coord_flip() +\n",
    "                theme_bw() +\n",
    "                labs(x=\"\", y=\"Feature Coefficients\", title=f\"{model_name} Model (CP {CP_num} with a {prediction_window}-yr prediction window)\")\n",
    "            )\n",
    "        ax3 = pw.load_ggplot(feature_importance_plot, figsize=(5, 4))\n",
    "        ax_all = (ax1 | ax2)/ax3\n",
    "    else:\n",
    "        ax_all = None\n",
    "    results_list = [auc , pre, sensitivity , specificity , ppv, npv , sensitivity_90, sensitivity_95, ppv_90, ppv_95]\n",
    "\n",
    "    return ax_all, results_list\n",
    "\n",
    "def evaluate_ensemble_model( X_test,  y_test, model, CP_num, prediction_window, N, feature_map, model_name='', plot=True, pretrained_model=None, X_test_format=None):\n",
    "    print('evaluate_ensemble_model...')\n",
    "    best_mod = model\n",
    "    X_test_input = X_test.values\n",
    "    X_test_format_input = X_test_format.values\n",
    "    \n",
    "    predicted_proba = predict_proba_stack(model, pretrained_model, X_test_input, X_test_format_input )[:, 1]\n",
    "    print('obtain proba!')\n",
    "\n",
    "    # predicted_labels = predict_joint_stack(model, pretrained_model, X_test_input, X_test_format_input)\n",
    "    # print('obtain labels...')\n",
    "    predicted_labels = None\n",
    "    # print(predicted_labels, predicted_proba, y_test)\n",
    "#     print('auc | pre | sensitivity | specificity | ppv | npv')\n",
    "    auc, pre,sensitivity , specificity , ppv, npv = calculate_metrics(y_test, predicted_proba, predicted_labels)\n",
    "#     numbers = [float(\"{:.5f}\".format(num)) for num in [auc, pre,sensitivity, specificity, ppv, npv]]\n",
    "#     print(numbers)\n",
    "    add_results = ppv_sensitivity([0.9, 0.95], y_test, predicted_proba)\n",
    "    sensitivity_90, sensitivity_95 = add_results['Sensitivity']\n",
    "    ppv_90, ppv_95 = add_results['PPV']\n",
    "    plot = False\n",
    "    if plot: # plot prc curve, confusion matrix, feature importance using default method from each type of models itself. Could skip this plot. \n",
    "        print(f\"\\nDisplaying performance for CP {CP_num} with a {prediction_window}-year prediction window:\\n\")\n",
    "\n",
    "        prec, rec, threshold = precision_recall_curve(y_test, predicted_proba)\n",
    "        prc_df = pd.DataFrame({\"Recall\": rec, \"Precision\": prec})\n",
    "        ap_score = average_precision_score(y_test, predicted_proba)\n",
    "        base_ap_score = np.mean(y_test)\n",
    "\n",
    "        prc_plot = (\n",
    "            ggplot(prc_df, aes(\"Recall\", \"Precision\")) + \n",
    "            geom_line(color=\"#3C5488B2\") +\n",
    "            theme_bw() +\n",
    "            theme() +\n",
    "            coord_fixed() +\n",
    "            geom_hline(yintercept=base_ap_score, linetype=\"dashed\") +\n",
    "            labs(title=\"Precision-Recall Curve\") +\n",
    "            annotate(\"text\", x=0.15, y=1, label=f\"AP={ap_score:.2f}\", size=8) +\n",
    "            annotate(\"text\", x=0.3, y=0.95, label=f\"Chance Level AP={base_ap_score:.2f}\", size=8)\n",
    "            )\n",
    "        ax1 = pw.load_ggplot(prc_plot, figsize=(2.5, 2.5))\n",
    "\n",
    "\n",
    "        ax2 = pw.Brick(figsize=(2.5, 2.5))\n",
    "        cm = confusion_matrix(y_test, predicted_labels, labels=best_mod.classes_)\n",
    "        sns.heatmap(cm, annot=True, linewidth=1, cmap=\"GnBu\", fmt=\"g\",\n",
    "                    yticklabels=[\"Control\", \"Case\"], xticklabels=[\"Control\", \"Case\"], ax=ax2)\n",
    "        ax2.set_title(\"Confusion Matrix\")\n",
    "        ax2.set_xlabel(\"Predicted Label\")\n",
    "        ax2.set_ylabel(\"True Label\")\n",
    "\n",
    "        feature_importances = best_mod.feature_importances_\n",
    "        top_N_feature_index = np.argsort(feature_importances)[-N:]\n",
    "        top_N_feature_names = X_test.columns[top_N_feature_index]\n",
    "\n",
    "        def get_name(x):\n",
    "            if isinstance(x, str):\n",
    "                return feature_map.get(x.strip(), x)\n",
    "            else:\n",
    "                return feature_map.get(x, x)\n",
    "\n",
    "        top_N_feature_names = pd.Series(top_N_feature_names).apply(get_name)\n",
    "\n",
    "        top_N_importances = feature_importances[top_N_feature_index]\n",
    "\n",
    "        coef_plot_df = pd.DataFrame({\n",
    "            \"feature_name\": top_N_feature_names,\n",
    "            \"importance\": top_N_importances\n",
    "        })\n",
    "\n",
    "        coef_plot_df['feature_name'] = pd.Categorical(coef_plot_df['feature_name'], categories=coef_plot_df.sort_values('importance')['feature_name'])\n",
    "\n",
    "        feature_importance_plot = (\n",
    "            ggplot(coef_plot_df, aes(\"feature_name\", \"importance\")) +\n",
    "                geom_bar(stat=\"identity\", fill=\"#91D1C2B2\", color=\"black\") +\n",
    "                coord_flip() +\n",
    "                theme_bw() +\n",
    "                labs(x=\"\", y=\"Feature Importance\", title=f\"{model_name} Model (CP {CP_num} with a {prediction_window}-yr prediction window)\")\n",
    "            )\n",
    "        ax3 = pw.load_ggplot(feature_importance_plot, figsize=(5, 4))\n",
    "        ax_all = (ax1 | ax2)/ax3\n",
    "    else:\n",
    "        ax_all = None\n",
    "    results_list = [auc , pre, sensitivity , specificity , ppv, npv , sensitivity_90, sensitivity_95, ppv_90, ppv_95]\n",
    "    print(results_list)\n",
    "    return ax_all , results_list\n",
    "\n",
    "\n",
    "def format_input(_current_feature, prediction_window, f_reference):\n",
    "    cp = 1\n",
    "    try:\n",
    "        saved_model_features = f_reference[f'CP_{cp}_{prediction_window}_yr'].drop('person_id', axis=1).columns\n",
    "    except:\n",
    "        refer_cols = f_reference[f'CP_{cp}_{prediction_window}_yr']\n",
    "\n",
    "        if not isinstance(refer_cols, list):\n",
    "            refer_cols= f_reference[f'CP_{cp}_{prediction_window}_yr'].to_list()\n",
    "        saved_model_features = [i for i in  refer_cols if i != 'person_id']\n",
    "    \n",
    "    current_features = _current_feature.columns\n",
    "    overlapping_cols = set(saved_model_features).intersection(set(current_features))\n",
    "    missing_cols = set(saved_model_features) - set(current_features)\n",
    "    print('Current cols\\t', len(current_features))\n",
    "    print('Reference cols\\t', len(saved_model_features))\n",
    "    print('Overalaping cols\\t', len(overlapping_cols))\n",
    "    print('Missing_cols cols\\t', len(missing_cols), missing_cols)\n",
    "\n",
    "    f_input = _current_feature.reindex(columns=saved_model_features, fill_value=0)\n",
    "    assert 'patid' not in f_input.columns\n",
    "    return f_input\n",
    "\n",
    "'''\n",
    "def parallel_forest_predict_proba(forest, X, n_jobs=-1):\n",
    "    def predict_proba_tree(tree, X_subset):\n",
    "        leaf_ids = tree.apply(X_subset)\n",
    "        \n",
    "        leaf_values = tree.tree_.value  # Shape: (n_nodes, n_classes)\n",
    "        \n",
    "        probas = leaf_values[leaf_ids][:, 0]   \n",
    "        \n",
    "        probas = probas / probas.sum(axis=1, keepdims=True)\n",
    "        return probas\n",
    "    start = time.time()\n",
    "    tree_probas = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(predict_proba_tree)(tree, X) for tree in forest.estimators_\n",
    "    )\n",
    "    print('in parallel proba', time.time() - start, tree_probas)\n",
    "\n",
    "    # print('tree', len(tree_probas))\n",
    "    # avg_probas = np.mean(tree_probas, axis=0)  # Shape: (n_samples, n_classes)\n",
    "    all_probas = np.sum(tree_probas, axis=0)  # Shape: (n_samples, n_classes)\n",
    "    # print(all_probas.shape)\n",
    "    # avg_probas = all_probas / len(forest.estimators_)\n",
    "    return all_probas\n",
    "\n",
    "'''\n",
    "\n",
    "def predict_joint(_model, _loadmodel, input_np, input_np_format):\n",
    "\n",
    "    try:\n",
    "        check_is_fitted(_model, \"estimators_\")\n",
    "        estimators = _model.estimators_\n",
    "    except Exception as e:\n",
    "        print(\"Model is not fitted or estimators_ not initialized:\", e)\n",
    "        return None\n",
    "    \n",
    "    saved_estimators  =  _loadmodel.estimators_ \n",
    "    for tree in saved_estimators:\n",
    "        if not hasattr(tree, \"monotonic_cst\"):\n",
    "            tree.monotonic_cst = None  # Set default value to None\n",
    "\n",
    "    saved_predictions = [ tree.predict(input_np_format) for tree in saved_estimators]\n",
    "    predictions = [tree.predict(input_np) for tree in estimators]\n",
    "    all_predictions = np.array( saved_predictions + predictions)\n",
    "\n",
    "    if all_predictions.dtype != np.int64:\n",
    "        all_predictions = all_predictions.astype(int)\n",
    "\n",
    "    majority_vote = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_predictions)\n",
    "    return majority_vote\n",
    "\n",
    "\n",
    "\n",
    "def predict_joint_stack(_model, _loadmodel, input_np, input_np_format):\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "    def parallel_tree_predict(trees, X):\n",
    "        with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "            preds = list(executor.map(lambda tree: tree.predict(X), trees))\n",
    "        return np.array(preds)\n",
    "\n",
    "\n",
    "    estimators = _model.estimators_\n",
    "    saved_estimators  =  _loadmodel.estimators_ \n",
    "    for tree in saved_estimators:\n",
    "        if not hasattr(tree, \"monotonic_cst\"):\n",
    "            tree.monotonic_cst = None  # Set default value to None\n",
    "\n",
    "    saved_predictions = parallel_tree_predict(saved_estimators, input_np_format)\n",
    "    predictions = parallel_tree_predict(estimators, input_np)\n",
    "    \n",
    "    all_predictions = np.vstack([saved_predictions,predictions ]) \n",
    "\n",
    "    if all_predictions.dtype != np.int64:\n",
    "        all_predictions = all_predictions.astype(int)\n",
    "\n",
    "    majority_vote = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_predictions)\n",
    "    return majority_vote\n",
    "\n",
    "\n",
    "\n",
    "def predict_joint_by_proba_stack(_model, _loadmodel, input_np, input_np_format):\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    probas = predict_proba_stack(_model, _loadmodel, input_np, input_np_format)\n",
    "    labels = np.argmax(probas, axis=1)\n",
    "    return labels \n",
    "\n",
    "\n",
    "from scipy.stats import entropy\n",
    "def predict_proba_stack(_model, _loadmodel, input_np, input_np_format):\n",
    "    \n",
    "    def predict_proba_tree(tree, X_subset):\n",
    "        leaf_ids = tree.apply(X_subset)\n",
    "        \n",
    "        leaf_values = tree.tree_.value  # Shape: (n_nodes, n_classes)\n",
    "        \n",
    "        probas = leaf_values[leaf_ids][:, 0]   \n",
    "        \n",
    "        probas = probas / probas.sum(axis=1, keepdims=True)\n",
    "        return probas\n",
    "    \n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "    def parallel_tree_predict_proba(trees, X):\n",
    "        with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "            preds = list(executor.map(lambda tree: predict_proba_tree(tree, X), trees))\n",
    "        return preds\n",
    "\n",
    "    _loadmodel.n_classes_ = 2\n",
    "    _model.n_classes_ = 2\n",
    "    saved_estimators  =  _loadmodel.estimators_ \n",
    "    start = time.time()\n",
    "\n",
    "    \n",
    "    for tree in saved_estimators:\n",
    "        if not hasattr(tree, \"monotonic_cst\"):\n",
    "            tree.monotonic_cst = None  # Set default value to None\n",
    "    useentropy = False\n",
    "    if useentropy is True:\n",
    "\n",
    "        tree_probas = parallel_tree_predict_proba(saved_estimators, input_np_format)\n",
    "        proba_save = np.mean(tree_probas, axis=0)  # Shape: (n_samples, n_classes)\n",
    "\n",
    "        proba_re = _model.predict_proba(input_np)\n",
    "\n",
    "        entro_save = entropy(proba_save, axis=1)\n",
    "        entro_re = entropy(proba_re, axis=1)\n",
    "        uncerntain_save = 1 - (entro_save / np.max(entro_save))\n",
    "        uncerntain_re = 1 - (entro_re / np.max(entro_re))\n",
    "\n",
    "        final_proba = (uncerntain_save[:, np.newaxis] * proba_save + uncerntain_re[:, np.newaxis] * proba_re)\n",
    "\n",
    "        final_proba /= (uncerntain_save[:, np.newaxis] + uncerntain_re[:,np.newaxis])\n",
    "    else:\n",
    "        proba_re =  _model.predict_proba(input_np)\n",
    "        print('get proba from re', proba_re.shape)\n",
    "        # proba_pre =  _loadmodel.predict_proba(input_np_format)\n",
    "\n",
    "        tree_probas = parallel_tree_predict_proba(saved_estimators, input_np_format)\n",
    "        proba_pre = np.mean(tree_probas, axis=0)  \n",
    "        # proba_pre = all_probas/len(saved_estimators)  \n",
    "        print('get proba from pre', proba_pre.shape)\n",
    "        sum_probas = proba_re * len(_model.estimators_) + proba_pre * len(_loadmodel.estimators_)  \n",
    "        \n",
    "        final_proba  = sum_probas/ ( len(_model.estimators_) + len(_loadmodel.estimators_)   )\n",
    "\n",
    "    return final_proba\n",
    "\n",
    "    \n",
    "def predict_proba_joint(_model, _loadmodel, input_np, input_np_format):\n",
    "    from sklearn.utils.validation import check_is_fitted\n",
    "    import numpy as np\n",
    "\n",
    "    try:\n",
    "        check_is_fitted(_model, \"estimators_\")\n",
    "        estimators = _model.estimators_\n",
    "    except Exception as e:\n",
    "        print(\"Model is not fitted or estimators_ not initialized:\", e)\n",
    "        return None\n",
    "\n",
    "    saved_estimators = _loadmodel.estimators_\n",
    "    for tree in saved_estimators:\n",
    "        if not hasattr(tree, \"monotonic_cst\"):\n",
    "            tree.monotonic_cst = None  # Set default value to None\n",
    "    start = time.time()\n",
    "    # saved_probas = np.array([tree.predict_proba(X_inner_valid_format) for tree in saved_estimators])  # Shape: (n_saved_estimators, n_samples, 2)\n",
    "    \n",
    "    trained_probas = np.array([tree.predict_proba(input_np) for tree in estimators])  # Shape: (n_trained_estimators, n_samples, 2)\n",
    "    trained_sum_probas = np.sum(trained_probas, axis=0)\n",
    "    print('trained proba', time.time()-start, trained_sum_probas)\n",
    "    start = time.time()\n",
    "\n",
    "    saved_sum_probas = parallel_forest_predict_proba(_loadmodel, input_np_format) \n",
    "    saved_sum_probas = np.sum(saved_sum_probas, axis=0)\n",
    "\n",
    "    print('saved proba after parallel forest predict', time.time()-start)\n",
    "    all_estimators_num = len(saved_estimators) + len(estimators) \n",
    "    mean_probas = (saved_sum_probas + trained_sum_probas)  / all_estimators_num  \n",
    "\n",
    "\n",
    "    return mean_probas  # Return class probabilities\n",
    "\n",
    "\n",
    "def evaluate_params(random_params, inner_cv, X_train, y_train, pretrained_model, X_train_format, random_seed):\n",
    "    inner_scores = []\n",
    "    print('evaluate params...', random_params)\n",
    "    for inner_train_idx, inner_valid_idx in inner_cv.split(X_train, y_train):\n",
    "        X_inner_train, X_inner_valid = (\n",
    "            X_train.iloc[inner_train_idx],\n",
    "            X_train.iloc[inner_valid_idx],\n",
    "        )\n",
    "        y_inner_train, y_inner_valid = (\n",
    "            y_train[inner_train_idx],\n",
    "            y_train[inner_valid_idx],\n",
    "        )\n",
    "        X_inner_train_format, X_inner_valid_format = (\n",
    "            X_train_format.iloc[inner_train_idx],\n",
    "            X_train_format.iloc[inner_valid_idx],\n",
    "        )\n",
    "        start = time.time()\n",
    "\n",
    "        model = RandomForestClassifier(random_state=random_seed, n_jobs=-1, **random_params)\n",
    "        model.fit(X_inner_train, y_inner_train)\n",
    "\n",
    "        X_inner_valid_np = X_inner_valid.values\n",
    "        X_inner_valid_format_np = X_inner_valid_format.values\n",
    "\n",
    "        y_pred = predict_joint_by_proba_stack(model, pretrained_model, X_inner_valid_np, X_inner_valid_format_np)\n",
    "        # print('predict joint', time.time()-start)\n",
    "\n",
    "        score = f1_score(y_inner_valid, y_pred, average=\"macro\")\n",
    "        # score = roc_auc_score(y_inner_valid, y_pred)\n",
    "        inner_scores.append(score)\n",
    "        print('time inner', time.time()-start)\n",
    "    avg_inner_score = np.mean(inner_scores)\n",
    "    print('Done evaluating params...', random_params)\n",
    "\n",
    "    del X_train,  X_train_format, X_inner_train, X_inner_valid, X_inner_train_format, X_inner_valid_format, y_inner_train, y_inner_valid\n",
    "    return avg_inner_score, random_params\n",
    "\n",
    "\n",
    "def nested_cv_pipeline_parallel(\n",
    "    X_input,\n",
    "    y,\n",
    "    model_type,\n",
    "    cp,\n",
    "    prediction_window,\n",
    "    featuremap,\n",
    "    random_seed=99,\n",
    "    score=None,\n",
    "    pretrained_model=None,\n",
    "    pretrained_scalar=None,\n",
    "    reference_cols=None,\n",
    "):\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "    assert model_type == \"rf\"\n",
    "\n",
    "    if model_type == \"rf\":\n",
    "        _, param_grid = build_RF_model(random_seed)\n",
    "        X = X_input\n",
    "    outer_cv_splits = list(outer_cv.split(X, y))\n",
    "\n",
    "    outer_results = pd.DataFrame(columns=[\"cv\", \"auc\", \"pre\", \"sensitivity\", \"specificity\", \"ppv\", \"npv\", \"sensitivity_90\", \"sensitivity_95\", \"ppv_90\", \"ppv_95\"])\n",
    "    outer_best_models = []\n",
    "    standardizer_models = []\n",
    "\n",
    "    import itertools\n",
    "    all_combi = list(itertools.product(param_grid['n_estimators'], param_grid['max_depth'],param_grid['min_samples_split'],param_grid['max_features'] ))\n",
    "    # all_combi  = [(500, 20, 5, 'sqrt') ]\n",
    "    # {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 5, 'max_features': 'sqrt'}]\n",
    "\n",
    "    print('all candidates: ', all_combi)\n",
    "    for cv_, (train_idx, test_idx) in enumerate(outer_cv_splits):\n",
    "        print(f\"---Outer CV Fold {cv_}---\")\n",
    "        X_format = format_input(X, prediction_window, reference_cols)\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        X_train_format, X_test_format = X_format.iloc[train_idx], X_format.iloc[test_idx]\n",
    "\n",
    "        X_train, X_test, scalar = preprocess(X_train, X_test)\n",
    "        X_train_format, X_test_format = preprocess_scalar(X_train_format, X_test_format, pretrained_scalar)\n",
    "        standardizer_models.append(scalar)\n",
    "        \n",
    "        \n",
    "        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "        results = Parallel(n_jobs=1)(\n",
    "            delayed(evaluate_params)(\n",
    "                {'n_estimators': all_combi[er][0],'max_depth': all_combi[er][1], 'min_samples_split': all_combi[er][2], 'max_features': all_combi[er][3]} ,  # Randomly sample parameters\n",
    "                inner_cv,\n",
    "                X_train.copy(),\n",
    "                y_train,\n",
    "                pretrained_model,\n",
    "                X_train_format.copy(),\n",
    "                random_seed,\n",
    "            )\n",
    "            for er in range(30)  # Test 20 random parameter settings\n",
    "        )\n",
    "        \n",
    "        # Extract best parameters and scores\n",
    "        best_score, best_params = max(results, key=lambda x: x[0])\n",
    "        print(f\"Best Parameters for Outer Fold {cv_}: {best_params}\")\n",
    "        print(f\"Best Inner CV Score for Outer Fold {cv_}: {best_score:.4f}\")\n",
    "\n",
    "        final_model = RandomForestClassifier(random_state=random_seed, n_jobs=-1, **best_params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "        \n",
    "        axall, results_list = evaluate_ensemble_model(\n",
    "            X_test,\n",
    "            y_test,\n",
    "            final_model,\n",
    "            cp,\n",
    "            prediction_window,\n",
    "            N=30,\n",
    "            feature_map=featuremap,\n",
    "            model_name=\"Random Forest\",\n",
    "            pretrained_model=pretrained_model,\n",
    "            X_test_format=X_test_format,\n",
    "        )\n",
    "        display(axall)\n",
    "        outer_results.loc[cv_] = [int(cv_)] + results_list\n",
    "        outer_best_models.append(final_model)\n",
    "\n",
    "    mean_values = outer_results.mean().tolist()\n",
    "    std_values = outer_results.std().tolist()\n",
    "    outer_results.loc[\"mean\"] = [\"-\"] + mean_values[1:]\n",
    "    outer_results.loc[\"std\"] = [\"-\"] + std_values[1:]\n",
    "    display(outer_results)\n",
    "\n",
    "    return outer_results, outer_best_models, standardizer_models\n",
    "\n",
    "import time \n",
    "%config Application.warn_no_config=True\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def run_finetune_matched_pipeline(X, y, all_map, model_type, cps=[1], years=[0], score=None, pretrained_model=None, pretrained_scalar=None, reference_cols=None):\n",
    "    cp_year_results = {}\n",
    "    cp_year_models = {}\n",
    "    cp_year_scalars = {}\n",
    "\n",
    "    for cp in cps:\n",
    "        for prediction_window in reversed(years):\n",
    "            start = time.time()\n",
    "            print(f\"\\nRunning pipeline for CP {cp} with {prediction_window}-year prediction window...\\n\")\n",
    "            xinput = X[prediction_window]\n",
    "            # return xinput\n",
    "            if 'patid' in xinput.columns:\n",
    "                print('drop patid')\n",
    "                f_input = xinput.drop('patid', axis=1)\n",
    "\n",
    "                outer_results, outer_best_models, standardizer_models = nested_cv_pipeline_parallel(f_input, y[prediction_window], model_type, cp, prediction_window,all_map,\\\n",
    "                                                                                            score=score, pretrained_model=pretrained_model, pretrained_scalar=pretrained_scalar, \\\n",
    "                                                                                                reference_cols=reference_cols)\n",
    "                cp_year_results[prediction_window] = outer_results\n",
    "                cp_year_models[prediction_window] = outer_best_models\n",
    "                cp_year_scalars[prediction_window] = standardizer_models\n",
    "\n",
    "                print('Time eplapse', time.time() - start )\n",
    "    return cp_year_results,cp_year_models,cp_year_scalars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune on matched training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_portion =0.5\n",
    "ratio = 10\n",
    "matched_f = pickle.load( open(f'./MiddleFeatures/demo_matched_fs.pkl', 'rb'))\n",
    "\n",
    "matched_t = pickle.load( open(f'./MiddleFeatures/matched_t_drop_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'rb'))\n",
    "matched_e = pickle.load( open(f'./MiddleFeatures/matched_e_drop_portion_{str(hold_out_portion).split('.')[-1]}_ratio_{str(ratio)}.pkl', 'rb'))\n",
    " \n",
    "all_map = pickle.load(open('all_map.pkl', 'rb'))\n",
    "for i, v in all_map.items():\n",
    "    all_map[i] = i + ' ' + v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "best_cvs = [4, 0, 1, 1, 0]\n",
    "windows = [10, 5, 2, 1, 0]\n",
    "\n",
    "\n",
    "all_rs_years, all_model_test_years, all_model_scalar_years = {}, {}, {}\n",
    "\n",
    "reference_cols = joblib.load('./rf_chunks/model_test_rf_all_feature/reference_cols_xgb_all_feature.pkl')\n",
    "scalarmodels = joblib.load('./rf_chunks/model_test_rf_all_feature/test_rf_scalar_all_feature.pkl')\n",
    "\n",
    "\n",
    "for inde in range(5):\n",
    "    print('Finetune for prediction window', windows[inde])\n",
    "\n",
    "    pretrainedRF = reconstruct_rf('model_test_rf_all_feature', modelkey=f'1_{windows[inde]}_rf', modelcv=best_cvs[inde])\n",
    "\n",
    "    pretrained_scalar = scalarmodels[f'1_{windows[inde]}_rf'][best_cvs[inde]] \n",
    "\n",
    "    rs_years, model_test_years, model_scalar_years  = run_finetune_matched_pipeline(matched_f, matched_t, all_map, 'rf', cps=[1], years=[windows[inde ]], score=None,\\\n",
    "                    pretrained_model=pretrainedRF, pretrained_scalar=pretrained_scalar, reference_cols=reference_cols)\n",
    "    \n",
    "    gc.collect()\n",
    "    print('Save model', windows[inde])\n",
    "    note = str(windows[inde])\n",
    "\n",
    "    pickle.dump(model_test_years, open(f'rf_chunks/model_test_rf_all_feature_finetuned/demo_all_uncommon_model_test_years_till{note}.pkl', 'wb'))\n",
    "    pickle.dump(model_scalar_years, open(f'rf_chunks/model_test_rf_all_feature_finetuned/demo_all_uncommon_model_scalar_years_till{note}.pkl', 'wb'))\n",
    "    pickle.dump(rs_years, open(f'rf_chunks/model_test_rf_all_feature_finetuned/demo_all_uncommon_rs_years_till{note}.pkl', 'wb'))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  testing on unmatched testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "def run_finetune_evaluate_pipeline(X, y, all_map, model_type,  years=[0,1,2,5,10], pre_trained_model=None, score=None, f_reference=None, model_name=None, pre_trained_scalar=None, plot=False, finetuned_scalars=None, finetuned_models=None):\n",
    "    cp_year_results = {}\n",
    "    show_results_dict = {}\n",
    "    shap_years = {}\n",
    "    print('evaluate')\n",
    "    for cp in [1]:\n",
    "        for prediction_window in reversed(years):\n",
    "            if pre_trained_model: # should always pass pretrained model inside\n",
    "                cv_results = []\n",
    "                cv_num = 5\n",
    "\n",
    "                xinput = X[prediction_window]\n",
    "\n",
    "\n",
    "                for cv in range(cv_num):  # test each cv from the pre-trained model or only test a part of cvs\n",
    "                    print('CV: ', cv, '| Prediction window: ', prediction_window, '| Model type: ', model_type)\n",
    "                    if cv_num > 1: \n",
    "                        # model = pre_trained_model[prediction_window][cv]\n",
    "                        # pretrained_scalar = scalars[prediction_window][cv]\n",
    "      \n",
    "                        scalar = finetuned_scalars[prediction_window][cv]\n",
    "                        model = finetuned_models[prediction_window][cv]\n",
    "\n",
    "\n",
    "                    f_input = xinput.drop('patid', axis=1)\n",
    "                    print(f_input.shape)\n",
    "\n",
    "                    f_input =  f_input.reindex(columns=model.feature_names_in_, fill_value=0)\n",
    "                    # print(f_input.shape)\n",
    "\n",
    "                    f_input, _ = preprocess_scalar(f_input, None, scalar)\n",
    "                    print(f_input.shape)\n",
    "\n",
    "                    f_input_format = xinput.drop('patid', axis=1)\n",
    "                    # print(f_input_format.shape)\n",
    "\n",
    "                    f_input_format = format_input(f_input_format, prediction_window, reference_cols) # reference_cols are for all years\n",
    "                    # print(f_input_format.shape)\n",
    "\n",
    "                    f_input_format, _ = preprocess_scalar(f_input_format, None, pre_trained_scalar)\n",
    "                    print(f_input_format.shape)\n",
    "\n",
    "                    y_input = y[prediction_window]\n",
    "                    \n",
    "                    if model_type == 'rf' or model_type == 'xgb':\n",
    "                        axall, results_list = evaluate_ensemble_model(f_input, y_input, model, 1, prediction_window, 30, all_map, model_name=model_name, plot=plot, pretrained_model=pre_trained_model, X_test_format=f_input_format)\n",
    "                    else:\n",
    "                        axall, results_list = evaluate_base_model(f_input, y_input, model, 1, prediction_window, 30, all_map, model_name=model_name, plot=plot)\n",
    "                    if plot: \n",
    "                        display(axall)\n",
    "\n",
    "                    cp_year_results[f\"{str(prediction_window)}_{model_type}_{str(cv)}\"] = results_list \n",
    "                    cv_results.append(results_list)  \n",
    "\n",
    "                showresults = pd.DataFrame(cv_results)\n",
    "                showresults.columns = ['auc', 'pre', 'sensitivity', 'specificity', 'ppv', 'npv', 'sensitivity_90', 'sensitivity_95', 'ppv_90', 'ppv_95']\n",
    "                showresults.loc['mean'] = showresults.mean()\n",
    "                showresults.loc['std'] = showresults.std()\n",
    "                display('Show results of cvs', showresults)\n",
    "                show_results_dict[f\"{str(cp)}_{str(prediction_window)}_{model_type}\"] = showresults\n",
    "\n",
    "    return show_results_dict, cp_year_results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "import time\n",
    "gc.collect()\n",
    "hold_out_portion = 0.5\n",
    "unmatched_f = pickle.load( open(f'./MiddleFeatures/demo_unmatched_fs.pkl', 'rb'))\n",
    "\n",
    "unmatched_t = pickle.load( open(f'./MiddleFeatures/test_t_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'rb'))\n",
    "unmatched_e = pickle.load( open(f'./MiddleFeatures/test_e_portion_{str(hold_out_portion).split('.')[-1]}.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "\n",
    "best_cvs = [4, 0, 1, 1, 0]\n",
    "windows = [10, 5, 2, 1, 0]\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "showresults, results = {}, {}\n",
    "\n",
    "reference_cols = joblib.load('./rf_chunks/model_test_rf_all_feature/reference_cols_xgb_all_feature.pkl')\n",
    "scalarmodels = joblib.load('./rf_chunks/model_test_rf_all_feature/test_rf_scalar_all_feature.pkl')\n",
    "\n",
    "for inde in range(5):\n",
    "    note=str(windows[inde])\n",
    "\n",
    "    model_test_years_save = pickle.load( open(f'rf_chunks/model_test_rf_all_feature_finetuned/demo_all_uncommon_model_test_years_till{note}.pkl', 'rb'))\n",
    "    model_scalar_years_save = pickle.load( open(f'rf_chunks/model_test_rf_all_feature_finetuned/demo_all_uncommon_model_scalar_years_till{note}.pkl', 'rb'))\n",
    "\n",
    "    pretrainedRF = reconstruct_rf('model_test_rf_all_feature', modelkey=f'1_{windows[inde]}_rf', modelcv=best_cvs[inde])\n",
    "\n",
    "    pretrained_scalar = scalarmodels[f'1_{windows[inde]}_rf'][best_cvs[inde]] \n",
    "\n",
    "    # model_test_years = model_test_years_save[windows[inde]]\n",
    "    # model_scalar_years = model_scalar_years_save[windows[inde]]\n",
    "    \n",
    "    model_test_years = model_test_years_save \n",
    "    model_scalar_years = model_scalar_years_save \n",
    "\n",
    "    showresults[windows[inde]], results[windows[inde]] = run_finetune_evaluate_pipeline(unmatched_f, unmatched_t, all_map, 'rf',  years=[windows[inde]],  pre_trained_model=pretrainedRF,\\\n",
    "                                score=None , model_name='all feature', pre_trained_scalar=pretrained_scalar, f_reference=reference_cols, finetuned_models=model_test_years, finetuned_scalars=model_scalar_years)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrdPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
